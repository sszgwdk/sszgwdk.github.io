[{"categories":["DB"],"content":"bustub的哈希索引结构 bustub使用的哈希索引是可扩展哈希的一种变体，在两级方案的基础上添加了一个HeaderPage​，按照文档的说法，这是为了使哈希表可以容纳更多值并有可能实现更好的多线程性能。 ​ bustub的哈希索引结构 ​ ","date":"2024-08-29","objectID":"/posts/cmu445_p2/:1:0","tags":["CMU15-445","DB"],"title":"CMU15-445 Fall 2023 Project 2","uri":"/posts/cmu445_p2/"},{"categories":["DB"],"content":"三种Page 如文档任务2所说，可扩展哈希表由三种Page组成：HeaderPage​、DirectoryPage​和BucketPage​。 HeaderPage 一般只有一个HeaderPage​，类似二级页表中的页目录表。 private: page_id_t directory_page_ids_[HTABLE_HEADER_ARRAY_SIZE]; uint32_t max_depth_; ​HeaderPage​维护成员包括一个DirectoryPage​数组和最大深度max_depth_​，记深度为x，数组的大小为2 ^ x​，且使用哈希值的最高有效x位进行索引。如位深度为2，32位哈希值为0x5f129982，最高有效位前2位为01，则会被索引到01对应的DirectoryPage上。 DirectoryPage private: uint32_t max_depth_; uint32_t global_depth_; uint8_t local_depths_[HTABLE_DIRECTORY_ARRAY_SIZE]; page_id_t bucket_page_ids_[HTABLE_DIRECTORY_ARRAY_SIZE]; ​DirectoryPage​的成员包含第三层的BucketPage​的PageId数组，全局位深度global_depth​和每一个Bucket的局部位深度local_depth​数组，PageId数组的大小为2 ^ global_depth​，且使用哈希值的最低有效global_depth​位进行索引。如全局位深度为2，32位哈希值为0x51129982，最低有效位后两位为10，则会被索引到10对应的BucketPage上。 BucketPage private: uint32_t size_; uint32_t max_size_; MappingType array_[HTableBucketArraySize(sizeof(MappingType))]; 对应一个哈希桶，成员包含一个Key-Value数组，以及数组大小size_​。被索引到该Bucket的Key-Value会追加到该Bucket上进行存储。有一个max_size_​限制，例如2。 对上面提到的三个depth​的直观理解： ​max_depth​：header、directory各自的最大深度限制，header的max_depth​还用于索引directory表项； ​global_depth​：对应directory的当前深度，用于索引bucket表项（当前的max_size不代表有效条目数）； ​local_depth​：对应bucket的当前深度（==非常重要，在insert流程中具体讲解作用==） ​一个限制​：global_depth \u003e= local_depth​ ​ 限制 ​ ","date":"2024-08-29","objectID":"/posts/cmu445_p2/:1:1","tags":["CMU15-445","DB"],"title":"CMU15-445 Fall 2023 Project 2","uri":"/posts/cmu445_p2/"},{"categories":["DB"],"content":"查找流程 假设哈希值固定为32bit 获取header_page​，计算key的哈希值hash​ 根据hash \u003e\u003e (32u - max_depth_)​（取哈希值最高的max_depth_​位）得到directory_page_index​，进而得到derectory_page​ 根据hash \u0026 (1u \u003c\u003c global - 1)​（取哈希值最低的global_depth​位）得到buckect_page_index​，进而得到bucket_page​ 遍历bucket_page​中的所有条目，查找是否有目标key ","date":"2024-08-29","objectID":"/posts/cmu445_p2/:1:2","tags":["CMU15-445","DB"],"title":"CMU15-445 Fall 2023 Project 2","uri":"/posts/cmu445_p2/"},{"categories":["DB"],"content":"动态扩展原理 最核心需要理解的是插入操作可能触发的directory​扩容和桶拆分，以及删除操作可能触发的directory​缩容和桶合并。 Insert大致流程 首先根据key的hash值，定位到目标桶target​，如果目标桶没满，直接追加。 如果目标桶已经满了，这时候需要考虑directory​扩容和桶拆分，两种情况： ​global_depth == local_depth[target]​，此时代表只有一个表项指向target​（注意理解这里深度相同的含义），要做以下工作： ​global_depth++​，使得directory的容量翻倍，原来的每个表项都产生出自己的一个副本表项（称作split_image​，若一个原始表项的位置是bucket_index​，则它的split_image​对应位置是bucket_index + (1 \u003c\u003c (global_depth - 1))​），新的表项包含和原始表项一样的指针，即指向相同的bucket。 为什么只需要分裂一个桶，但是所有旧表项都要有一个副本呢？ 因为global_depth++之后，由hash值获取目标桶的idx会增加一位，有可能就会指向副本的idx，此时副本表项指向相同的桶，就不会对结果造成影响。 系统分配一个new_bucket​，让目标桶的副本表项指向它。并且令​**local_depth[target]**​和​**local_depth[new_bucket]**​都等于​**global_depth**​**，代表均只有一个表项指向它们。** 将目标桶中的所有记录和要插入的新记录合起来，根据记录的key的hash值的后global_depth​位（现在已经增加了一位）重新散列到target_bucket​和new_bucket​。 ​global_depth \u003e local_depth[target]​，此时directory中不止一个表项指向目标桶target​，会有2^(global_depth - local_depth[target])​个表项。此时不需要增加global_depth​，直接分裂目标桶即可： 系统分配一个new_bucket​ 关键注意：哪些表项修改为指向​**new_bucket**​**?因为此时不是两个了** 以local_depth = 2，global_depth = 4为例，此时有2^(4 - 2) = 4​个表项指向target_bucket​（假设已有一个确定的bucket_index​，那么其他三个满足idx \u0026 0x3 == bucket_index \u0026 0x3​的idx即为指向target_bucket​的表项索引（最低两位相同））。 我们想要拆分bucket，也就是让local_depth+1，在这个例子中这些idx最低两位相同，要实现拆分，要让idx的第3位发挥作用，所以拆分的逻辑便是：让第3位 = 0 的idx对应的表项指向原始target_bucket​，让倒数第3位 = 1 的idx对应的表项指向新拆分得到的new_bucket​。 那么这个“第3位”怎么得到？很容易推出就是local_depth+1​，这也是local_depth​的含义之一，代表多少位发挥作用。进而可以用1U \u003c\u003c local_depth​检查idx的对应位是否为1，以此来进行桶的拆分。 重新散列所有记录，local_depth++​ Remove大致流程 当Remove成功之后，目标桶变空，就需要合并桶，此时只需要改变所有指向空桶的指针，然后删除掉对应页，同时对其local_depth--​即可。 注意，有可能合并之后桶还是空的，此时要持续合并直到不为空，不然不能通过测试。 最后还可能需要对directory缩容global_depth--​（如果所有bucket的local_depth都比global_depth小）。 ‍ ","date":"2024-08-29","objectID":"/posts/cmu445_p2/:1:3","tags":["CMU15-445","DB"],"title":"CMU15-445 Fall 2023 Project 2","uri":"/posts/cmu445_p2/"},{"categories":["DB"],"content":"Task#1：Read Write Page Guards Task1要求实现三种PageGuard​类，分别是BasicPageGuard​、ReadPageGuard​和WritePageGuard​。三种PageGuard​类都使用RAII的思想保护Buffer Pool Manager​的缓冲页，防止用户遗漏调用Unpin​方法导致缓冲页被固定无法驱逐，与只能指针相似，当PageGuard​对象生命周期结束时，在析构函数中调用Unpin​方法来确保释放缓冲页；进一步的，ReadPageGuard​和WritePageGuard​还会保护缓存页的读写一致性，且避免死锁（若其他时候后忘记解锁，则在析构时解锁）。 三种类中主要实现的方法有： 三种PageGuard​的移动构造函数、移动赋值运算符、Drop​方法与析构函数 ​BasicPageGuard​类中的UpgradeRead()​函数和UpgradeWrite()​函数 关于移动构造函数、移动赋值运算符，要对右值引用相关概念有所了解 ​Drop​函数为对外提供的释放页接口，它需要做的事情就是调用一下BufferPoolManager​的UnpinPage​函数，将page_​中维护的页释放（如果有的话）；在ReadPageGuard​和WritePageGuard​中，则还需要解锁（读锁或写锁）。 实现完这三种PageGuard​类后，需要使用它们实现BufferPoolManager​类中的FetchPageBasic​、FetchPageRead​、FetchPageWrite​和NewPageGuarded​函数。实现也比较简单，都是先调用FetchPage​或NewPage​，然后构造一个PageGuard​，注意如果是ReadPageGuard​要加读锁RLatch​，如果是WritePageGuard​要加写锁WLatch​。 ‍ ","date":"2024-08-29","objectID":"/posts/cmu445_p2/:2:0","tags":["CMU15-445","DB"],"title":"CMU15-445 Fall 2023 Project 2","uri":"/posts/cmu445_p2/"},{"categories":["DB"],"content":"Task#2：Extendible Hash Table Pages 实现三种可扩展哈希中使用的数据结构，也就是三层结构中每一层的页面布局。如上文所说，分别是HeaderPage​、DirectoryPage​和BucketPage​。 这三个类中需要我们实现的接口都非常简单，需要注意一些边界条件，例如max_depth​的限制，local_depth \u003c= global_depth​的限制等。 另外一个需要注意的是DirectoryPage​的IncrGlobalDepth()​，不仅要对global_depth_++​，如上文“Insert大致流程”中所说，还要为原有的表项创建副本，即拷贝bucket表项和local_depth。 ‍ ","date":"2024-08-29","objectID":"/posts/cmu445_p2/:3:0","tags":["CMU15-445","DB"],"title":"CMU15-445 Fall 2023 Project 2","uri":"/posts/cmu445_p2/"},{"categories":["DB"],"content":"Task#3：Extendible Hashing Implementation 实现Extendible Hashing的查询、插入和删除。难点在于“bucket splitting/merging and directory growing/shrinking”。 查询比较简单，按照header-\u003edirectory-\u003ebucket的顺序来查找即可，注意使用Task1中实现的FetchPageRead​和三种PageGuard​的Drop​（尽早释放，提高并发度）。 ","date":"2024-08-29","objectID":"/posts/cmu445_p2/:4:0","tags":["CMU15-445","DB"],"title":"CMU15-445 Fall 2023 Project 2","uri":"/posts/cmu445_p2/"},{"categories":["DB"],"content":"插入Insert ​​ Insert ​​ ​InsertToNewDirectory​和InsertToNewBucket​的实现步骤是相似的： 通过NewPageGuarded​新建数据页，并获取PageGuard​ 通过UpgradeWrite()​提升为写锁 ​Init​初始化页面 ​SetDirectoryPageId​或SetBucketPageId​：在headerPage或directoryPage设置表项 ​InsertToNewBucket​或bucket_page-\u003eInsert(k, v, cmp_)​ 在上面的流程图中可以发现，除了HeaderPage​，DirectoryPage​和BucketPage​都是直到最后才释放写锁的（可能有并发度更高的实现），这里可以体现出bustub的可扩展哈希表增加了一个HeaderPage​的优势，这样在不同directoryPage​上的插入操作可以并发进行，从而能有更好的多线程性能。 分裂桶的步骤根据上文“Insert大致流程”，实现逻辑如下： ​ SplitBucket ​ ‍ ","date":"2024-08-29","objectID":"/posts/cmu445_p2/:4:1","tags":["CMU15-445","DB"],"title":"CMU15-445 Fall 2023 Project 2","uri":"/posts/cmu445_p2/"},{"categories":["DB"],"content":"删除Remove Remove操作首先按照类似查询的逻辑一步步定位到bucket_page，然后删除掉对应条目entry，如果出现未找到的情况，则直接返回false。 删除成功之后，如果当前bucket_page不为空，则直接返回true，否则要尝试合并bucket。合并详细流程如下（画得是真丑，轻喷） ​​ MergeBucket ​​ ‍ ","date":"2024-08-29","objectID":"/posts/cmu445_p2/:4:2","tags":["CMU15-445","DB"],"title":"CMU15-445 Fall 2023 Project 2","uri":"/posts/cmu445_p2/"},{"categories":["DB"],"content":"Project 1是为Bustub构建一个面向磁盘的缓存管理器（Storage Manager）。 缓存管理器（也叫缓存池，Buffer Pool）是数据库系统中一个必不可少的组件，可以显著减少数据库的I/O操作，降低数据库负载。在DBMS中，记录是按照行来存储的，但是数据库的读取并不是以行为单位的，否则一次读取（也就是一次 I/O 操作）只能处理一行数据，效率会非常低。Bustub的数据是按页为单位进行读写的，页的大小可以是4KB、8KB、16KB等，是缓存管理的最小单元。 为了减少磁盘I/O操作，Buffer Pool的作用就是把最热的数据页缓存在内存中，下次需要这个数据页时可以直接从内存读取，而不是进行一次磁盘I/O。当需要读取或写入数据时，存储引擎首先检查缓冲池中是否已经存在所需的数据页。如果数据页在缓冲池中，DBMS可以直接从内存中读取或写入数据，避免了磁盘IO的开销。如果数据页不在缓冲池中，DBMS就需要从磁盘加载数据页到缓冲池，并进行相应的操作。 相应的，缓冲池的管理也涉及到数据页的替换策略。当缓冲池已满时，需要替换一些数据页以腾出空间来存储新的数据页。常见的数据页替换策略包括最近最少使用（LRU）和时钟（Clock）算法。 有关数据库缓存机制，可以学习参考以下文章：MySQL缓冲池（Buffer Pool）深入解析：原理、组成及其在数据操作中的核心作用-腾讯云开发者社区-腾讯云 (tencent.com) Bustub的缓存管理器需要我们实现三个组件： LRU-K页面替换策略：管理缓存页的替换、淘汰 磁盘调度器：执行底层的磁盘IO操作 缓存池管理器：封装向上层提供的缓存页面操作接口（FetchPage​、FlushPage​、NewPage​等） ","date":"2024-08-29","objectID":"/posts/cmu445_p1/:0:0","tags":["CMU15-445","DB"],"title":"CMU15-445 Fall 2023 Project 1","uri":"/posts/cmu445_p1/"},{"categories":["DB"],"content":"Task#1 LRU-K Replacement Policy LRU-K是LRU算法的一种衍生，每次替换会优先替换 k-distance 最远的一个数。LRU算法实现虽然简单，并且在大量频繁访问热点页面时十分高效，但同样也有一个缺点，就是如果该热点页面在偶然一个时间节点被其他大量仅访问了一次的页面所取代（例如Scan操作），那自然造成了浪费。LRU-K的主要目的是为了解决 LRU 算法\"缓存污染\"的问题，其核心思想是将“最近使用过 1 次”的判断标准扩展为“最近使用过 K 次”。 这里的 k-distance 是这样定义的： 如果它访问的次数大于等于 k ，那么它的 k-distance 是倒数第k次访问的时间 如果它访问的次数小于 k ，那么它的 k-distance 是无穷大 +inf 驱逐方法是这样的： 如果有 k-distance 为 +inf 的，优先驱逐 如果有多个 k-distance 为 +inf 的，根据题目的定义，采用FIFO，即最早访问的 否则，驱逐 k-distance 最小的 在lru_k_replacer.h​和lru_k_replacer.cpp​中，首先需要完成LRUKNode​类的封装： const size_t K_MAX_TIMESTAMP = std::numeric_limits\u003csize_t\u003e::max(); class LRUKNode { public: LRUKNode() = default; explicit LRUKNode(size_t timestamp, size_t k, frame_id_t fid, bool is_evictable); // std::weak_ptr\u003cLRUKNode\u003e prev_; // std::shared_ptr\u003cLRUKNode\u003e next_; auto GetKDistance() const -\u003e size_t; auto GetFrameId() const -\u003e frame_id_t; auto GetEarliestAccessTimestamp() const -\u003e size_t; void RemoveAccessHistory(); void UpdateAccessHistory(size_t timestamp); auto IsEvictable() const -\u003e bool; void SetEvictable(bool is_evictable); private: /** History of last seen K timestamps of this page. Least recent timestamp stored in front. */ // Remove maybe_unused if you start using them. Feel free to change the member variables as you want. std::list\u003csize_t\u003e history_; size_t k_; frame_id_t fid_; bool is_evictable_{false}; }; ​GetKDistance()​：如果history_.size() \u003c k_​，返回K_MAX_TIMESTAMP​，否则返回history_.front()​（我们在history_​中只需要维护最近的k_​个访问时间戳） ​GetEarliestAccessTimestamp()​：返回history_.front()​ ​RemoveAccessHistory()​：清空history_​ ​UpdateAccessHistory(ts)​：尾部插入ts​，如果当前大小超过k_​，从头部pop直到size == k_​ ​LRUKReplacer​是实现LRU-K算法的主要类，为了方便算法实现，需要增加几个私有成员。 class LRUKReplacer { public: ... private: // TODO(student): implement me! You can replace these member variables as you like. // Remove maybe_unused if you start using them. std::unordered_map\u003cframe_id_t, std::shared_ptr\u003cLRUKNode\u003e\u003e node_store_; std::list\u003cframe_id_t\u003e less_k_list_; std::list\u003cframe_id_t\u003e more_k_list_; size_t current_timestamp_{0}; size_t curr_size_{0}; size_t replacer_size_; size_t k_; std::mutex latch_; }; ​node_store_​维护缓存页号frame_id_t​（注意区分数据页的概念，对应page_id_t​）到LRUKNode​的映射 ​less_k_list_​和more_k_list_​分别维护访问次数小于和大于等于k的缓存页链表 我们要实现LRUKReplacer​的以下几个接口： ​Evict​：淘汰一个缓存页 ​RecordAccess​：记录一次对一个缓存页的访问 ​SetEvictable​：设置一个缓存页的is_evictable_​ ​Remove​：直接删除某个特定的缓存页，不用考虑k-distance 特别注意，由于node_store_​、less_k_list_​等都不是线程安全的，因此对于每个接口，都要用std::lock_guard\u003cstd::mutex\u003e lock(latch_)​加一把大锁。 ","date":"2024-08-29","objectID":"/posts/cmu445_p1/:1:0","tags":["CMU15-445","DB"],"title":"CMU15-445 Fall 2023 Project 1","uri":"/posts/cmu445_p1/"},{"categories":["DB"],"content":"Evict 首先淘汰less_k_list_​中的缓存页，如果不为空 取出头部的frame_id​并pop_front​，从node_store_​中获取对应的LRUKNode​，记作node​ ​node-\u003eRemoveAccessHistory();​ ​node-\u003eSetEvictable(false);​ ​curr_size_--;​ 返回true​ 在考虑淘汰more_k_list_​中的缓存页 选择链表中EarliestAccessTimestamp​最小的缓存页号frame_id​ 后续步骤同less_k_list_​ ","date":"2024-08-29","objectID":"/posts/cmu445_p1/:1:1","tags":["CMU15-445","DB"],"title":"CMU15-445 Fall 2023 Project 1","uri":"/posts/cmu445_p1/"},{"categories":["DB"],"content":"RecordAccess 这里不用考虑做Evict，由上层调用 ​current_timestamp_++​ ​BUSTUB_ASSERT​检查frame_id​是否合理，即frame_id \u003c= replacer_size_​ 如果当前frame_id​不在node_store_​，说明是一个新节点，需要创建LRUKNode​，默认is_evictable_​设置为false，因此不需要插入more_k_list_​或less_k_list_​ 如果当前frame_id​在node_store_​，说明是一个老节点，需要更新LRUKNode​的history_​，并从原来的list中remove掉，如果is_evictable_=true​，根据kdistance重新插入到more_k_list_​或less_k_list_​尾部 ","date":"2024-08-29","objectID":"/posts/cmu445_p1/:1:2","tags":["CMU15-445","DB"],"title":"CMU15-445 Fall 2023 Project 1","uri":"/posts/cmu445_p1/"},{"categories":["DB"],"content":"SetEvictable ​BUSTUB_ASSERT​检查frame_id​是否合理，即frame_id \u003c= replacer_size_​ 在node_store_​中查找，如果节点不存在，直接返回 检查is_evictable​是否有变化，无变化直接返回；如果有变化，先从more_k_list_​和less_k_list_​中remove掉，更新节点的is_evictable_​； 如果set_evictable​为True，curr_size_++​，并根据当前kdistance插入对应链表 如果为false，curr_size_--​ ","date":"2024-08-29","objectID":"/posts/cmu445_p1/:1:3","tags":["CMU15-445","DB"],"title":"CMU15-445 Fall 2023 Project 1","uri":"/posts/cmu445_p1/"},{"categories":["DB"],"content":"Remove 同样​BUSTUB_ASSERT​检查frame_id​是否合理 检查是否能在node_store_​中查找到 ​BUSTUB_ASSERT​检查node是否可淘汰，根据node的is_evictable_​，如果不可淘汰，报错 从less_k_list_​或more_k_list_​中remove掉 ​node-\u003eRemoveAccessHistory();​、node-\u003eSetEvictable(false);​ ​node_store_.erase(frame_id);​、curr_size--​ ","date":"2024-08-29","objectID":"/posts/cmu445_p1/:2:0","tags":["CMU15-445","DB"],"title":"CMU15-445 Fall 2023 Project 1","uri":"/posts/cmu445_p1/"},{"categories":["DB"],"content":"Task#2 Disk Scheduler 磁盘调度器，主要实现Schedule(DiskRequest r)​和StartWorkerThread()​两个接口。 ","date":"2024-08-29","objectID":"/posts/cmu445_p1/:3:0","tags":["CMU15-445","DB"],"title":"CMU15-445 Fall 2023 Project 1","uri":"/posts/cmu445_p1/"},{"categories":["DB"],"content":"Schedule 主要任务：调度一个request给DiskManager​执行。 注意这里不需要使用background_thread_​，而是直接将一个请求加入到channel当中，即只有一个线程来Schedule，background_thread_​是用于process这些request的。 void DiskScheduler::Schedule(DiskRequest r) { request_queue_.Put(std::move(r)); } ","date":"2024-08-29","objectID":"/posts/cmu445_p1/:3:1","tags":["CMU15-445","DB"],"title":"CMU15-445 Fall 2023 Project 1","uri":"/posts/cmu445_p1/"},{"categories":["DB"],"content":"StartWorkerThread 这是后台线程background_thread_​执行的函数，是一个死循环，当收到空的请求时才break掉。 它负责循环访问channel（request_queue_​），获取其中请求调用disk_manager_​的读写接口进行实际磁盘I/O处理，并且在I/O操作完成后将请求中的callback_​设置为true，用以通知上层（缓存管理器，buffer_pool_manager）操作完成。 ​Channel\u003cstd::optionaldiskrequest\u0026gt; request_queue_;​​ /** * Channels allow for safe sharing of data between threads. This is a multi-producer multi-consumer channel. */ template \u003cclass T\u003e class Channel { public: Channel() = default; ~Channel() = default; /** * @brief Inserts an element into a shared queue. * * @param element The element to be inserted. */ void Put(T element) { std::unique_lock\u003cstd::mutex\u003e lk(m_); q_.push(std::move(element)); lk.unlock(); cv_.notify_all(); } /** * @brief Gets an element from the shared queue. If the queue is empty, blocks until an element is available. */ auto Get() -\u003e T { std::unique_lock\u003cstd::mutex\u003e lk(m_); cv_.wait(lk, [\u0026]() { return !q_.empty(); }); T element = std::move(q_.front()); q_.pop(); return element; } private: std::mutex m_; std::condition_variable cv_; std::queue\u003cT\u003e q_; }; 源码src/include/common/channel.h​提供了一个可供多线程安全访问的队列，是一个多生产者多消费者模型。（不过目前我们在磁盘调度器中的使用方式是单生产者单消费者，后续可以优化） ​disk_manager_​实现的I/O逻辑也比较简单，可以看到在bustub当中，数据页号page_id​与其在数据文件中的偏移的计算公式为offset = page_id * BUSTUB_PAGE_SIZE​。 void DiskManager::WritePage(page_id_t page_id, const char *page_data) { std::scoped_lock scoped_db_io_latch(db_io_latch_); size_t offset = static_cast\u003csize_t\u003e(page_id) * BUSTUB_PAGE_SIZE; // set write cursor to offset num_writes_ += 1; db_io_.seekp(offset); db_io_.write(page_data, BUSTUB_PAGE_SIZE); // check for I/O error if (db_io_.bad()) { LOG_DEBUG(\"I/O error while writing\"); return; } // needs to flush to keep disk file in sync db_io_.flush(); } 涉及的C++知识： ​std::promise​ 是C++11并发编程中常用的一个类，常配合std::future​使用。其作用是在一个线程t1中保存一个类型typename T的值，可供相绑定的std::future​对象在另一线程t2中获取。promise一个典型应用场景就是callback函数。 [C++11]std::promise介绍及使用_c++ 中的promise-CSDN博客 条件变量std::condition_variable​用于阻塞一个或多个线程，直到某个线程修改线程间的共享变量，并通过condition_variable​通知其余阻塞线程。从而使得已阻塞的线程可以继续处理后续的操作。经常与unique_lock​搭配使用。 条件变量condition_variable的使用及陷阱 - 封fenghl - 博客园 (cnblogs.com) ","date":"2024-08-29","objectID":"/posts/cmu445_p1/:3:2","tags":["CMU15-445","DB"],"title":"CMU15-445 Fall 2023 Project 1","uri":"/posts/cmu445_p1/"},{"categories":["DB"],"content":"Task #3 - Buffer Pool Manager Buffer Pool Manger有两个主要职责： 利用DiskScheduler​从磁盘获取数据页并存储在内存当中； 当明确指示将脏页写入磁盘时，或者当需要逐出页面以便为新页腾出空间时，将脏页写入磁盘。 我们先来看一下BufferPoolManager​的定义： private: /** Number of pages in the buffer pool. */ const size_t pool_size_; /** The next page id to be allocated */ std::atomic\u003cpage_id_t\u003e next_page_id_ = 0; /** Array of buffer pool pages. */ Page *pages_; /** Pointer to the disk sheduler. */ std::unique_ptr\u003cDiskScheduler\u003e disk_scheduler_ __attribute__((__unused__)); /** Pointer to the log manager. Please ignore this for P1. */ LogManager *log_manager_ __attribute__((__unused__)); /** Page table for keeping track of buffer pool pages. */ std::unordered_map\u003cpage_id_t, frame_id_t\u003e page_table_; /** Replacer to find unpinned pages for replacement. */ std::unique_ptr\u003cLRUKReplacer\u003e replacer_; /** List of free frames that don't have any pages on them. */ std::list\u003cframe_id_t\u003e free_list_; /** This latch protects shared data structures. We recommend updating this comment to describe what it protects. */ std::mutex latch_; 其中几个比较重要的成员的作用： ​pages_​：存储数据页的数组，它的下标对应缓存页号frame_id_t​，Page​中存的是数据页号page_id_t​； ​disk_scheduler_​：磁盘调度器； ​page_table_​：数据页号到缓存页号的映射； ​replacer_​：LRU-K算法，实现页面淘汰； ​free_list_​：当前空闲的缓存页； ​latch_​：一把大锁保平安，后续再考虑优化。 在类的构造函数当中，需要为pages_​申请内存空间，初始化free_list_​，创建disk_scheduler_​和repalcer_​。析构函数中只需要delete[] pages_​。 ​Page​类中还有一个pin_count_​成员需要注意，记录了被不用线程 pinned 的次数，BufferPoolManager 不应该驱逐被 pinned 的 Page。 class Page { public: ... private: /** Zeroes out the data that is held within the page. */ inline void ResetMemory() { memset(data_, OFFSET_PAGE_START, BUSTUB_PAGE_SIZE); } /** The actual data that is stored within a page. */ // Usually this should be stored as `char data_[BUSTUB_PAGE_SIZE]{};`. But to enable ASAN to detect page overflow, // we store it as a ptr. char *data_; /** The ID of this page. */ page_id_t page_id_ = INVALID_PAGE_ID; /** The pin count of this page. */ int pin_count_ = 0; /** True if the page is dirty, i.e. it is different from its corresponding page on disk. */ bool is_dirty_ = false; /** Page latch. */ ReaderWriterLatch rwlatch_; }; 我们需要实现头文件 （src/include/buffer/buffer_pool_manager.h） 和源文件（src/buffer/buffer_pool_manager.cpp） 中定义的以下函数： ​FetchPage(page_id_t page_id)​：用于获取数据页。首先在page_table_​中查找，如果存在，令pin_count++​，在replacer_​中记录以及设置为不可淘汰，然后返回Page；如果没找到，先从free_list_​获取空闲的缓存页，如果为空，利用replacer_-\u003eEvict​淘汰一个，如果还是失败，直接返回；如果成功得到一个缓存页，更新page_table_​指向新数据页，如果缓存页对应的旧数据页是脏页，调用DiskScheduler​写回磁盘，完成写操作后，将page_id_​、is_dirty_​、pin_count_​赋新值或初始化，并ResetMemory()​，接着再次利用磁盘调度器读取新数据页中的数据到内存当中，完成读操作后，在replacer_​中记录以及设置为不可淘汰。 ​UnpinPage(page_id_t page_id, bool is_dirty)​：用于减少指定页面的pin_count_​，并在必要时标记页面为脏页。首先检查页面id是否有效，查找page_table_​获取缓存页号，失败均返回false；访问对应数据页的内存，设置is_dirty_​和pin_count_--​，如果减少到0，标记缓存页可淘汰，最后返回true。 ​FlushPage(page_id_t page_id)​：不论is_dirty_​，强制flush到磁盘。 ​NewPage(page_id_t* page_id)​：新建一个数据页。首先从free_list_​（优先）或replacer_​中得到一个缓存页frame_id，如果失败则返回nullptr​；接着调用AllocatePage()​为新数据页分配id，并在page_table_​更新映射关系，如果缓存页对应的旧数据页是脏页，调用DiskScheduler​写回磁盘，完成写操作后，将page_id_​、is_dirty_​、pin_count_​赋新值或初始化，并ResetMemory()​。最后在replacer_​中记录以及设置为不可淘汰。 ​DeletePage(page_id_t page_id)​：删除一个数据页。首先在page_table_​中查找，如果不存在，直接返回true。接着检查pin_count_​，如果大于0，说明不能被删除，返回false。如果满足删除的条件，依次从page_table_​、repalcer_​中删除，并将对应的缓存页添加到free_list_​，并重置该数据页的数据和元数据。 ​FlushAllPages()​：将page_table_​中所有有效的数据页都flush到磁盘，此时可以先批量发出所有I/O请求，然后统一等待I/O完成。 对于上述所有接口，建议先用一把大锁进行并发控制，确保逻辑正确通过测试后，再考虑性能的优化。 ‍ ","date":"2024-08-29","objectID":"/posts/cmu445_p1/:4:0","tags":["CMU15-445","DB"],"title":"CMU15-445 Fall 2023 Project 1","uri":"/posts/cmu445_p1/"},{"categories":["DB"],"content":"Project 0是一个C++热身项目。使用的C++的版本是C++17，但是知道C++11的知识点就足够了。 ","date":"2024-08-23","objectID":"/posts/cmu445_p0/:0:0","tags":["CMU15-445","DB"],"title":"CMU15-445 Fall 2023 Project 0","uri":"/posts/cmu445_p0/"},{"categories":["DB"],"content":"C++前置知识 从我的编码过程来看，P0主要涉及智能指针、强制类型转换、右值引用和锁管理四个部分的C++知识。 C++智能指针分为unique_ptr​、shared_ptr​和weak_ptr​，unique_ptr​是独占的，而shared_ptr​是共享的，底层使用引用计数，当引用为0时会自动释放指针所指向的资源，能够有效避免内存泄漏问题。可以通过std::make_unique​和std::make_shared​创建对应的智能指针。weak_ptr​则是为了协助shared_ptr​，它不会引起引用计数的变化，在bustub中几乎没有用到。 C++强制类型转换运算符有static_cast​、const_cast​、reinterpret_cast​和dynamic_cast​。重点关注后两个，reinterpret_cast​和dynamic_cast​均可以将多态基类的指针或引用强制转换为派生类的指针或引用，区别在于dynamic_cast​会检查转换的安全性，对于不安全的指针转换，会返回nullptr​。dynamic_cast​通过“运行时类型检查”来保证安全性。但是reinterpret_cast​更加灵活，dynamic_cast​不能用于将非多态基类的指针或引用强制转换为派生类的指针或引用——这种转换没法保证安全性，只好用reinterpret_cast​来完成。 右值引用涉及的概念较多，这里我只简单描述一下我的理解，可能存在谬误。 首先右值可以简单理解为临时对象，没办法取它的地址。之所以取名右值，是因为在等式右边的表达式值往往是临时对象。C++ 11引入了右值引用类型\u0026\u0026​，可以用来实现移动语义和完美转发，在bustub中主要关注移动语义。 什么是移动语义？举个例子，当一个类中的指针成员申请了堆内存，发生拷贝操作时如果没有自定义拷贝构造函数，会使用默认的拷贝构造函数，即成员逐个赋值，那么就会出现多个指针指向同一块堆内存的情况，在析构时就会出现double free的问题，也就是我们常说的浅拷贝的问题。通过自定义拷贝构造函数实现深拷贝自然可以解决，如果此时我们只需要一份数据，那么深拷贝就增加了很多不必要的资源申请和释放操作。我们完全可以在拷贝的过程中，在新对象的指针赋值之后，将原始对象的指针均置为nullptr，这样就省去了拷贝带来的资源消耗，这也正是移动语义的作用之一。 那么如何使用右值引用实现移动语义？通过在类中定义移动构造函数和移动赋值运算符，参数就是该类的右值引用，这样在传入一个右值时就会调用移动构造函数或移动赋值运算符，有时为了避免拷贝，还可以禁用拷贝构造函数和拷贝赋值运算符。可以参考下面的代码：（来自trie.h​） /// A special type that will block the move constructor and move assignment operator. Used in TrieStore tests. class MoveBlocked { public: explicit MoveBlocked(std::future\u003cint\u003e wait) : wait_(std::move(wait)) {} MoveBlocked(const MoveBlocked \u0026) = delete; MoveBlocked(MoveBlocked \u0026\u0026that) noexcept { if (!that.waited_) { that.wait_.get(); } that.waited_ = waited_ = true; } auto operator=(const MoveBlocked \u0026) -\u003e MoveBlocked \u0026 = delete; auto operator=(MoveBlocked \u0026\u0026that) noexcept -\u003e MoveBlocked \u0026 { if (!that.waited_) { that.wait_.get(); } that.waited_ = waited_ = true; return *this; } bool waited_{false}; std::future\u003cint\u003e wait_; }; 需要注意的是，如果禁用了拷贝构造函数和拷贝赋值函数，此时通过该类的旧对象（此时为左值）创建新的类对象会报错，要通过std::move​将旧对象强制转为右值引用。例如：MoveBlocked b(std::move(a))​或者auto b = std::make_shared\u003cMoveBlocked\u003e(std::move(a))​。此时a中的资源会被转移到b中，如果a未赋新值，后续不应再被使用。 锁管理比较简单就是std::lock_guard​和std::unique_lock​的使用。可以参考以下文章：C++ 锁管理：std::lock_guard 和 std::unique_lock 使用方法_c++ lockguard 手动开锁-CSDN博客 ","date":"2024-08-23","objectID":"/posts/cmu445_p0/:1:0","tags":["CMU15-445","DB"],"title":"CMU15-445 Fall 2023 Project 0","uri":"/posts/cmu445_p0/"},{"categories":["DB"],"content":"Task1 Copy-On-Write Trie trie字典树这个数据结构不过多介绍，Copy-On-Write即写时复制，用在字典树上是指在进行写操作时，原有树形或数据保持不变（这样还能处理同时进行的读，便于并发操作），对需要修改的节点进行拷贝和修改。 代码主要在trie.cpp​中，要求实现Get(key)​、Put(key, value)​、Delete(key)​三个接口。 Get过程和原始trie一致： 获取根节点root_​ 遍历key​的路径，期间如果节点不存在则返回nullptr​ 如果满足key​的节点不为空且is_value_node_​，通过dynamic_cast\u003cconst TrieNodeWithValue\u003cT\u003e *\u003e​强转为带值节点指针，如果转换成功，返回value_.get()​ 返回nullptr​ Put和Remove的实现则相对较难，需要仔细阅读文档理解Copy-On-Write Trie进行Put、Remove的过程。另外需要注意TrieNode​类的定义中，children_​成员指向的子节点类型都是const TrieNode​，所以一个写入操作需要自底向上构建。 // A TrieNode is a node in a Trie. class TrieNode { public: TrieNode() = default; explicit TrieNode(std::map\u003cchar, std::shared_ptr\u003cconst TrieNode\u003e\u003e children) : children_(std::move(children)) {} virtual ~TrieNode() = default; virtual auto Clone() const -\u003e std::unique_ptr\u003cTrieNode\u003e { return std::make_unique\u003cTrieNode\u003e(children_); } std::map\u003cchar, std::shared_ptr\u003cconst TrieNode\u003e\u003e children_; bool is_value_node_{false}; }; ","date":"2024-08-23","objectID":"/posts/cmu445_p0/:2:0","tags":["CMU15-445","DB"],"title":"CMU15-445 Fall 2023 Project 0","uri":"/posts/cmu445_p0/"},{"categories":["DB"],"content":"Put 首先自底向上，创建不存在的TrieNode​（或 TrieNodeWithValue​） 如果TrieNode​已存在，进行拷贝，将下一层的新节点进行进行插入或覆盖，即 children_[c]=newNode​ 返回Trie(new_root)​ 建议采用递归实现，递归部分伪代码如下 void DiguiPut(new_ptr, key, val) { bool flag_find = false; auto iter = new_ptr-\u003echildren_.find(key.at(0)); // if found if (iter != new_ptr-\u003echildren_.end()) { flag_find = true; // when remian key size \u003e 1, continue to digui if (key.size() \u003e 1 ) { // (1) clone // (2) digui_put // (3) update children of new_ptr auto node = iter-\u003esecond-\u003eClone(); auto node_ptr = shared_ptr(std::move(node)); DiguiPut(node_ptr, key[1:], std::move(val)); update children of new_ptr; (iter-\u003esecond = node_ptr) } else { // now key size == 1, means the last element of the key // only need to create a new value node create a new value node with val; update children of new_ptr; } return; } // if not found, need to create more new empty nodes if (!find_flag) { if (key.size() \u003e 1) { create empty node and node_ptr; DiguiPut(node_ptr, key[1:], std::move(val)); update children of new_ptr; } else { // now key.size() == 1, only need to create a new value node create a value_node and val_ptr; update children of new_ptr; } } } ","date":"2024-08-23","objectID":"/posts/cmu445_p0/:2:1","tags":["CMU15-445","DB"],"title":"CMU15-445 Fall 2023 Project 0","uri":"/posts/cmu445_p0/"},{"categories":["DB"],"content":"Remove Remove的过程与Put相近 首先自底向上，找到需要删除的TrieNodeWithValue​（如果是TrieNode​或找不到，说明不需要做任何事） 如果有孩子节点，转为TrieNode​保留 如果没有孩子节点，说明需要删除 自底向上对路径上的节点进行拷贝，对下一层节点进行修改或删除 返回Trie(new_root)​ ","date":"2024-08-23","objectID":"/posts/cmu445_p0/:2:2","tags":["CMU15-445","DB"],"title":"CMU15-445 Fall 2023 Project 0","uri":"/posts/cmu445_p0/"},{"categories":["DB"],"content":"Task2 Concurrent Key-Value Store 任务2需要对1中实现的Copy-On-Write Trie进行封装，实现可并发的kv存储，支持多读一写。 这一部分代码较为简单，锻炼锁的使用。 ","date":"2024-08-23","objectID":"/posts/cmu445_p0/:3:0","tags":["CMU15-445","DB"],"title":"CMU15-445 Fall 2023 Project 0","uri":"/posts/cmu445_p0/"},{"categories":["DB"],"content":"Get Get操作只需要在访问root_​时加锁，获取root_​之后即可解锁 对root_lock_​加unique_lock​，获取root_​到临时变量root​，解锁 调用root.Get\u003cT\u003e(key)​，获取val​ 如果val != nullptr​，返回一个ValueGuard​，否则返回一个std::nullopt​ ​std::optional​管理一个可选﻿的容纳值，既可以存在也可以不存在的值。方便后续对返回std::nullopt的处理。 ","date":"2024-08-23","objectID":"/posts/cmu445_p0/:3:1","tags":["CMU15-445","DB"],"title":"CMU15-445 Fall 2023 Project 0","uri":"/posts/cmu445_p0/"},{"categories":["DB"],"content":"Put 写操作之间是冲突的，所以需要加一把大锁write_lock_​ 对write_lock_​加​unique_lock​ ​new_trie = root_.Put\u003cT\u003e(key, std::move(value));​ 对root_lock_​加锁，修改root_ = new_trie​ 解锁root_lock_​和write_lock_​ 这里调用root_.Put\u003cT\u003e(key, std::move(value));​时并未对root_lock_​加锁，是因为加了write_lock_​，同一时间不存在对root_​的修改操作。并且是写时拷贝，不会对原始Trie进行修改。 之后获取访问锁root_lock_​，此时需要更新trie，所以需要避免其他线程读。 对Get访问root_加锁的根本原因就是避免Put或Remove在同一时刻修改root_，造成错误。 Remove与Put一致。 由于使用的都是shared_ptr​，调用put​、remove​返回新的root节点并完成赋值以后，旧的根节点指针在引用计数变为0后就会自动释放，进而它的所有子节点引用计数也会减一，对应put、remove的key，key路径上的所有旧节点都会被自动释放。 官方给的测试步骤如下： $ cd build $ make trie_test trie_store_test -j$(nproc) $ make trie_noncopy_test trie_store_noncopy_test -j$(nproc) $ ./test/trie_test $ ./test/trie_noncopy_test $ ./test/trie_store_test $ ./test/trie_store_noncopy_test ","date":"2024-08-23","objectID":"/posts/cmu445_p0/:3:2","tags":["CMU15-445","DB"],"title":"CMU15-445 Fall 2023 Project 0","uri":"/posts/cmu445_p0/"},{"categories":["DB"],"content":"Task3 Debugging 我们需要进行调试的是trie_debug_test.cpp​。我的环境配置VScode + clangd + codeLLDB debug步骤： $ cd build $ make trie_debug_test 编译完成后，根据trie_debug_test.cpp​中的提示设置断点，开启F5调试，起初会报错显示没有配置文件，会自动在.vscode​目录下创建launch.json​，我们需要将program​字段修改成\"${workspaceFolder}/build/test/trie_debug_test\"​（对照上面的测试脚本./test/trie_test​） { // 使用 IntelliSense 了解相关属性。 // 悬停以查看现有属性的描述。 // 欲了解更多信息，请访问: https://go.microsoft.com/fwlink/?linkid=830387 \"version\": \"0.2.0\", \"configurations\": [ { \"type\": \"lldb\", \"request\": \"launch\", \"name\": \"Debug\", \"program\": \"${workspaceFolder}/build/test/trie_debug_test\", \"args\": [], \"cwd\": \"${workspaceFolder}\" } ] } 然后在题目指定的位置加上断点，再次点击F5，就可以开始调试啦。 实在不行可以在测试代码里面打印结果。 ","date":"2024-08-23","objectID":"/posts/cmu445_p0/:4:0","tags":["CMU15-445","DB"],"title":"CMU15-445 Fall 2023 Project 0","uri":"/posts/cmu445_p0/"},{"categories":["DB"],"content":"Task4 SQL String ​string_expression.h​：Compute​实现大小写转换，直接用std::tolower​和std::toupper​ ​plan_func_call.cpp​：Planner::GetFuncCallFromFactory​，按照注释要求来写即可 // NOLINTNEXTLINE auto Planner::GetFuncCallFromFactory(const std::string \u0026func_name, std::vector\u003cAbstractExpressionRef\u003e args) -\u003e AbstractExpressionRef { // 1. check if the parsed function name is \"lower\" or \"upper\". // 2. verify the number of args (should be 1), refer to the test cases for when you should throw an `Exception`. // 3. return a `StringExpression` std::shared_ptr. throw Exception(fmt::format(\"func call {} not supported in planner yet\", func_name)); } ","date":"2024-08-23","objectID":"/posts/cmu445_p0/:5:0","tags":["CMU15-445","DB"],"title":"CMU15-445 Fall 2023 Project 0","uri":"/posts/cmu445_p0/"},{"categories":["DB"],"content":"风格、内存泄漏检查和提交Grade $ make format $ make check-clang-tidy-p0 内存泄漏检查会在Debug模式下运行程序时自动执行，如果有内存泄漏，会有错误的报告 cmake -DCMAKE_BUILD_TYPE=Debug .. 代码提交到Gradescope： 在build目录下运行以下代码，得到project0-submission.zip make submit-p0 cd .. python3 gradescope_sign.py 最后一步Gradscope这个网站的一些要求，要签个名啥的，之后就可以上传到Gradescope了。 ","date":"2024-08-23","objectID":"/posts/cmu445_p0/:6:0","tags":["CMU15-445","DB"],"title":"CMU15-445 Fall 2023 Project 0","uri":"/posts/cmu445_p0/"},{"categories":["DB"],"content":"作为一名新接触存储领域的学生，我在自学了tinykv之后，感觉自己的基础实在是过于薄弱，对于常见的数据库组件，诸如缓存、索引、事务引擎等，都几乎毫无了解，又感到无从抓起。前段时间刚好在CS自学指南 (csdiy.wiki)看到CMU15-445这门课，它的project里面所涉及的缓存管理、索引、算子与优化器和并发控制，正是我当前迫切需要的知识。于是前前后后花了两个月左右的时间完成了所有的project，同样是不希望把工夫浪费了，因此写个Blog记录一下。 Semester Project 0 Project 1 Project 2 Project 3 Project 4 Fall 2017 / Buffer Pool Manager B+ Tree Index Two-Phase Locking + Concurrent B+ Tree Index Logging and Recovery Fall 2018 / Buffer Pool Manager B+ Tree Index Two-Phase Locking + Deadlock Prevention/Detection Logging and Recovery Fall 2019 / Buffer Pool Manager Hash Index Query Execution Logging and Recovery Fall 2020 / Buffer Pool Manager Hash Index Query Execution 2PL Concurrency Control + Deadlock Detection Fall 2021 / Buffer Pool Manager Hash Index Query Execution 2PL Concurrency Control + Deadlock Prevention Fall 2022 Trie Buffer Pool Manager B+ Tree Index Query Execution + Optimization Hierarchy 2PL Concurrency Control + Deadlock Detection Spring 2023 Copy-on-write Trie Buffer Pool Manager B+ Tree Index Query Execution + Optimization, Hierarchy 2PL Concurrency Control + Deadlock Detection Fall 2023 Copy-on-write Trie Buffer Pool Manager Hash Index Query Execution + Optimization, Multi-Version Concurrency Control CMU15-445有多个不同的版本，课程工作人员每学期都会尝试轮换一些内容，例如索引这一块有的是B+树有的是哈希索引。2023 Fall这一版引入了更多的新东西，特别是多版本并发控制，并且在平衡课程难度上花了不少工夫（项目难度是递增的，在以往的版本中听说B+树通常是最困难的），学生能够获得渐进和流畅的学习体验。 Project 0的内容是实现一个简单的字典树，目的是让学生提前熟悉一下后续项目要用到的一些c++特性，例如智能指针，锁管理，右值引用，强制类型转换等，打下良好的基础。 Project 1要求实现缓存管理，包括LRU-K淘汰算法、磁盘io调度器、缓存池管理接口，内容比较简单。 Project 2要在Project 1的基础上做，要求实现一个Extendible Hash Table，我认为难点主要是对该数据结构的理解以及处理一些边界条件，整体也不难。 Project 3与Project 1和2是解耦的，不过依赖与它们的正确性，内容实现基于火山模型的查询算子，例如scan、insert、aggregation等等。整体比较难，倒不在于实现方面，而在于需要阅读并理解bustub提供的很多框架代码和一些样例算子的实现，起步会非常艰难。 Project 4则更难，要在Project 3的基础上重构很多算子，实现事务管理和Hyper MVCC，前80分相对容易获得，只要通过一个并发测试；后面的20分和奖励任务要考虑更多的并发测试和隐藏条件，还要再重构一次算子，很麻烦并且难度也很大。 ‍ ","date":"2024-08-18","objectID":"/posts/cmu445_start/:0:0","tags":["CMU15-445","DB"],"title":"CMU15-445 Fall 2023 入门","uri":"/posts/cmu445_start/"},{"categories":["DB"],"content":"资源链接 官方project文档（非常建议慢慢地阅读英文文档，访问应该要用魔法） Assignments | CMU 15-445/645 :: Intro to Database Systems (Fall 2023) 代码测试/提交网站Grade：课程Entry Code: KK5DVJ 15-445/645 (Non-CMU) Dashboard | Gradescope 本来还想找一些CMU445的课程中文讲义搭配project食用，找了几个发现都非常零碎，没办法获得清晰成体系的知识。 这里留个坑，有时间好好学习和整理存储的相关知识。 ‍ ","date":"2024-08-18","objectID":"/posts/cmu445_start/:1:0","tags":["CMU15-445","DB"],"title":"CMU15-445 Fall 2023 入门","uri":"/posts/cmu445_start/"},{"categories":["DB"],"content":"环境配置 ​git clone --branch v20231227-2023fall https://github.com/cmu-db/bustub.git​ 官方文档提供了VSCode和Clion的环境配置以及调试的指导，已经足够了。 If you are using VSCode, we recommend you to install CMake Tools, C/C++ Extension Pack and clangd. After that, follow this tutorial to learn how to use the visual debugger in VSCode: Debug a C++ project in VS Code. If you are using CLion, we recommend you to follow this tutorial: CLion Debugger Fundamentals. 官方仓库同样也提供了一键配置运行软件环境的脚本，运行一下命令即可。 ​sudo build_support/packages.sh​ 另外，项目文档中都有提供编译和测试的指导，照着来就行。 ‍ ","date":"2024-08-18","objectID":"/posts/cmu445_start/:2:0","tags":["CMU15-445","DB"],"title":"CMU15-445 Fall 2023 入门","uri":"/posts/cmu445_start/"},{"categories":["DB"],"content":"通过情况 完成了所有的任务包括p4的两个奖励任务，下面是在所有任务基础上的total-qps排名24（在所有通过奖励关的人中排倒数第3哈哈哈），最后实在是懒得优化了。 ​ rank ​ 瞄了一眼除了奖励关都通过的人数，大概60人左右，估计之前版本的也得有100多了，感觉也许还真有点烂大街了罢。 ​ pass ​ ‍ ","date":"2024-08-18","objectID":"/posts/cmu445_start/:3:0","tags":["CMU15-445","DB"],"title":"CMU15-445 Fall 2023 入门","uri":"/posts/cmu445_start/"},{"categories":["kv"],"content":"参考资料 Project 4 通过建立一个事务系统实现多版本并发控制 MVCC。在编码之前，需要对事务的相关概念提前了解（事务的属性，事务隔离级别等）。 文档可以参考以下翻译（细节部分需要看英文文档）： 实验指导书（翻译）Project 4: Transactions_从实验六中的people-CSDN博客 TinyKV的事务设计遵循 Percolator 模型，它本质上是一个两阶段提交协议（2PC）。如果看完文档还是云里雾里，可以通过这篇文章加深对 Percolator 的理解： Percolator模型及其在TiKV中的实现-腾讯云开发者社区-腾讯云 (tencent.com) ","date":"2024-08-15","objectID":"/posts/tinykv_p4/:1:0","tags":["raft","kv","tinykv"],"title":"TinyKV Project 4","uri":"/posts/tinykv_p4/"},{"categories":["kv"],"content":"Part A Part A主要实现 MVCC 。基于lock、write、default三个列族，封装MvccTxn​的 API，代码主要在transaction.go​中，里面提供了一些用于对key进行编解码的辅助函数。 type MvccTxn struct { StartTS uint64 Reader storage.StorageReader writes []storage.Modify } 事务所有的写入操作都存入writes当中，便于被一次性写到底层数据库，保障原子性。因此PutWrite​、PutLock​、DeleteLock​、PutValue​和DeleteValue​都是将 key、cf、value（delete不包含）append到writes​中。需要注意的是不同cf对应的key的构成： lock：CfLock + Key write：CfWrite + Key + CommitTs default：CfDefault + Key + StartTs val的构成也不同： type Lock struct { Primary []byte Ts uint64 Ttl uint64 Kind WriteKind } type Write struct { StartTS uint64 Kind WriteKind } // default value value []byte ","date":"2024-08-15","objectID":"/posts/tinykv_p4/:2:0","tags":["raft","kv","tinykv"],"title":"TinyKV Project 4","uri":"/posts/tinykv_p4/"},{"categories":["kv"],"content":"GetValue 查询当前事务下，传入 key 对应的 Value。 通过 iter.Seek(EncodeKey(key, txn.StartTS))​ 查找遍历 Write，找到满足 commitTs \u003c= ts 的最新 write； 判断该 write 的 userkey 与当前 key相同，如果不是，说明不存在，直接返回； 判断该 write 的Kind​是不是WriteKindPut​，如果不是，说明不存在，直接返回； 从 Default 中通过 EncodeKey(key, write.StartTS) 获取目标 Value； ","date":"2024-08-15","objectID":"/posts/tinykv_p4/:2:1","tags":["raft","kv","tinykv"],"title":"TinyKV Project 4","uri":"/posts/tinykv_p4/"},{"categories":["kv"],"content":"CurrentWrite 查询当前事务下，传入 key 的最新 write。 通过 iter.Seek(EncodeKey(key, ^uint64(0)))​查询该 key 的最新 write； 如果​ write.StartTS \u003e txn.StartTS​，继续遍历，直到找到 write.StartTS == txn.StartTS​ 的 write； 返回这个 write 和 它的 commitTs； ","date":"2024-08-15","objectID":"/posts/tinykv_p4/:2:2","tags":["raft","kv","tinykv"],"title":"TinyKV Project 4","uri":"/posts/tinykv_p4/"},{"categories":["kv"],"content":"​​MostRecentWrite​ 查询传入 key 的最新 write，不用考虑当前事务的开始时间戳。 ","date":"2024-08-15","objectID":"/posts/tinykv_p4/:2:3","tags":["raft","kv","tinykv"],"title":"TinyKV Project 4","uri":"/posts/tinykv_p4/"},{"categories":["kv"],"content":"Part B Part B主要利用 A 中封装的 Mvcc API 实现 Percolator 模型的事务性 API，包括KvGet​、KvPrewrite​和KvCommit​，分别对应读写过程。 ","date":"2024-08-15","objectID":"/posts/tinykv_p4/:3:0","tags":["raft","kv","tinykv"],"title":"TinyKV Project 4","uri":"/posts/tinykv_p4/"},{"categories":["kv"],"content":"KvGet 对应 Percolator 的读过程： 获取一个时间戳ts（req.GetVersion​），并创建一个 MvccTxn​； 查询当前我们要读取的 key 上是否存在一个时间戳在[0, ts]范围内的锁。 如果存在一个时间戳在[0, ts]范围的锁，那么意味着当前的数据被一个比当前事务更早启动的事务锁定了，但是当前这个事务还没有提交。此时直接返回Locked​错误。 如果没有锁，或者锁的时间戳大于ts，那么读请求可以被满足。 从write列族中查询在[0, ts]范围内的最大 commit_ts 的记录，然后依此获取到对应的start_ts。 根据上一步获取的start_ts，从data列获取对应的记录（3，4两步已经被封装在 GetValue​当中）； 如果发生RegionError​或者未找到要返回对应的错误。 ","date":"2024-08-15","objectID":"/posts/tinykv_p4/:3:1","tags":["raft","kv","tinykv"],"title":"TinyKV Project 4","uri":"/posts/tinykv_p4/"},{"categories":["kv"],"content":"KvPrewrite 和 KvCommit 对应 2PC 的 Prewrite 和 Commit两个阶段。 在Prewrite阶段： 利用req.GetStartVersion()​创建一个 Mvcc txn​； 遍历req.Mutations​，对于其中的每一个key​： 利用txn.MostRecentWrite​检查是否一个有一个比startTs​更大的新write，如果存在，则append一个WriteConflict​错误到​keyErrors​； 利用txn.GetLock​检查是否已经存在一个lock，如果存在，则append一个Locked​错误到​keyErrors​； 根据WriteKind​调用txn.PutValue​或txn.DeleteValue​； 利用txn.PutLock​写入该key的lock。 如果len(keyErrors) \u003e 0​，作为响应返回； 最后利用server.storage.Write(req.Context, txn.Writes())​一次性写到底层数据库 在Commit阶段： 利用req.GetStartVersion()​创建一个 Mvcc txn​； ​server.Latches.WaitForLatches(req.Keys)​锁定 Commit 请求中涉及的所有key，避免局部竞争条件； 遍历请求中的所有key​： 利用txn.CurrentWrite(key)​检查是否有重复的提交，如果是重复提交则返回； 利用txn.GetLock(key)​检查是否 lock 依然存在，如果lock为空或者lock的时间戳与当前请求的StartVersion​不相等，说明该事务已经被其他事务清理，返回一个Retryable​错误； ​txn.PutWrite​，txn.DeleteLock​ ​server.storage.Write(req.Context, txn.Writes())​一次性写到底层数据库，完成提交。 ","date":"2024-08-15","objectID":"/posts/tinykv_p4/:3:2","tags":["raft","kv","tinykv"],"title":"TinyKV Project 4","uri":"/posts/tinykv_p4/"},{"categories":["kv"],"content":"Part C ","date":"2024-08-15","objectID":"/posts/tinykv_p4/:4:0","tags":["raft","kv","tinykv"],"title":"TinyKV Project 4","uri":"/posts/tinykv_p4/"},{"categories":["kv"],"content":"KvScan 扫描操作需要利用mvcc.NewScanner(req.StartKey, txn)​创建一个scanner​来进行迭代，对于迭代过程中每个key的处理，都是与KvGet​中类似的，需要检查一下lock的情况；如果某个key出现错误，不能中止迭代，将错误信息append到最终的结果当中即可。 ","date":"2024-08-15","objectID":"/posts/tinykv_p4/:4:1","tags":["raft","kv","tinykv"],"title":"TinyKV Project 4","uri":"/posts/tinykv_p4/"},{"categories":["kv"],"content":"​KvCheckTxnStatus​​ 用于 Client failure 后，想继续执行时，先检查 Primary Key 的状态，以此决定是回滚还是继续推进 commit。 通过txn.CurrentWrite(req.PrimaryKey)​获取 primary key 的write​，如果write​且不是WriteKindRollback​，则说明已经被commit了，直接返回 commitTs；如果是WriteKindRollback​说明已经被回滚，因此无需操作，返回Action_NoAction​即可。 通过txn.GetLock(req.PrimaryKey)​获取 primary key 的lock​；如果lock == nil​，打上 rollback 标记（WriteKindRollback​），返回 Action_LockNotExistRollback​； 如果lock​存在，并且超时了。那么删除这个lock​，并且删除对应的Value​，同时打上rollback标记，然后返回Action_TTLExpireRollback​； 如果以上条件均为发生，说明write​不存在，且lock​存在但并没有超时，直接返回，事务继续执行。 根据文档，检查lock超时要用到mvcc.PhysicalTime​，方法如下 func isLockTimeout(lock *mvcc.Lock, currentTs uint64) bool { return mvcc.PhysicalTime(lock.Ts)+lock.Ttl \u003c= mvcc.PhysicalTime(currentTs) } ","date":"2024-08-15","objectID":"/posts/tinykv_p4/:4:2","tags":["raft","kv","tinykv"],"title":"TinyKV Project 4","uri":"/posts/tinykv_p4/"},{"categories":["kv"],"content":"​​KvBatchRollback​​ 回滚操作与Commit逻辑相反，执行流程是相似的： ​server.Latches.WaitForLatches(req.Keys)​锁定本次回滚操作涉及的所有key 遍历所有key ​txn.CurrentWrite(key)​获取当前事务下的write​，如果不为空，若write.Kind == mvcc.WriteKindRollback​，说明该key​已经回滚，continue，否则说明事务已提交，直接返回一个Abort​响应； 利用txn.GetLock(k)​获取lock​，若lock​为空或者不是本事务的，则直接打上 rollback 标记（WriteKindRollback​）并continue；若本事务还持有lock​，则要依次txn.DeleteLock​、txn.DeleteValue​、打上 rollback 标记。 ​server.storage.Write(req.Context, txn.Writes())​一次性写到底层数据库，完成rollback。 ","date":"2024-08-15","objectID":"/posts/tinykv_p4/:4:3","tags":["raft","kv","tinykv"],"title":"TinyKV Project 4","uri":"/posts/tinykv_p4/"},{"categories":["kv"],"content":"​​KvResolveLock​ 这个方法主要用于解决锁冲突，当客户端已经通过KvCheckTxnStatus()​检查了 primary key 的状态，要么全部回滚，要么全部提交，具体取决于 ResolveLockRequest​的CommitVersion​。 首先遍历CfLock​，找到当前事务下的所有现存lock，将对应的key加入集合keys​，若len(keys) == 0​，直接返回； 若req.CommitVersion == 0​，调用KvBatchRollback​全部回滚；否则，调用KvCommit​全部提交 ‍ ","date":"2024-08-15","objectID":"/posts/tinykv_p4/:4:4","tags":["raft","kv","tinykv"],"title":"TinyKV Project 4","uri":"/posts/tinykv_p4/"},{"categories":["kv"],"content":"3c要求实现region balance调度器，文档给了详细的算法步骤，相对比较容易。 ","date":"2024-08-14","objectID":"/posts/tinykv_p3c/:0:0","tags":["raft","kv","tinykv"],"title":"TinyKV Project 3C","uri":"/posts/tinykv_p3c/"},{"categories":["kv"],"content":"processRegionHeartbeat 每个region都会周期性的发送心跳给调度器，调度器需要首先检查RegionEpoch是否是最新的，如果是则进行更新，否则忽略。 检查的逻辑是： 如果该心跳对应的region id在调度器中存在，检查心跳中的RegionEpoch是否过时，如果过时则直接返回； 如果该心跳对应的region id在调度器中找不到，扫描调度器中所有与心跳region有重叠的Regions。同样的方法对比RegionEpoch，如果Regions中存在一个region比心跳region新，那么就是过时的。 ","date":"2024-08-14","objectID":"/posts/tinykv_p3c/:1:0","tags":["raft","kv","tinykv"],"title":"TinyKV Project 3C","uri":"/posts/tinykv_p3c/"},{"categories":["kv"],"content":"Schedule region balance调度器目标是让集群中的stores所负载的region数目趋于平衡。一个调度命令通常就是将某个region从一个store移动到另一个store，因此Schedule的逻辑是先找到合适的region和目标store，然后创建一个MovePeerOperator​。详细算法官方文档给的很详细，具体步骤如下： 获取所有“合适”的store并根据RegionSize排序 “合适”的条件：store.IsUp() \u0026\u0026 store.DownTime() \u003c maxStoreDownTime 遍历第一步获取的suitableStores（从RegionSize最大的开始），找到最适合在 store 中移动的 region GetPendingRegionsWithLock：尝试选择一个挂起的region GetFollowersWithLock：尝试选择一个follower region GetLeadersWithLock：尝试选择一个leader region 若成功选择了一个要移动的 region，选择RegionSize最小的一个 store 作为目标 检查此次移动是否有价值： 判断条件：如果fromStore.GetRegionSize()-toStore.GetRegionSize() \u003c 2*region.GetApproximateSize()，表明两个store负载的region大小相近，此次移动没有必要 CreateMovePeerOperator ","date":"2024-08-14","objectID":"/posts/tinykv_p3c/:2:0","tags":["raft","kv","tinykv"],"title":"TinyKV Project 3C","uri":"/posts/tinykv_p3c/"},{"categories":["kv"],"content":"我在做tinykv这个课程的过程中，3b之前遇到的问题大多能在网上找到解决方法，因此也没怎么做记录；而3b由于引入了Multi Raft以及集群成员变更、Split Region等Admin命令，对很多细节处理要求严格，并且测试点会模拟丢包和网络隔离等异常情况，需要做较多额外的针对性处理，因此我在做的时候遇到了许多网上难以找到的问题，通过在海量日志中一步步跟踪才解决了它们。 下面将对我在3b中遇到的疑难杂症及解决方法进行汇总。 ","date":"2024-08-14","objectID":"/posts/tinykv_p3b_problems/:0:0","tags":["raft","kv","tinykv"],"title":"TinyKV Project 3B 疑难杂症","uri":"/posts/tinykv_p3b_problems/"},{"categories":["kv"],"content":"SnapShot相关 ","date":"2024-08-14","objectID":"/posts/tinykv_p3b_problems/:1:0","tags":["raft","kv","tinykv"],"title":"TinyKV Project 3B 疑难杂症","uri":"/posts/tinykv_p3b_problems/"},{"categories":["kv"],"content":"（1）Request Timeout-1 问题原因：当前节点有snapshot正在被应用时，收到新的snapshot没有过滤掉 解决方法：当r.RaftLog.pendingSnapshot != nil​时不接受新的snapShot，并且不处理Append​消息。 func (r *Raft) handleAppendEntries(m pb.Message) { if r.RaftLog.pendingSnapshot != nil { return } ... func (r *Raft) handleSnapshot(m pb.Message) { if r.RaftLog.pendingSnapshot != nil { return } ... ","date":"2024-08-14","objectID":"/posts/tinykv_p3b_problems/:1:1","tags":["raft","kv","tinykv"],"title":"TinyKV Project 3B 疑难杂症","uri":"/posts/tinykv_p3b_problems/"},{"categories":["kv"],"content":"（2）Request Timeout-2 问题原因：follower在处理append消息的时候，prevLogIndex​的条目在follower已经被Compact掉了（Leader当中反而没有被Compact，怪了），此时通过RaftLog.Term()​会返回错误，无法完成日志匹配动作，导致leader中记录的Match会保持不变，在集群中只有两个节点的情况下committed无法推进。 解决方法：follower返回响应，令leader主动发送一个Snapshot给自己，我这里利用了MsgAppendResponse​中没有使用的commit​字段。 const ShouldSendSnapByCommit uint64 = 123456 func (r *Raft) handleAppendEntries(m pb.Message) { 。。。 tTerm, terr := r.RaftLog.Term(m.Index) // debug lastTerm, _ := r.RaftLog.Term(r.RaftLog.LastIndex()) if terr != nil { log.DPrintfRaft(\"node %d handleAppendEntries from %d failed, get term failed: %v\\n\", r.id, m.From, terr) log.DPrintfRaft(\"l.entries: %d, i: %d, l.dummyIndex: %d, l.lastIndex: %d\\n\", len(r.RaftLog.entries), m.Index, r.RaftLog.dummyIndex, r.RaftLog.LastIndex()) // tell leader should send snapshot r.msgs = append(r.msgs, pb.Message{ MsgType: pb.MessageType_MsgAppendResponse, To: m.From, From: r.id, Term: lastTerm, Index: 0, Reject: true, Commit: ShouldSendSnapByCommit, }) return 。。。 } func (r *Raft) handleAppendResponse(m pb.Message) { if m.Term \u003e r.Term { r.greaterTermRspNum += 1 if r.greaterTermRspNum \u003e GreaterTermRspThreshold { r.greaterTermRspNum = 0 r.becomeFollower(m.Term, None) return } // r.becomeFollower(m.Term, None) return } // special handle for need to send snapshot if m.Commit == ShouldSendSnapByCommit { r.sendSnapshot(m.From) return } 。。。 } ‍ ","date":"2024-08-14","objectID":"/posts/tinykv_p3b_problems/:1:2","tags":["raft","kv","tinykv"],"title":"TinyKV Project 3B 疑难杂症","uri":"/posts/tinykv_p3b_problems/"},{"categories":["kv"],"content":"TransferLeader相关 ","date":"2024-08-14","objectID":"/posts/tinykv_p3b_problems/:2:0","tags":["raft","kv","tinykv"],"title":"TinyKV Project 3B 疑难杂症","uri":"/posts/tinykv_p3b_problems/"},{"categories":["kv"],"content":"（1）Request Timeout（选举冲突的问题） [RaftStore]: node[[region 1] 2] propose common request: header:\u003cregion_id:1 peer:\u003cid:2 store_id:2 \u003e region_epoch:\u003cconf_ver:4 version:1 \u003e \u003e requests:\u003ccmd_type:Put put:\u003ccf:\"default\" key:\"2 00000053\" value:\"x 2 53 y\" \u003e \u003e , leader: 2 2024/02/22 07:53:32 [Raft]: node 2 is leader, but leadTransferee is not None, stop accept propose 2024/02/22 07:53:32 [Raft]: node 2 tryUpdateCommit: len(r.Prs) = 2, r.RaftLog.LastIndex() = 584, r.committed = 583 2024/02/22 07:53:32 [Raft]: node 2 transfer leader to 1 success! sendTimeoutNow! 2024/02/22 07:53:32 [RaftStore]: ------------------------------------------------------------------------- 2024/02/22 07:53:32 [RaftStore]: node[[region 1] 2] propose common request: header:\u003cregion_id:1 peer:\u003cid:2 store_id:2 \u003e region_epoch:\u003cconf_ver:4 version:1 \u003e \u003e requests:\u003ccmd_type:Snap snap:\u003c\u003e \u003e , leader: 2 2024/02/22 07:53:32 [Raft]: node 2 tryUpdateCommit: len(r.Prs) = 2, r.RaftLog.LastIndex() = 585, r.committed = 584 2024/02/22 07:53:32.433521 cluster.go:200: \u001b[0;37m[info] [Cluster] request err resp. err: message:\"raft proposal dropped\" , req: header:\u003cregion_id:1 peer:\u003cid:2 store_id:2 \u003e region_epoch:\u003cconf_ver:4 version:1 \u003e \u003e requests:\u003ccmd_type:Put put:\u003ccf:\"default\" key:\"1 00000062\" value:\"x 1 62 y\" \u003e \u003e \u001b[0m 2024/02/22 07:53:32.433521 cluster.go:200: \u001b[0;37m[info] [Cluster] request err resp. err: message:\"raft proposal dropped\" , req: header:\u003cregion_id:1 peer:\u003cid:2 store_id:2 \u003e region_epoch:\u003cconf_ver:4 version:1 \u003e \u003e requests:\u003ccmd_type:Put put:\u003ccf:\"default\" key:\"4 00000046\" value:\"x 4 46 y\" \u003e \u003e \u001b[0m 2024/02/22 07:53:32.433564 cluster.go:200: \u001b[0;37m[info] [Cluster] request err resp. err: message:\"raft proposal dropped\" , req: header:\u003cregion_id:1 peer:\u003cid:2 store_id:2 \u003e region_epoch:\u003cconf_ver:4 version:1 \u003e \u003e requests:\u003ccmd_type:Put put:\u003ccf:\"default\" key:\"2 00000053\" value:\"x 2 53 y\" \u003e \u003e \u001b[0m 2024/02/22 07:53:32 [RaftStore]: node[[region 1] 2]: process common request: header:\u003cregion_id:1 peer:\u003cid:2 store_id:2 \u003e region_epoch:\u003cconf_ver:4 version:1 \u003e \u003e requests:\u003ccmd_type:Put put:\u003ccf:\"default\" key:\"0 00000057\" value:\"x 0 57 y\" \u003e \u003e 2024/02/22 07:53:32 [Raft]: node 1 handleAppendEntries from 2, lastIndex: 584,preLogIndex: 583, preLogTerm: 6, len(m.Entries): 1 2024/02/22 07:53:32 [Raft]: node 1 handleAppendEntries from 2 success, conflictFlag: true 2024/02/22 07:53:32 [Raft]: node 1 handleAppendEntries from 2, lastIndex: 584,preLogIndex: 583, preLogTerm: 6, len(m.Entries): 1 2024/02/22 07:53:32 [Raft]: node 1 handleAppendEntries from 2 success, conflictFlag: true 2024/02/22 07:53:32 [Raft]: node 1 handleAppendEntries from 2, lastIndex: 584,preLogIndex: 584, preLogTerm: 6, len(m.Entries): 0 2024/02/22 07:53:32 [Raft]: node 1 handleAppendEntries from 2 success, conflictFlag: true 可以看到，当Node 2sendTimeOutNow​给1之后，由于还是leader，且leaderTransferee == None​，能够propose新的entry，导致node 1发起选举之后，没办法得到选票，没办法当选，从而造成冲突。 解决方法：leader在sendTimeoutNow​之后主动变为follower，从而不会接收到propose产生新的entry，目标节点能够顺利当选。 ‍ ","date":"2024-08-14","objectID":"/posts/tinykv_p3b_problems/:2:1","tags":["raft","kv","tinykv"],"title":"TinyKV Project 3B 疑难杂症","uri":"/posts/tinykv_p3b_problems/"},{"categories":["kv"],"content":"（2）TransferLeader的超时机制 如果领导权转移的目标节点失联，会导致leader一直处于leadTransferee的状态，没办法propose entry，因此要为TransferLeader增加超时机制，我直接使用选举超时作为领导权转移的超时时间。TransferLeader成功后becomefollower()​会完成清零选举超时的工作。 func (r *Raft) tickLeader() { // special handle with leadTransforee if r.leadTransferee != None { r.electionElapsed += 1 // can't transfer success before a ElectTimeout, give up if r.electionElapsed \u003e= r.randElectTimeout { r.leadTransferee = None r.electionElapsed = 0 r.randElectTimeout = r.electionTimeout + rand.Intn(r.electionTimeout) } } ... ‍ ‍ ","date":"2024-08-14","objectID":"/posts/tinykv_p3b_problems/:2:2","tags":["raft","kv","tinykv"],"title":"TinyKV Project 3B 疑难杂症","uri":"/posts/tinykv_p3b_problems/"},{"categories":["kv"],"content":"confChange相关 ","date":"2024-08-14","objectID":"/posts/tinykv_p3b_problems/:3:0","tags":["raft","kv","tinykv"],"title":"TinyKV Project 3B 疑难杂症","uri":"/posts/tinykv_p3b_problems/"},{"categories":["kv"],"content":"（1）无法正确处理confChange命令 --- FAIL: TestBasicConfChange3B (6.34s) panic: have peer: id:2 store_id:2 [recovered] panic: have peer: id:2 store_id:2 错误原因：ConfChange条目中的数据不正确 Solution：在Propose中，针对 ConfChange 请求的序列化和反序列化方式与其他请求不同 ctx, marErr := msg.Marshal() if marErr != nil { panic(marErr) } err := d.RaftGroup.ProposeConfChange(eraftpb.ConfChange{ ChangeType: msg.AdminRequest.ChangePeer.ChangeType, NodeId: msg.AdminRequest.ChangePeer.Peer.Id, Context: ctx, // notice }) ","date":"2024-08-14","objectID":"/posts/tinykv_p3b_problems/:3:1","tags":["raft","kv","tinykv"],"title":"TinyKV Project 3B 疑难杂症","uri":"/posts/tinykv_p3b_problems/"},{"categories":["kv"],"content":"（2）AddNode后 RequestTimeout（对于未初始化的Raft节点的处理） -- FAIL: TestBasicConfChange3B (52.68s) panic: request timeout [recovered] panic: request timeout 错误原因：对于Raft节点不在集群当中（尚未初始化）的情况，屏蔽掉了HeartBeat​和Snapshot​消息，使得节点不能通过leader发送的Snapshot完成初始化，进而导致committed始终无法向前推进（由于新节点的原因无法满足“集群大多数”的条件），请求无法应用而超时。 Solution：对于Raft节点不在集群当中（尚未初始化）的情况，只能屏蔽选举相关的消息。 ","date":"2024-08-14","objectID":"/posts/tinykv_p3b_problems/:3:2","tags":["raft","kv","tinykv"],"title":"TinyKV Project 3B 疑难杂症","uri":"/posts/tinykv_p3b_problems/"},{"categories":["kv"],"content":"（3）​[region x] x meta corruption detected​ 2023/12/29 12:16:14 request err resp. panic: [region 1] 5 meta corruption detected goroutine 321 [running]: github.com/pingcap-incubator/tinykv/kv/raftstore.(*peerMsgHandler).destroyPeer(0xc17c9b9e10) /home/sszgwdk/project/tinykv/tinykv/kv/raftstore/peer_msg_handler.go:975 +0x265 github.com/pingcap-incubator/tinykv/kv/raftstore.(*peerMsgHandler).processConfChange(0xc17c9b9e10, 0xc17c924d50?, 0xc17c9b9aa0) /home/sszgwdk/project/tinykv/tinykv/kv/raftstore/peer_msg_handler.go:253 +0x6de github.com/pingcap-incubator/tinykv/kv/raftstore.(*peerMsgHandler).process(0xc17c9b9e10, 0xc17c9b9c40) /home/sszgwdk/project/tinykv/tinykv/kv/raftstore/peer_msg_handler.go:351 +0x199 github.com/pingcap-incubator/tinykv/kv/raftstore.(*peerMsgHandler).HandleRaftReady(0xc17c9b9e10) /home/sszgwdk/project/tinykv/tinykv/kv/raftstore/peer_msg_handler.go:450 +0x408 github.com/pingcap-incubator/tinykv/kv/raftstore.(*raftWorker).run(0xc000264920, 0xc000039440, 0x0?) /home/sszgwdk/project/tinykv/tinykv/kv/raftstore/raft_worker.go:70 +0x439 created by github.com/pingcap-incubator/tinykv/kv/raftstore.(*Raftstore).startWorkers /home/sszgwdk/project/tinykv/tinykv/kv/raftstore/raftstore.go:271 +0x17b FAIL github.com/pingcap-incubator/tinykv/kv/test_raftstore 2.015s FAIL 根据报错信息可以找到出错的位置在destroyPeer​，出错的代码： if _, ok := meta.regions[regionID]; !ok { panic(d.Tag + \" meta corruption detected\") } 出错原因：对于RemoveNode命令的apply，不需要对storeMeta​进行修改，destoryPeer​会负责对storeMeta​的修改。因此下列操作是不应该做的： d.ctx.storeMeta.Lock() delete(d.ctx.storeMeta.regions, targetPeer.Id) d.ctx.storeMeta.regionRanges.Delete(\u0026regionItem{region: region}) d.ctx.storeMeta.Unlock() Solution：在RemoveNode的Apply处理中去掉 ","date":"2024-08-14","objectID":"/posts/tinykv_p3b_problems/:3:3","tags":["raft","kv","tinykv"],"title":"TinyKV Project 3B 疑难杂症","uri":"/posts/tinykv_p3b_problems/"},{"categories":["kv"],"content":"（4）confChange Request TimeOut（propose成功，但是confChange无法添加到RaftLog当中） --- FAIL: TestConfChangeRecover3B (58.85s) panic: request timeout [recovered] panic: request timeout 错误原因：当r.PendingConfIndex != None​，会忽略所有新的confChange Entry而不报错，这就造成了能够ProposeConfChange​成功，但是Entry却没有append进去。 func (r *Raft) appendEntries(entries ...*pb.Entry) { ents := make([]pb.Entry, 0) for _, e := range entries { if e.EntryType == pb.EntryType_EntryConfChange { if r.PendingConfIndex != None { continue } r.PendingConfIndex = e.Index } ents = append(ents, pb.Entry{ EntryType: e.EntryType, Term: e.Term, Index: e.Index, Data: e.Data, }) } r.RaftLog.appendEntries(ents...) r.Prs[r.id].Match = r.RaftLog.LastIndex() r.Prs[r.id].Next = r.RaftLog.LastIndex() + 1 } 显然这个条件r.PendingConfIndex != None​是不合适的，错误地忽略了不该忽略的confChange Entry。 正确的条件应该是：r.PendingConfIndex != None \u0026\u0026 r.PendingConfIndex \u003e r.RaftLog.applied​，代表还有confChange Entry没有被Apply（已经Apply的显然可以忽略了） ","date":"2024-08-14","objectID":"/posts/tinykv_p3b_problems/:3:4","tags":["raft","kv","tinykv"],"title":"TinyKV Project 3B 疑难杂症","uri":"/posts/tinykv_p3b_problems/"},{"categories":["kv"],"content":"（5）删除节点时会遇到 Request timeout 问题 观察日志，发现一直在进行选举 ​ test log ​ 解决参考： TinyKV-White-Paper/Project3-MultiRaftKV.md at main · Smith-Cruise/TinyKV-White-Paper (github.com) 问题原因：只剩两个节点，然后被移除的那个节点正好是 Leader。因为网络是 unreliable，Leader 广播给另一个 Node 的心跳正好被丢了，也就是另一个节点的 commit 并不会被推进，也就是对方节点并不会执行 remove node 操作。而这一切 Leader 并不知道，它自己调用 d.destroyPeer()​ 已经销毁了。此时另一个节点并没有移除 Leader，它会发起选举，但是永远赢不了，因为需要收到被移除 Leader 的投票。 白皮书提供的解决方法： 在 propose 阶段，如果已经处于两节点，被移除的正好是 Leader，那么直接拒绝该 propose，并且发起 Transfer Leader 到另一个节点上。Client 到时候会重试 remove node 指令。 在 apply 阶段 DestroyPeer 上面做一个保险（有必要，虽然用到的概率很低），让 Leader 在自己被 remove 前重复多次发送心跳到目标节点，尝试推动目标节点的 commit。重复多次是为了抵消测试用例的 unreliable 我这里两个都实现了，不同的是第二个的保险措施我发送的是Append消息而不是心跳消息（我的Raft层实现中follower处理心跳没有推进Commit），并且根据我的测试，重复的次数为5已经够了。 即使如此，仍然只是在概率上避免了这个问题，不能完美的解决。 ‍ ","date":"2024-08-14","objectID":"/posts/tinykv_p3b_problems/:3:5","tags":["raft","kv","tinykv"],"title":"TinyKV Project 3B 疑难杂症","uri":"/posts/tinykv_p3b_problems/"},{"categories":["kv"],"content":"Split相关 ","date":"2024-08-14","objectID":"/posts/tinykv_p3b_problems/:4:0","tags":["raft","kv","tinykv"],"title":"TinyKV Project 3B 疑难杂症","uri":"/posts/tinykv_p3b_problems/"},{"categories":["kv"],"content":"（1）Snap请求处理超时 日志显示总是snap消息的处理问题，总是报错key not in region​，而snap不涉及key。 问题原因：出在propose上面，对common request进行key in region​检查时应该跳过Snap request​，而这里忘记跳过了。 解决方法： // check if key in region for _, req := range msg.Requests { if req.CmdType != raft_cmdpb.CmdType_Snap { if err := util.CheckKeyInRegion(getRequestKey(req), d.Region()); err != nil { cb.Done(ErrResp(err)) return } } } ","date":"2024-08-14","objectID":"/posts/tinykv_p3b_problems/:4:1","tags":["raft","kv","tinykv"],"title":"TinyKV Project 3B 疑难杂症","uri":"/posts/tinykv_p3b_problems/"},{"categories":["kv"],"content":"（2）key [xxx…] is not in region xx panic: key [49 55 32 48 48 48 48 48 48 48 48] is not in region id:1 end_key:\"13 00000001\" region_epoch:\u003cconf_ver:1 version:2 \u003e peers:\u003cid:1 store_id:1 \u003e peers:\u003cid:2 store_id:2 \u003e peers:\u003cid:3 store_id:3 \u003e peers:\u003cid:4 store_id:4 \u003e peers:\u003cid:5 store_id:5 \u003e goroutine 327 [running]: github.com/pingcap-incubator/tinykv/kv/storage/raft_storage.(*RegionIterator).Seek(0xc17cb84ed0, {0xc17cc01d50, 0xb, 0x10?}) /home/sszgwdk/project/tinykv/tinykv/kv/storage/raft_storage/region_reader.go:83 +0xec github.com/pingcap-incubator/tinykv/kv/test_raftstore.(*Cluster).Scan(0xb86019?, {0xc17cc01d50, 0xb, 0x10}, {0xc17c6dddb0, 0xb, 0x20}) /home/sszgwdk/project/tinykv/tinykv/kv/test_raftstore/cluster.go:388 +0x322 github.com/pingcap-incubator/tinykv/kv/test_raftstore.GenericTest.func1(0x11, 0x0?) /home/sszgwdk/project/tinykv/tinykv/kv/test_raftstore/test_test.go:218 +0x533 github.com/pingcap-incubator/tinykv/kv/test_raftstore.runClient(0x0?, 0x0?, 0x0?, 0x0?) /home/sszgwdk/project/tinykv/tinykv/kv/test_raftstore/test_test.go:27 +0x78 created by github.com/pingcap-incubator/tinykv/kv/test_raftstore.SpawnClientsAndWait /home/sszgwdk/project/tinykv/tinykv/kv/test_raftstore/test_test.go:37 +0x85 FAIL github.com/pingcap-incubator/tinykv/kv/test_raftstore 6.645s FAIL 问题原因分析：报错是由Scan操作（对应Snap命令）引起的，Seek()函数产生的报错，错误原因是key不在该region中。 func (it *RegionIterator) Seek(key []byte) { if err := util.CheckKeyInRegion(key, it.region); err != nil { panic(err) } it.iter.Seek(key) } 猜想哪些情况会导致上述错误的发生： 处理Snap命令时region := c.GetRegion(key)​获取了错误的region（这种情况有可能是在更新region后，没有及时告知Scheduler） 目标region在请求过程中发生了变化，导致错误产生（然而Snap请求不包含key字段，没办法在raftStore中通过checkKeyInRegion​检查，所以为了避免Snap的key错误，当region发生变化时，清空所有proposals么？？） 再仔细打印和观察日志 2024/02/04 17:23:03 [RaftStore]: process common request: header:\u003cregion_id:1 peer:\u003cid:3 store_id:3 \u003e region_epoch:\u003cconf_ver:1 version:1 \u003e \u003e requests:\u003ccmd_type:Snap snap:\u003c\u003e \u003e 2024/02/04 17:23:03 [RaftStore]: process common request: header:\u003cregion_id:1 peer:\u003cid:3 store_id:3 \u003e region_epoch:\u003cconf_ver:1 version:1 \u003e \u003e requests:\u003ccmd_type:Snap snap:\u003c\u003e \u003e 2024/02/04 17:23:03 [RaftStore]: process admin request: Split 2024/02/04 17:23:03.477082 cluster.go:204: \u001b[0;37m[info] [Cluster] kv/test_raftstore/cluster.go: request success. req: header:\u003cregion_id:1 peer:\u003cid:3 store_id:3 \u003e region_epoch:\u003cconf_ver:1 version:1 \u003e \u003e requests:\u003ccmd_type:Snap snap:\u003c\u003e \u003e \u001b[0m 2024/02/04 17:23:03.477110 cluster.go:204: \u001b[0;37m[info] [Cluster] kv/test_raftstore/cluster.go: request success. req: header:\u003cregion_id:1 peer:\u003cid:3 store_id:3 \u003e region_epoch:\u003cconf_ver:1 version:1 \u003e \u003e requests:\u003ccmd_type:Snap snap:\u003c\u003e \u003e \u001b[0m 2024/02/04 17:23:03.477145 region_reader.go:84: \u001b[0;37m[info] seek key [51 32 48 48 48 48 48 48 48 48] in region 1 failed: key [51 32 48 48 48 48 48 48 48 48] is not in region id:1 end_key:\"18 00000000\" region_epoch:\u003cconf_ver:1 version:2 \u003e peers:\u003cid:1 store_id:1 \u003e peers:\u003cid:2 store_id:2 \u003e peers:\u003cid:3 store_id:3 \u003e peers:\u003cid:4 store_id:4 \u003e peers:\u003cid:5 store_id:5 \u003e \u001b[0m 2024/02/04 17:23:03.477158 region_reader.go:84: \u001b[0;37m[info] seek key [56 32 48 48 48 48 48 48 48 48] in region 1 failed: key [56 32 48 48 48 48 48 48 48 48] is not in region id:1 end_key:\"18 00000000\" region_epoch:\u003cconf_ver:1 version:2 \u003e peers:\u003cid:1 store_id:1 \u003e peers:\u003cid:2 store_id:2 \u003e peers:\u003cid:3 store_id:3 \u003e peers:\u003cid:4 store_id:4 \u003e peers:\u003cid:5 store_id:5 \u003e \u001b[0m 发现错误原因是，已经处理完了Snap请求，但是之后执行了一个Split操作，导致region更新以后，再进行Scan就出错了，这是因为Snap响应中包含一个当前region的指针，Split之后就会导致key not in region​的错误。 解决方法：在Snap命令之后执行的Split命令不应当造成影响，因此要在返回Snap响应时拷贝一份region，从而避免后面的命令对region的修改 case raft_cmdpb.CmdType_Snap: // solve key not in region error: copy region region := new(metapb.Region) err := util.CloneMsg(d.Region(), region) if err != nil { panic(err) } // resp.Responses = []*raft_cmdpb.Response{{CmdType: raft_cmdpb.CmdType_Snap, Snap: \u0026raft_cmdpb.SnapResponse{Region: d.Region()}}} resp.Responses = append(resp.Responses, \u0026raft_cmdpb.Response{CmdType: raft_cmdpb.CmdType_Snap, Snap: \u0026raft_cmdpb.SnapRespons","date":"2024-08-14","objectID":"/posts/tinykv_p3b_problems/:4:2","tags":["raft","kv","tinykv"],"title":"TinyKV Project 3B 疑难杂症","uri":"/posts/tinykv_p3b_problems/"},{"categories":["kv"],"content":"（3）test timed out after 10m0s 这个是困扰我最久的一个错误，在上一篇project3b思路中也提到了。 错误原因是测试时间太久了。但是我根本不知道为什么这么久，一开始以为是选举太慢了，直到看到了peer​中两个成员： // An inaccurate difference in region size since last reset. // split checker is triggered when it exceeds the threshold, it makes split checker not scan the data very often // (Used in 3B split) SizeDiffHint uint64 // Approximate size of the region. // It's updated everytime the split checker scan the data // (Used in 3B split) ApproximateSize *uint64 原来split checker会依据SizeDiffHint​来判断region承载的数据量是否超出阈值，从而触发split操作。 因此要做如下两个修改 Apply Admin_Split完成后，要对SizeDiffHint​和ApproximateSize​更新 // clear region size d.SizeDiffHint = 0 d.ApproximateSize = new(uint64) Apply put/delete​时对SizeDiffHint​进行调整，在 Put​ 的时候，SizeDiffHint​ 加上 key​ 和 value​ 的大小；在 Delete​ 的时候，减去 key​ 的大小。 case raft_cmdpb.CmdType_Put: wb.SetCF(req.Put.Cf, req.Put.Key, req.Put.Value) d.SizeDiffHint += uint64(len(req.Put.Key) + len(req.Put.Value)) case raft_cmdpb.CmdType_Delete: wb.DeleteCF(req.Delete.Cf, req.Delete.Key) d.SizeDiffHint -= uint64(len(req.Delete.Key)) 当时没仔细看白皮书，它里面有提到过这个问题： 在 nclient \u003e= 8 \u0026\u0026 crash = true \u0026\u0026 split = true​ 这种条件下，测试在 Delete 阶段卡死问题，这是因为在 apply CmdType_Put​ 和 CmdType_Delete​ 请求的时候没有更新 SizeDiffHint​。因此需要在 Put​ 的时候，SizeDiffHint​ 加上 key​ 和 value​ 的大小；在 Delete​ 的时候，减去 key​ 的大小。 ","date":"2024-08-14","objectID":"/posts/tinykv_p3b_problems/:4:3","tags":["raft","kv","tinykv"],"title":"TinyKV Project 3B 疑难杂症","uri":"/posts/tinykv_p3b_problems/"},{"categories":["kv"],"content":"（4）runtime error: index out of range [1] with length 1 2024/02/16 00:50:40.882060 peer.go:55: \u001b[0;37m[info] [region 1] replicates peer with ID 14\u001b[0m 2024/02/16 00:50:40.885097 peer_storage.go:182: \u001b[0;37m[info] [region 1] 5 requesting snapshot\u001b[0m 2024/02/16 00:50:40.984182 peer_storage.go:341: \u001b[0;37m[info] [region 1] 14 begin to apply snapshot\u001b[0m 2024/02/16 00:50:40.984211 region_task.go:93: \u001b[0;37m[info] begin apply snap data. [regionId: 1]\u001b[0m 2024/02/16 00:50:40.984223 region_task.go:137: \u001b[0;37m[info] succeed in deleting data in range. [regionId: 1, startKey: , endKey: 32203030303030303033]\u001b[0m 2024/02/16 00:50:40.996109 snap.go:700: \u001b[0;37m[info] apply snapshot ingested 1 tables\u001b[0m 2024/02/16 00:50:40.996163 region_task.go:117: \u001b[0;37m[info] applying new data. [regionId: 1, timeTakes: 11.892027ms]\u001b[0m panic: runtime error: index out of range [1] with length 1 goroutine 288 [running]: github.com/pingcap-incubator/tinykv/kv/raftstore.(*peerMsgHandler).processAdminRequest(0xc191a41e10, 0xc00000000a?, 0x27?) /home/sszgwdk/project/tinykv/tinykv/kv/raftstore/peer_msg_handler.go:137 +0xe08 github.com/pingcap-incubator/tinykv/kv/raftstore.(*peerMsgHandler).process(0xc191a41e10, 0xc224bbdea0) /home/sszgwdk/project/tinykv/tinykv/kv/raftstore/peer_msg_handler.go:442 +0x347 github.com/pingcap-incubator/tinykv/kv/raftstore.(*peerMsgHandler).HandleRaftReady(0xc191a41e10) /home/sszgwdk/project/tinykv/tinykv/kv/raftstore/peer_msg_handler.go:677 +0x57f github.com/pingcap-incubator/tinykv/kv/raftstore.(*raftWorker).run(0xc1051b3840, 0xc000194d20, 0x0?) /home/sszgwdk/project/tinykv/tinykv/kv/raftstore/raft_worker.go:70 +0x439 created by github.com/pingcap-incubator/tinykv/kv/raftstore.(*Raftstore).startWorkers /home/sszgwdk/project/tinykv/tinykv/kv/raftstore/raftstore.go:271 +0x17b FAIL github.com/pingcap-incubator/tinykv/kv/test_raftstore 6.899s FAIL 报错点在应用Split命令处 case raft_cmdpb.AdminCmdType_Split: leftRegion := d.Region() // check ... // backup rawRegion for delete regionRange rawRegion := new(metapb.Region) util.CloneMsg(leftRegion, rawRegion) // create rightRegion // // create rightRegion // rightRegion := new(metapb.Region) util.CloneMsg(leftRegion, rightRegion) // newPeers and split Region newPeers := make([]*metapb.Peer, 0) for i, peer := range leftRegion.Peers { newPeers = append(newPeers, \u0026metapb.Peer{ Id: split.NewPeerIds[i], StoreId: peer.StoreId, }) } // [StartKey, SplitKey) -\u003e leftRegion // [SplitKey, EndKey) -\u003e rightRegion rightRegion.Id = split.NewRegionId rightRegion.StartKey = split.SplitKey rightRegion.Peers = newPeers leftRegion.EndKey = split.SplitKey leftRegion.RegionEpoch.Version += 1 rightRegion.RegionEpoch.Version += 1 ... 这里默认leftRegion.Peers​与split.NewPeerIds​长度相同了，但实际可能没有，因此要修改，保证不会越界即可。 我这里直接判定长度是否相等了，如果不相等直接返回，等客户端重试。 if len(leftRegion.Peers) != len(split.NewPeerIds) { return } ","date":"2024-08-14","objectID":"/posts/tinykv_p3b_problems/:4:4","tags":["raft","kv","tinykv"],"title":"TinyKV Project 3B 疑难杂症","uri":"/posts/tinykv_p3b_problems/"},{"categories":["kv"],"content":"（5）find no peer for store 4 in region id:13 start_key:“0 00000011” end_key:“2 00000004” panic: find no peer for store 4 in region id:13 start_key:\"0 00000011\" end_key:\"2 00000004\" region_epoch:\u003cconf_ver:6 version:3 \u003e peers:\u003cid:14 store_id:2 \u003e goroutine 271 [running]: github.com/pingcap-incubator/tinykv/kv/raftstore.(*peerMsgHandler).processAdminRequest(0xc17bdbfe10, 0xc10000000a?, 0x27?) /home/sszgwdk/project/tinykv/tinykv/kv/raftstore/peer_msg_handler.go:187 +0xdfd github.com/pingcap-incubator/tinykv/kv/raftstore.(*peerMsgHandler).process(0xc17bdbfe10, 0xc20d840140) /home/sszgwdk/project/tinykv/tinykv/kv/raftstore/peer_msg_handler.go:445 +0x347 github.com/pingcap-incubator/tinykv/kv/raftstore.(*peerMsgHandler).HandleRaftReady(0xc17bdbfe10) /home/sszgwdk/project/tinykv/tinykv/kv/raftstore/peer_msg_handler.go:680 +0x57f github.com/pingcap-incubator/tinykv/kv/raftstore.(*raftWorker).run(0xc000130b80, 0xc0002a51a0, 0x0?) /home/sszgwdk/project/tinykv/tinykv/kv/raftstore/raft_worker.go:70 +0x439 created by github.com/pingcap-incubator/tinykv/kv/raftstore.(*Raftstore).startWorkers /home/sszgwdk/project/tinykv/tinykv/kv/raftstore/raftstore.go:271 +0x17b FAIL github.com/pingcap-incubator/tinykv/kv/test_raftstore 7.331s FAIL rm -rf /tmp/*test-raftstore* 报错位置：Apply Split创建新的Peer // Peer: creat and register and start newPeer, err := createPeer(d.ctx.store.Id, d.ctx.cfg, d.ctx.regionTaskSender, d.ctx.engine, rightRegion) if err != nil { panic(err) } 出问题的地方与上一个错误一样，没有对newPeers​是否为空进行判断，也没有对newPeers​是否包含当前store进行检查。 解决方法： newPeers := make([]*metapb.Peer, 0) hasCurrentStore := false if len(leftRegion.Peers) != len(split.NewPeerIds) { return } for i, peer := range leftRegion.Peers { if i \u003c len(split.NewPeerIds) { newPeers = append(newPeers, \u0026metapb.Peer{ Id: split.NewPeerIds[i], StoreId: peer.StoreId, }) if peer.StoreId == d.ctx.store.GetId() { hasCurrentStore = true } } } if len(newPeers) == 0 || !hasCurrentStore { return } ","date":"2024-08-14","objectID":"/posts/tinykv_p3b_problems/:4:5","tags":["raft","kv","tinykv"],"title":"TinyKV Project 3B 疑难杂症","uri":"/posts/tinykv_p3b_problems/"},{"categories":["kv"],"content":"project3b是整个tinykv中最难的部分，测试会出现很多问题，往往需要通过打印大量日志才能找到问题的原因，因此在编程时要尤其注意一些细节。不过调试这些Bug也是该项目的内容之一，锻炼发现问题解决问题的能力，加深对分布式kv引擎的认识。 project3b的代码实现最主要还是在kv/raftstore/peer_msg_handler.go​当中，当然在调试过程中必然会发现Raft层的处理也会有很多问题要进行修改。 project3b增加了三个admin命令：TransferLeader，ChangePeer，Split。为了使tinykv支持这些命令，要完成对应的Propose和Process的逻辑。建议尽量将普通命令和admin命令的Propose和Process分开处理，不要太耦合。 ","date":"2024-08-14","objectID":"/posts/tinykv_p3b/:0:0","tags":["raft","kv","tinykv"],"title":"TinyKV Project 3B","uri":"/posts/tinykv_p3b/"},{"categories":["kv"],"content":"TransferLeader 如文档所说，TransferLeader是一个动作不需要作为一条日志Propose到Raft层共识，更不需要Process，只需要调用 RawNode 的 TransferLeader() 方法并返回响应。 ","date":"2024-08-14","objectID":"/posts/tinykv_p3b/:1:0","tags":["raft","kv","tinykv"],"title":"TinyKV Project 3B","uri":"/posts/tinykv_p3b/"},{"categories":["kv"],"content":"ChangePeer propose ChangePeer命令的流程与之前普通命令类似，不同的是调用的RawNode接口由Propose​变成了ProposeConfChange​ perr := d.RaftGroup.ProposeConfChange(eraftpb.ConfChange{ ChangeType: msg.AdminRequest.ChangePeer.ChangeType, NodeId: msg.AdminRequest.ChangePeer.Peer.Id, Context: ctx, }) process的流程则相对复杂，需要按照check、apply、response三步来做。 对于check，由于ChangePeer不涉及key，所以不要考虑ErrKeyNotInRegion，但是需要考虑ErrEpochNotMatch，使用util.CheckRegionEpoch​方法，具体用法可以参考preProposeRaftCommand​中的代码。 实际上在propose之前也需要检查**ErrEpochNotMatch，**不过已经在preProposeRaftCommand​中已经实现了。 如果检查出错误，需要利用proposals中的回调返回errResponse。 对于apply，分为AddNode和RemoveNode两种。 ","date":"2024-08-14","objectID":"/posts/tinykv_p3b/:2:0","tags":["raft","kv","tinykv"],"title":"TinyKV Project 3B","uri":"/posts/tinykv_p3b/"},{"categories":["kv"],"content":"AddNode的Apply 检查是否是重复的命令，即如果节点已在集群中，此时跳过apply 修改并写入RegionLocalState​（使用​meta.WriteRegionState​），包括RegionEpoch​和Region's peers​ ​region.Peers = append(region.Peers, targetPeer)​ ​region.RegionEpoch.ConfVer += 1​ 更新GlobalContext storeMeta​，包括regions​和regionRanges​，注意访问和修改时的加锁 ​insertPeerCache​，d.RaftGroup.ApplyConfChange​ 注意不需要实际创建的一个Peer，这里是先加入到集群当中，Leader发送心跳，转发消息时发现节点不存在，由storeWorker调用maybeCreatePeer()进行实际的创建 ","date":"2024-08-14","objectID":"/posts/tinykv_p3b/:2:1","tags":["raft","kv","tinykv"],"title":"TinyKV Project 3B","uri":"/posts/tinykv_p3b/"},{"categories":["kv"],"content":"RemoveNode的Apply 如果需要Remove的节点ID与当前节点ID相等，调用d.destroyPeer()​ 检查是否是重复的命令，即如果节点已不在集群中，此时跳过apply 修改并写入RegionLocalState​（使用meta.WriteRegionState​），包括RegionEpoch​和Region's peers​ 不用更新GlobalContext storeMeta​，这个是由d.destroyPeer()​完成的 ​removePeerCache​，​d.RaftGroup.ApplyConfChange​ 另外，在完成process后，要检查节点是否停止，因为有可能会销毁当前节点，此时直接返回即可，不需要做后面的任何处理。 for _, entry := range ready.CommittedEntries { d.process(\u0026entry) // may destroy oneself, so need to check if stopped if d.stopped { return } 对于response，按照之前在project2b中相同的处理，我使用自定义的clearStaleAndGetTargetProposal​（详见project2b思路），注意在最后需要调用d.notifyHeartbeatScheduler(region, d.peer)​给Scheduler（project3c）发送一个心跳，来通知region的变化（冗余的更新不会影响正确性，因此建议在发生region修改的地方都发送一个心跳） if d.clearStaleAndGetTargetProposal(entry) { p := d.proposals[0] resp := \u0026raft_cmdpb.RaftCmdResponse{ Header: \u0026raft_cmdpb.RaftResponseHeader{}, } switch req.CmdType { case raft_cmdpb.AdminCmdType_ChangePeer: resp.AdminResponse = \u0026raft_cmdpb.AdminResponse{ CmdType: raft_cmdpb.AdminCmdType_ChangePeer, ChangePeer: \u0026raft_cmdpb.ChangePeerResponse{}, // ChangePeer: \u0026raft_cmdpb.ChangePeerResponse{Region: d.Region()}, } } p.cb.Done(resp) d.proposals = d.proposals[1:] } d.notifyHeartbeatScheduler(region, d.peer) func (d *peerMsgHandler) notifyHeartbeatScheduler(region *metapb.Region, peer *peer) { clonedRegion := new(metapb.Region) err := util.CloneMsg(region, clonedRegion) if err != nil { return } d.ctx.schedulerTaskSender \u003c- \u0026runner.SchedulerRegionHeartbeatTask{ Region: clonedRegion, Peer: peer.Meta, PendingPeers: peer.CollectPendingPeers(), ApproximateSize: peer.ApproximateSize, } } 以上就完成了最基本的confChange，但是不能通过所有测试，由于可能存在网络不稳定和隔离等情况，需要做一些特殊处理和优化，这些处理会在下一篇“tinykv project3b疑难杂症”中汇总。 ","date":"2024-08-14","objectID":"/posts/tinykv_p3b/:2:2","tags":["raft","kv","tinykv"],"title":"TinyKV Project 3B","uri":"/posts/tinykv_p3b/"},{"categories":["kv"],"content":"Region Split Split命令的Propose过程与ChangePeer也是类似的，不同的是Split命令中包含一个split_key​，代表将当前region按split_key​拆分，因此要检查ErrKeyNotInRegion。 之前在project2b中对于普通命令没有对ErrKeyNotInRegion检查，此处也需要为除了Snap（Snap命令中不包含key）命令之外的其他普通命令增加检查ErrKeyNotInRegion的代码。 Split命令的Process同样可以分成check、apply、response三步。 对于check，实际上是重复Propose的检查过程，需要检查ErrEpochNotMatch和ErrKeyNotInRegion。 ","date":"2024-08-14","objectID":"/posts/tinykv_p3b/:3:0","tags":["raft","kv","tinykv"],"title":"TinyKV Project 3B","uri":"/posts/tinykv_p3b/"},{"categories":["kv"],"content":"apply Split apply的过程则相对复杂，我的实现步骤如下： ​split := req.GetSplit()​​获取Split命令中的数据，拷贝一份当前节点原始Region信息暂存在rawRegion​​中（利用util.CloneMsg​​方法），原始Region使用leftRegion​​命名，再拷贝一份rightRegion​​，代表拆分后的右半region。 使用split.NewPeerIds​​初始化rightRegion.Peers​​，将split.NewRegionId​​赋值给rightRegion.Id​​，将split.SplitKey​​赋值给rightRegion.StartKey​​，将split.SplitKey​​赋值给leftRegion.EndKey​​，即[StartKey, SplitKey) -\u003e leftRegion​​、[SplitKey, EndKey) -\u003e rightRegion​​。最后不要忘记leftRegion.RegionEpoch.Version += 1​​、rightRegion.RegionEpoch.Version += 1​​。此时leftRegion​​和rightRegion​​对应Split之后的左右Region。（注意leftRegion继承原始region的所有数据） 使用meta.WriteRegionState​​写入两个region 更新storeMeta​​，包括： 在storeMeta.regionRanges​​中删除rawRegion​​ 在storeMeta.regions​​中添加rightRegion​​ 使用leftRegion​​和rightRegion​​更新storeMeta.regionRanges​​（调用方法storeMeta.regionRanges.ReplaceOrInsert​​） 注意加锁 清理region size，包括SizeDiffHint​和ApproximateSize​，这个很关键，在下一篇疑难杂症也会提到 使用createPeer​​方法创建newPeer​​，利用d.ctx.router​​注册和启动该节点 reponse的过程与ChangePeer类似： notifyHeartbeatScheduler​​发送心跳，注意两个Region都要调用 返回响应，还是利用自定义的clearStaleAndGetTargetProposal​​ ","date":"2024-08-14","objectID":"/posts/tinykv_p3b/:3:1","tags":["raft","kv","tinykv"],"title":"TinyKV Project 3B","uri":"/posts/tinykv_p3b/"},{"categories":["kv"],"content":"其他修改 ","date":"2024-08-14","objectID":"/posts/tinykv_p3b/:4:0","tags":["raft","kv","tinykv"],"title":"TinyKV Project 3B","uri":"/posts/tinykv_p3b/"},{"categories":["kv"],"content":"ApplySnapshot后的region状态更新 应用快照会通常会伴随region的更新（例如未初始化的新节点），SaveReadyState​的返回值中有一个*ApplySnapResult​，如果它不为nil​且其中的PrevRegion​和Region​不相等，说明发生了Region更新，不仅要在内存中更新regionLocalState​以及持久化，还要更新全局的storeMeta​并发送心跳给Scheduler，如下： result, err := d.peerStorage.SaveReadyState(\u0026ready) if err != nil { panic(err) } // update region if result != nil \u0026\u0026 !reflect.DeepEqual(result.PrevRegion, result.Region) { d.peerStorage.SetRegion(result.Region) storeMeta := d.ctx.storeMeta storeMeta.Lock() storeMeta.regions[result.Region.GetId()] = result.Region storeMeta.regionRanges.ReplaceOrInsert(\u0026regionItem{region: result.Region}) storeMeta.Unlock() d.HeartbeatScheduler(d.ctx.schedulerTaskSender) } ","date":"2024-08-14","objectID":"/posts/tinykv_p3b/:4:1","tags":["raft","kv","tinykv"],"title":"TinyKV Project 3B","uri":"/posts/tinykv_p3b/"},{"categories":["kv"],"content":"普通命令的修改 由于引入了region，对普通命令的propose和process也要做相应修改。 首先就是对Get、Put、Delete检查ErrEpochNotMatch​和ErrKeyNotInRegion。 其次对于Put和Delete命令的应用，需要记录当前region的大小变化，这是通过SizeDiffHint​记录的。 type peer struct { ... // An inaccurate difference in region size since last reset. // split checker is triggered when it exceeds the threshold, it makes split checker not scan the data very often // (Used in 3B split) SizeDiffHint uint64 // Approximate size of the region. // It's updated everytime the split checker scan the data // (Used in 3B split) ApproximateSize *uint64 ... } 对于Put，​d.SizeDiffHint += uint64(len(req.Put.Key) + len(req.Put.Value))​ 对于Delete命令，​d.SizeDiffHint -= uint64(len(req.Delete.Key))​ 在Split中也提到Apply Admin_Split完成后，要对SizeDiffHint​和ApproximateSize​更新。 如果不做上述处理在测试时会引发Request Timeout。原来split checker会依据SizeDiffHint​来判断region承载的数据量是否超出阈值，从而触发split操作。这在文档中并没有说明，害我调了很久。 ‍ 以上就完成了3B的所有基本内容，但测试通常是过不了的，会有很多异常情况，下一篇将对我当时遇到的疑难杂症进行汇总。 ","date":"2024-08-14","objectID":"/posts/tinykv_p3b/:4:2","tags":["raft","kv","tinykv"],"title":"TinyKV Project 3B","uri":"/posts/tinykv_p3b/"},{"categories":["kv"],"content":"在Raft层实现领导者转移和成员变更（虽然Raft层实现简单，但是存在很多细节问题需要注意，后面3B的测试问题一半都来自Raft层） ","date":"2024-08-14","objectID":"/posts/tinykv_p3a/:0:0","tags":["raft","kv","tinykv"],"title":"TinyKV Project 3A","uri":"/posts/tinykv_p3a/"},{"categories":["kv"],"content":"领导者转移 HandleTransferLeader 检查目标节点是否在集群当中，如果不在直接返回 如果当前节点不是leader，转发给Leader后返回 r.leadTransferee = m.From 接收该请求 检查目标节点的日志是否最新，如果不是则sendAppend​，而后返回（等后续HandleAppendResponse​来进一步处理 如果目标节点的日志已经最新，发送TimeoutNow​消息给它，让其立即开始选举；最后将leadTransferee置为None HandleAppendResponse 当转移的目标节点日志不是最新时，HandleTransferLeader不能立即发送TimeoutNow​消息，而是sendAppend​使目标日志最新，这时需要在HandleAppendResponse中增加发送TimeoutNow​和重置r.leadTransferee​的逻辑。 注意，当leaderTransferee != None​时，即在领导这转移过程中，不接受Propose请求，避免循环。 ‍ ","date":"2024-08-14","objectID":"/posts/tinykv_p3a/:1:0","tags":["raft","kv","tinykv"],"title":"TinyKV Project 3A","uri":"/posts/tinykv_p3a/"},{"categories":["kv"],"content":"成员变更 Raft层的逻辑十分简单，主要是针对r.Prs​的修改。 // addNode add a new node to raft group func (r *Raft) addNode(id uint64) { // Your Code Here (3A). // if exit if _, ok := r.Prs[id]; ok { r.PendingConfIndex = None return } r.Prs[id] = \u0026Progress{ Match: 0, Next: 1, } r.PendingConfIndex = None } // removeNode remove a node from raft group func (r *Raft) removeNode(id uint64) { // Your Code Here (3A). if _, ok := r.Prs[id]; !ok { r.PendingConfIndex = None return } delete(r.Prs, id) // important: if leader, should update commit if r.State == StateLeader { r.tryUpdateCommit() } r.PendingConfIndex = None } notice 当removeNode​时，由于集群成员数量发生变化，Leader要尝试推进日志的提交 PendingConfIndex是一个值得注意的变量。当其不为None时，代表目前正有confChange发生，不再接收新的confChange请求，因此要在HandlePropose​中做一定的检查。判断条件为r.PendingConfIndex != None \u0026\u0026 r.PendingConfIndex \u003e r.RaftLog.applied​，代表存在尚未应用的confChange。 ","date":"2024-08-14","objectID":"/posts/tinykv_p3a/:2:0","tags":["raft","kv","tinykv"],"title":"TinyKV Project 3A","uri":"/posts/tinykv_p3a/"},{"categories":["kv"],"content":"Project 3 MultiRaftKV 在 Project2 中，你建立了一个基于Raft的高可用的kv服务器，做得很好！但还不够，这样的kv服务器是由单一的 raftGroup 支持的，不能无限扩展，并且每一个写请求都要等到提交后再逐一写入 badger，这是保证一致性的一个关键要求，但也扼杀了任何并发性。 ​ image1 ​ 在这个项目中，你将实现一个带有平衡调度器的基于 multi Raft 的kv服务器，它由多个 Raft group 组成，每个 Raft group 负责一个单独的 key 范围，在这里被命名为 region ，布局将看起来像上图。对单个 region 的请求的处理和以前一样，但多个 region 可以同时处理请求，这提高了性能，但也带来了一些新的挑战，如平衡每个 region 的请求，等等。 这个项目有3个部分，包括： 对 Raft 算法实现成员变更和领导变更 在 raftstore 上实现Conf change和 region split 引入 scheduler ","date":"2024-08-14","objectID":"/posts/tinykv_p3_translate/:1:0","tags":["raft","kv","tinykv"],"title":"TinyKV Project 3 文档翻译","uri":"/posts/tinykv_p3_translate/"},{"categories":["kv"],"content":"Part A 在这一部分中，你将在基本的 Raft 算法上实现成员变更和领导者变更，这些功能是后面两部分所需要的。成员变更，即 confChange，用于添加或删除 peer 到Raft Group，这可能会改变 RaftGroup 的节点数目，所以要小心。领导权变更，即领导权转移，用于将领导权转移给另一个 peer，这对平衡调度器非常有用。 代码 你需要修改的代码都是关于 raft/raft.go​ 和 raft/rawnode.go​ 的，也可以参见proto/proto/eraft.proto​以了解你需要处理的新信息。confChange 和 leader transfer 都是由上层程序触发的，所以你可能想从 raft/rawnode.go​ 开始。 实现领导者转移 为了实现领导者的转移，让我们引入两个新的消息类型，MsgTransferLeader​ 和MsgTimeoutNow​。为了转移领导权，你需要首先在当前领导上调用带有MsgTransferLeader​ 消息的 raft.Raft.Step​，为了确保转移的成功，当前领导应该首先检查被转移者（即转移目标）的资格，比如：被转移者的日志是否为最新的，等等。如果被转移者不合格，当前领导可以选择放弃转移或者帮助被转移者，既然放弃对程序本身没有帮助，就选择帮助被转移者吧。如果被转移者的日志不是最新的，当前的领导者应该向被转移者发送 MsgAppend​ 消息，并停止接受新的 propose，以防我们最终会出现循环。因此，如果被转移者符合条件（或者在现任领导的帮助下），领导应该立即向被转移者发送 MsgTimeoutNow​ 消息，在收到 MsgTimeoutNow​ 消息后，被转移者应该立即开始新的选举，无论其选举超时与否，被转移者都有很大机会让现任领导下台，成为新领导者。 实现成员变更 这里要实现的 conf change 算法不是扩展Raft论文中提到的联合共识算法，联合共识算法可以一次性增加和/或移除任意 peer，相反，这个算法只能一个一个地增加或移除 peer，这更简单，更容易推理。此外，ConfChange从调用领导者的raft.RawNode.ProposeConfChange​开始，它将提出一个日志，其中pb.Entry.EntryType​设置为EntryConfChange​，pb.Entry.Data​设置为输入pb.ConfChange​ 。当 EntryConfChange​ 类型的日志被提交时，你必须通过RawNode.ApplyConfChange​ 与日志中的 pb.ConfChange​ 一起应用它，只有这样你才能根据 pb.ConfChange​ 通过 raft.Raft.addNode​ 和 raft.Raft.removeNode​ 向这个Raft 子节点添加或删除 peer。 提示： ​MsgTransferLeader​消息是本地消息，不是来自网络的。 将MsgTransferLeader​消息的Message.from​设置为被转移者（即转移目标）。 要立即开始新的选举，你可以用MsgHup​消息调用Raft.Step​ 调用 pb.ConfChange.Marshal​ 来获取 pb.ConfChange​ 的字节表示，并将其放入 pb.Entry.Data​。 ","date":"2024-08-14","objectID":"/posts/tinykv_p3_translate/:1:1","tags":["raft","kv","tinykv"],"title":"TinyKV Project 3 文档翻译","uri":"/posts/tinykv_p3_translate/"},{"categories":["kv"],"content":"Part B 由于 Raft 模块现在支持成员变更和领导变更，在这一部分中，你需要在 Part A 的基础上使 TinyKV 支持这些​ admin 命令。你可以在 proto/proto/raft_cmdpb.proto​ 中看到，有四种 admin 命令： CompactLog (已经在 Project2 的 PartC 实现) TransferLeader ChangePeer Split TransferLeader 和 ChangePeer 是基于 Raft 支持的领导变更和成员变更的命令。这些将被用作平衡调度器的基本操作步骤。Split 将一个 region 分割成两个 region，这是 multi Raft 的基础。你将一步一步地实现它们。 代码 所有的变化都是基于 Project2 的实现，所以你需要修改的代码都是关于 kv/raftstore/peer_msg_handler.go​ 和 kv/raftstore/peer.go​。 Propose TransferLeader 这一步相当简单。作为一个 Raft 命令，TransferLeader 将被 Propose 为一个Raft 日志项。但是 TransferLeader 实际上是一个动作，不需要复制到其他 peer，所以你只需要调用 RawNode 的 ​​TransferLeader()​​ 方法，而不是 TransferLeader 命令的Propose()​。 在raftstore中实现confChange confChange有两种不同的类型：AddNode 和 RemoveNode。正如它的名字所暗示的，它添加一个 Peer 或从 region 中删除一个 Peer。为了实现 confChange，你应该先学习 RegionEpoch 的概念。RegionEpoch 是 metapb.Region​ 的元信息的一部分。当一个 Region 增加或删除 Peer 或 Split 时，Region的poch​就会发生变化。RegionEpoch 的 ​​conf_ver​​ 在 ConfChange 期间增加，而​version​**在分裂期间增加。**它将被用来保证一个 Region 中的两个领导者在网络隔离下有最新的region信息。 你需要使 raftstore 支持处理 conf change 命令。这个过程是： 通过 ProposeConfChange​ 提出 conf change admin 命令 在日志被提交后，改变 RegionLocalState​，包括 RegionEpoch 和 Region 中的Peers。 调用 raft.RawNode​ 的 ApplyConfChange()​。 提示： 对于执行AddNode，新添加的 Peer 将由领导者的心跳来创建，查看storeWorker​ 的 maybeCreatePeer()​。在那个时候，这个 Peer 是未初始化的，它的 region 的任何信息对我们来说都是未知的，所以我们用 0 来初始化它的日志任期和索引。这时领导者会知道这个跟随者没有数据（存在一个从0到5的日志间隙），它将直接发送一个Snapshot给这个跟随者。 对于执行 RemoveNode，你应该明确地调用 destroyPeer()​ 来停止 Raft 模块。销毁逻辑是为你提供的。 不要忘记更新 ​GlobalContext​ 的 ​​storeMeta​​ 中的 ​​regionState​**。** 测试代码会多次安排一个 conf change 的命令，直到该 conf change 被应用，所以你需要考虑如何忽略同一 conf change 的重复命令。 在raftstore中实现region split ​ image2 ​ 为了支持 multi-raft，系统进行了数据分片，使每个 raft 组只存储一部分数据。Hash 和 Range 是常用的数据分片方式。TinyKV 使用 Range，主要原因是 Range 可以更好地聚合具有相同前缀的key，这对扫描等操作来说很方便。此外，Range在分片上比 Hash 更有优势。通常情况下，它只涉及元数据的修改，不需要移动数据。 message Region { uint64 id = 1; // Region key range [start_key, end_key). bytes start_key = 2; bytes end_key = 3; RegionEpoch region_epoch = 4; repeated Peer peers = 5 } 让我们重新审视一下 Region 的定义，它包括两个字段 start_key​ 和 end_key​，以表明 Region 所负责的数据范围。所以 Split 是支持多进程的关键步骤。在开始时，只有一个Region，其范围是[\"\", \"\")​。你可以把 key 空间看作一个圈，所以[\"\", \"\")​代表整个空间。随着数据的写入，SplitChecker​ 将在每一个 cfg.SplitRegionCheckTickInterval​ 检查 region 的大小（SplitChecker​如何获取region大小？），并在可能的情况下生成一个split_key​，将该 region 切割成两部分，你可以在 kv/raftstore/runner/split_check.go​ 中查看其逻辑。​split_key​将被包装成一个 MsgSplitRegion​，由 onPrepareSplitRegion()​ 处理。 为了确保新创建的 Region 和 Peers 的 id 是唯一的，这些 id 是由调度器分配的。onPrepareSplitRegion()​ 实际上为 pd Worker​ 安排了一个任务，向调度器索取id。并在收到调度器的响应后做出一个 Split admin命令，见kv/raftstore/runner/scheduler_task.go​ 中的 onAskSplit()​。 所以你的任务是实现处理 Split admin 命令的过程，就像 conf change 那样。提供的框架支持 multi-raft，见 kv/raftstore/router.go​。当一个 Region 分裂成两个 Region 时，其中一个 Region 将继承分裂前的元数据，只是修改其 Range 和 RegionEpoch，而另一个将创建相关的元信息。 提示: 这个新创建的 Region 的对应 Peer 应该由 createPeer()​ 创建，并注册到 router.regions​。而 region 的信息应该插入 ctx.StoreMeta​ 中的regionRanges​ 中。 对于有网络隔离的 region split 情况，要应用的快照可能会与现有 region 的范围有重叠。检查逻辑在 kv/raftstore/peer_msg_handler.go​ 的checkSnapshot()​ 中。请在实现时牢记这一点，并照顾到这种情况。 使用 engine_util.ExceedEndKey()​ 与 region 的 end key 进行比较。因为当end key 等于\" “时，任何 key 都将等于或大于” “。有更多的错误需要考虑。ErrRegionNotFound, ErrKeyNotInRegion, ErrEpochNotMatch。 ","date":"2024-08-14","objectID":"/posts/tinykv_p3_translate/:1:2","tags":["raft","kv","tinykv"],"title":"TinyKV Project 3 文档翻译","uri":"/posts/tinykv_p3_translate/"},{"categories":["kv"],"content":"Part C 正如上面所介绍的，我们的kv存储中的所有数据被分割成几个 region，每个region 都包含多个副本。一个问题出现了：我们应该把每个副本放在哪里？我们怎样才能找到副本的最佳位置？谁来发送以前的 AddPeer 和 RemovePeer 命令？Scheduler承担了这个责任。 为了做出明智的决定，Scheduler 应该拥有关于整个集群的一些信息。它应该知道每个 region 在哪里。它应该知道它们有多少个 key。它应该知道它们有多大…为了获得相关信息，Scheduler 要求每个 region 定期向 Scheduler 发送一个心跳请求。你可以在 /proto/proto/schedulerpb.proto​ 中找到心跳请求结构 RegionHeartbeatRequest​。在收到心跳后，调度器将更新本地 region 信息。 同时，调度器会定期检查 region 信息，以发现我们的 TinyKV 集群中是否存在不平衡现象。例如，如果任何 store 包含了太多的 region，region 应该从它那里转移到其他 store 。这些命令将作为相应 region 的心跳请求的响应被接收。 在这一部分，你将需要为 Scheduler 实现上述两个功能。按照我们的指南和框架，这不会太难。 代码 需要修改的代码都是关于 scheduler/server/cluster.go​ 和 scheduler/server/schedulers/balance_region.go​ 的。如上所述，当调度器收到一个 region 心跳时，它将首先更新其本地 region 信息。然后，它将检查是否有这个 region 的未决命令。如果有，它将作为响应被发送回来。 你只需要实现 processRegionHeartbeat​ 函数，其中 Scheduler 更新本地信息；以及 balance_region 的 Scheduler 函数，其中 Scheduler 扫描 Store 并确定是否存在不平衡以及它应该移动哪个 region。 收集区域心跳 正如你所看到的，processRegionHeartbeat​ 函数的唯一参数是一个 regionInfo​。它包含了关于这个心跳的发送者 region 的信息。Scheduler 需要做的仅仅是更新本地region 记录。但是，它应该为每次心跳更新这些记录吗？ 肯定不是！有两个原因。有两个原因。一个是当这个 region 没有变化时，更新可能被跳过。更重要的一个原因是，Scheduler 不能相信每一次心跳。特别是说，如果集群在某个部分有分区，一些节点的信息可能是错误的。 例如，一些 Region 在被分割后会重新启动选举和分割，但另一批孤立的节点仍然通过心跳向 Scheduler 发送过时的信息。所以对于一个 Region 来说，两个节点中的任何一个都可能说自己是领导者，这意味着 Scheduler 不能同时信任它们。 哪一个更可信呢？Scheduler 应该使用 ​conf_ver​ 和 version​ 来确定，即 RegionEpoch​​​。Scheduler 应该首先比较两个节点的 Region version​ 的值。如果数值相同，Scheduler 会比较 conf_ver​ 的数值。拥有较大 conf_ver​ 的节点必须拥有较新的信息。 简单地说，你可以按以下方式组织检查程序： 检查本地存储中是否有一个具有相同 Id 的 region。如果有，并且至少有一个心跳的 conf_ver 和版本小于它，那么这个心跳 region 就是过时的。 如果没有，则扫描所有与之重叠的区域。心跳的 conf_ver​ 和 version​ 应该大于或等于所有的，否则这个 region 是陈旧的。 那么 Scheduler 如何确定是否可以跳过这次更新？我们可以列出一些简单的条件。 如果新的 version​ 或 conf_ver​ 大于原来的版本，就不能被跳过。 如果领导者改变了，它不能被跳过 如果新的或原来的有挂起的 peer，它不能被跳过。 如果 ApproximateSize 发生变化，则不能跳过。 … 不要担心。你不需要找到一个严格的充分和必要条件。冗余的更新不会影响正确性。 如果 Scheduler 决定根据这个心跳来更新本地存储，有两件事它应该更新：region tree 和 store status。你可以使用 RaftCluster.core.PutRegion​ 来更新 region-tree ，并使用 RaftCluster.core.UpdateStoreStatus​ 来更新相关存储的状态（如领导者数量、区域数量、待处理的 peer 数量…）。 实现 region balance 调度器 在调度器中可以有许多不同类型的调度器在运行，例如，balance_region 调度器和balance_leader 调度器。这篇学习材料将集中讨论 balance_region 调度器。 每个调度器都应该实现了 Scheduler​ 接口，你可以在 /scheduler/server/schedule/scheduler.go​ 中找到它。调度器将使用 GetMinInterval​ 的返回值作为默认的时间间隔来定期运行 Schedule​ 方法。如果它的返回值为空（有几次重试），Scheduler​ 将使用 GetNextInterval​ 来增加间隔时间。通过定义 GetNextInterval​，你可以定义时间间隔的增加方式。如果它返回一个操作符，Scheduler​ 将派遣这些操作符作为相关区域的下一次心跳的响应。 ​Scheduler​ 接口的核心部分是 Schedule​ 方法。这个方法的返回值是操作符，它包含多个步骤，如 AddPeer 和 RemovePeer。例如，MovePeer 可能包含 AddPeer、transferLeader 和 RemovePeer，你在前面的部分已经实现了。以下图中的第一个RaftGroup为例。调度器试图将 peer 从第三个 store 移到第四个 store。首先，它应该为第四个 store 添加 peer。然后它检查第三家是否是领导者，发现不是，所以不需要转移领导者。然后，它删除第三个 store 的 peer。 你可以使用 scheduler/server/schedule/operator​ 包中的CreateMovePeerOperator​ 函数来创建一个 MovePeer​ 操作。 ​ image3 ​ ​ image4 在这一部分，你需要实现的唯一函数是scheduler/server/schedulers/balance_region.go​ 中的 Schedule​ 方法。这个调度器避免了在一个 store 里有太多的 region。首先，Scheduler 将选择所有合适的 store。然后根据它们的 region 大小进行排序。然后，调度器会尝试从 reigon 大小最大的 store 中找到要移动的 region。 调度器将尝试找到最适合在 store 中移动的 region。首先，它将尝试选择一个挂起的 region，因为挂起可能意味着磁盘过载。如果没有一个挂起的 region，它将尝试找到一个 Follower region。如果它仍然不能挑选出一个 region，它将尝试挑选 Leader region。最后，它将挑选出要移动的 region，或者 Scheduler 将尝试下一个 region 大小较小的 store，直到所有的 store 都将被尝试。 在您选择了一个要移动的 region 后，调度器将选择一个 store 作为目标。实际上，调度器将选择 region 大小最小的 store 。然后，调度程序将通过检查原始 store 和目标 store 的 region 大小之间的差异来判断这种移动是否有价值。如果差异足够大，Scheduler 应该在目标 store 上分配一个新的 peer 并创建一个 Movepeer 操作。 正如你可能已经注意到的，上面的例程只是一个粗略的过程。还剩下很多问题： 哪些存储空间适合移动？ 简而言之，一个合适的 store 应该是 Up 的，而且 down 的时间不能超过集群的MaxStoreDownTime​，你可以通过 cluster.GetMaxStoreDownTime()​ 得到。 如何选择区域？ Scheduler 框架提供了三种方法来获取区域。GetPendingRegionsWithLock​, GetFollowersWithLock​ 和 GetLeadersWithLock​。Scheduler 可以从中获取相关region。然后你可以选择一个随机的region。 如何判断这个操作是否有价值？ 如果原始 region 和目标 region 的 region 大小差异太小，在我们将 region 从原始 store 移动到目标 store 后，Scheduler 可能希望下次再移动回来。所以我们要确保这个差值必须大于 region 近似大小的2倍，这样才能保证移动后，目标 store 的 region 大小仍然小于原 store。 ","date":"2024-08-14","objectID":"/posts/tinykv_p3_translate/:1:3","tags":["raft","kv","tinykv"],"title":"TinyKV Project 3 文档翻译","uri":"/posts/tinykv_p3_translate/"},{"categories":["kv"],"content":"project2c目的是实现RaftLog GC和Snapshot支持。在Raft和raftstore中均需要修改和新增代码。 ","date":"2024-08-14","objectID":"/posts/tinykv_p2c/:0:0","tags":["raft","kv","tinykv"],"title":"TinyKV Project 2C","uri":"/posts/tinykv_p2c/"},{"categories":["kv"],"content":"问题分析 raft 一致性算法并没有考虑log无限增长的情况，若不做任何处理，随着系统的长时间运行，Raft节点中的RaftLog会占用大量内存；所以要引进applied index，把applied之前的条目定期压缩（compact）起来然后落盘，最后在内存删除它们，只需要记录最后applied的Index、Term​，以及一些状态。这就是RaftLog GC。 基于RaftLog GC，我们需要实现Snapshot支持来保障Raft算法的正常运行。这主要是在日志复制的过程中，leader需要给follower发送[next, LastIndex]​的条目以及next-1​的index和Term，很可能next-1​（最小索引）的条目已经被compact掉了，此时没法完成日志复制所需的匹配动作，因此leader需要发送一个Snapshot来帮助follower赶上进度。事实上，Project3中实现增加节点（Add Peer​）时，也是通过Snapshot来初始化新节点。 ","date":"2024-08-14","objectID":"/posts/tinykv_p2c/:1:0","tags":["raft","kv","tinykv"],"title":"TinyKV Project 2C","uri":"/posts/tinykv_p2c/"},{"categories":["kv"],"content":"Snapshot的生成 tinykv已经提供了Raft节点获取Snapshot的接口r.RaftLog.storage.Snapshot()​。可以发现是一个异步的实现，将一个任务丢给了ps.regionSched​，这是消费者端。 之所以采用异步实现，是因为SnapShot通常比较大，所以一般Leader第一次调用r.RaftLog.storage.Snapshot()​可能拿不到结果，不过worker已经开始生成了，等后面再调用时就能够直接返回。 // schedule snapshot generate task ps.regionSched \u003c- \u0026runner.RegionTaskGen{ RegionId: ps.region.GetId(), Notifier: ch, } 生产者端是在kv/raftstore/runner/region_task.go（根据RegionTaskGen​就可以找到） func (r *regionTaskHandler) Handle(t worker.Task) { switch t.(type) { case *RegionTaskGen: task := t.(*RegionTaskGen) // It is safe for now to handle generating and applying snapshot concurrently, // but it may not when merge is implemented. r.ctx.handleGen(task.RegionId, task.Notifier) case *RegionTaskApply: task := t.(*RegionTaskApply) r.ctx.handleApply(task.RegionId, task.Notifier, task.StartKey, task.EndKey, task.SnapMeta) case *RegionTaskDestroy: task := t.(*RegionTaskDestroy) r.ctx.cleanUpRange(task.RegionId, task.StartKey, task.EndKey) } } 最后会走到doSnapshot​当中，关键的部分如下。这样就生成了一个SnapShot snapshot := \u0026eraftpb.Snapshot{ Metadata: \u0026eraftpb.SnapshotMetadata{ Index: key.Index, Term: key.Term, ConfState: \u0026confState, }, } raft leader 发现某个节点落后较多（该节点的Next-1位置的entry已经被leader compact了），则给他发送一个Snapshot ","date":"2024-08-14","objectID":"/posts/tinykv_p2c/:1:1","tags":["raft","kv","tinykv"],"title":"TinyKV Project 2C","uri":"/posts/tinykv_p2c/"},{"categories":["kv"],"content":"Snapshot的Apply SnapShot的Apply也应该采用如上的方法，以异步的方式，创意一个RegionTaskApply​结构体丢给ps.regionSched​。 ‍ ","date":"2024-08-14","objectID":"/posts/tinykv_p2c/:1:2","tags":["raft","kv","tinykv"],"title":"TinyKV Project 2C","uri":"/posts/tinykv_p2c/"},{"categories":["kv"],"content":"在Raft中实现 对于Snapshot，在raft模块我们主要需要实现“leader获取并发送SnapShot”，“Follower处理Snapshot消息”，“log”，“ready”四块功能。 ​pb.Snapshot​类定义： type Snapshot struct { Data []byte `protobuf:\"bytes,1,opt,name=data,proto3\" json:\"data,omitempty\"` Metadata *SnapshotMetadata `protobuf:\"bytes,2,opt,name=metadata\" json:\"metadata,omitempty\"` XXX_NoUnkeyedLiteral struct{} `json:\"-\"` XXX_unrecognized []byte `json:\"-\"` XXX_sizecache int32 `json:\"-\"` } type SnapshotMetadata struct { ConfState *ConfState `protobuf:\"bytes,1,opt,name=conf_state,json=confState\" json:\"conf_state,omitempty\"` Index uint64 `protobuf:\"varint,2,opt,name=index,proto3\" json:\"index,omitempty\"` Term uint64 `protobuf:\"varint,3,opt,name=term,proto3\" json:\"term,omitempty\"` XXX_NoUnkeyedLiteral struct{} `json:\"-\"` XXX_unrecognized []byte `json:\"-\"` XXX_sizecache int32 `json:\"-\"` } type ConfState struct { // all node id Nodes []uint64 `protobuf:\"varint,1,rep,packed,name=nodes\" json:\"nodes,omitempty\"` XXX_NoUnkeyedLiteral struct{} `json:\"-\"` XXX_unrecognized []byte `json:\"-\"` XXX_sizecache int32 `json:\"-\"` } ​​其中Metadata​是我们需要注意的关键数据。 ","date":"2024-08-14","objectID":"/posts/tinykv_p2c/:2:0","tags":["raft","kv","tinykv"],"title":"TinyKV Project 2C","uri":"/posts/tinykv_p2c/"},{"categories":["kv"],"content":"leader获取并发送SnapShot 在“问题分析”中提到raft leader 发现某个节点落后较多（该节点的Next-1位置的entry已经被leader compact了），则给他发送一个Snapshot。 Compact实际做的事情就是日志截断。这里“被leader compact”的含义是由于截断leader中已经找不到Next-1（即论文中的preLogIndex​）位置的entry了，所以没办法通过单纯的sendAppend​进行日志同步（没办法获取preLogTerm​）。 所以需要做的修改是要在​**sendAppend**​加入检查并发送Snapshot的逻辑。 那么首先的一个问题就是如何检查next-1​位置的entry已经被compact了？ 已知preLogIndex=next-1​，通过r.RaftLog.Term(preLogIndex)​来获取preLogTerm​。 func (l *RaftLog) Term(i uint64) (uint64, error) { // Your Code Here (2A). if len(l.entries) \u003e 0 \u0026\u0026 i \u003e= l.dummyIndex { if i \u003e l.LastIndex() { return 0, ErrUnavailable } return l.entries[i-l.dummyIndex].Term, nil } else { term, err := l.storage.Term(i) return term, err } } 如果发生compact，那么显然上述调用会走到else分支，再来看一下l.storage.Term​的实现kv/raftstore/peer_storage.go。 func (ps *PeerStorage) Term(idx uint64) (uint64, error) { if idx == ps.truncatedIndex() { return ps.truncatedTerm(), nil } if err := ps.checkRange(idx, idx+1); err != nil { return 0, err } if ps.truncatedTerm() == ps.raftState.LastTerm || idx == ps.raftState.LastIndex { return ps.raftState.LastTerm, nil } var entry eraftpb.Entry if err := engine_util.GetMeta(ps.Engines.Raft, meta.RaftLogKey(ps.region.Id, idx), \u0026entry); err != nil { return 0, err } return entry.Term, nil } func (ps *PeerStorage) checkRange(low, high uint64) error { if low \u003e high { return errors.Errorf(\"low %d is greater than high %d\", low, high) } else if low \u003c= ps.truncatedIndex() { return raft.ErrCompacted } else if high \u003e ps.raftState.LastIndex+1 { return errors.Errorf(\"entries' high %d is out of bound, lastIndex %d\", high, ps.raftState.LastIndex) } return nil } 仔细阅读上面的代码，在checkRange​函数中找到了一个错误ErrCompacted​，它的条件是low \u003c= ps.truncatedIndex()​，即小于被截断的最高Index，符合我们上面描述的Compact的操作。因此可以确定只要对这个错误进行检查即可。 preLogTerm, err := r.RaftLog.Term(preLogIndex) if err != nil { if err == ErrCompacted { r.sendSnapshot(to) return false } return false } 接下来考虑实现sendSnapshot​这一函数。 首先是Snapshot的获取，根据在“Snapshot生成”中的描述，通过r.RaftLog.storage.Snapshot()​可以异步生成一个SnapShot，由于SnapShot通常比较大，因此第一次调用可能会返回错误，即SnapShot还没有准备好，因此要对这种情况做判断，对应的错误是ErrSnapshotTemporailyUnavailable​。 func (r *Raft) sendSnapshot(to uint64) { // Your Code Here (2C). snap, err := r.RaftLog.storage.Snapshot() // because snapshot is handled asynchronously, so we should check if snapshot is valid if err != nil { if err == ErrSnapshotTemporarilyUnavailable { return } panic(err) } r.msgs = append(r.msgs, pb.Message{ MsgType: pb.MessageType_MsgSnapshot, From: r.id, To: to, Term: r.Term, Snapshot: \u0026snap, }) // avoid snapshot is sent too frequently r.Prs[to].Next = snap.Metadata.Index + 1 } 注意，在发送Snapshot成功之后，可以直接在leader更新目标节点的Next，避免需要频繁发送snapshot造成较高的带宽占用。 ‍ ","date":"2024-08-14","objectID":"/posts/tinykv_p2c/:2:1","tags":["raft","kv","tinykv"],"title":"TinyKV Project 2C","uri":"/posts/tinykv_p2c/"},{"categories":["kv"],"content":"在raftstore中实现 ","date":"2024-08-14","objectID":"/posts/tinykv_p2c/:3:0","tags":["raft","kv","tinykv"],"title":"TinyKV Project 2C","uri":"/posts/tinykv_p2c/"},{"categories":["kv"],"content":"processAdminRequest 根据文档和之前的分析，需要在process​的逻辑中增加对AdminCmdType_CompactLog​这一消息的处理，不同于普通的Request​，它的类型是AdminRequest​，要分开处理。 func (d *peerMsgHandler) processAdminRequest(entry *eraftpb.Entry, cmd *raft_cmdpb.RaftCmdRequest) func (d *peerMsgHandler) process(entry *eraftpb.Entry) { cmd := \u0026raft_cmdpb.RaftCmdRequest{} cmd.Unmarshal(entry.Data) if cmd.AdminRequest != nil { d.processAdminRequest(entry, cmd) } ... } 对该消息的处理流程，文档中说明的比较详细，即先更新applyState.TruncatedState​的状态，然后通过接口d.ScheduleCompactLog​为 raftlog-gc worker 安排一个任务。Raftlog-gc worker 会异步完成实际的日志删除工作。 ‍ ","date":"2024-08-14","objectID":"/posts/tinykv_p2c/:3:1","tags":["raft","kv","tinykv"],"title":"TinyKV Project 2C","uri":"/posts/tinykv_p2c/"},{"categories":["kv"],"content":"ApplySnapshot ​appluSnapshot​即peer_storage​对于Ready()​获得的snapshot​进行实际应用，要做的事情基本上能根据前面的分析推出来：删除过时的数据（所有的数据）、更新各种状态、发送任务给region_worker进行实际应用。 删除过时数据，根据注释可知是ClearMeta​和ps.clearExtraData​，所需要的参数也十分简单。 ps.clearMeta(kvWB, raftWB) ps.clearExtraData(snapData.Region) 更新peer_storage的内存状态，RaftLocalState​ 、RaftApplyState​和 RegionLocalState​。 简要分析一下需要更新哪些状态，首先snapshot.MetaData​只有Index、Term、ConfState​三个字段。对于RaftLocalState​，很显然只要更新LastIndex​、LastTerm​（HardState​的更新在ApplySnapshot​的上层SaveReadyState​当中）。对于RaftApplyState​，AppliedIndex​肯定要更新到meta.Index​，TruncatedState​代表截断状态，同样有Index、Term​两个字段，很显然也要更新。文档中还指明“您还需要更新PeerStorage.snapState​到snap.SnapState_Applying​”。 type RaftApplyState struct { // Record the applied index of the state machine to make sure // not apply any index twice after restart. AppliedIndex uint64 `protobuf:\"varint,1,opt,name=applied_index,json=appliedIndex,proto3\" json:\"applied_index,omitempty\"` // Record the index and term of the last raft log that have been truncated. (Used in 2C) TruncatedState *RaftTruncatedState `protobuf:\"bytes,2,opt,name=truncated_state,json=truncatedState\" json:\"truncated_state,omitempty\"` XXX_NoUnkeyedLiteral struct{} `json:\"-\"` XXX_unrecognized []byte `json:\"-\"` XXX_sizecache int32 `json:\"-\"` } 但是文档中提到的RegionLocalState​怎么更新？ type PeerStorage struct { // current region information of the peer region *metapb.Region // current raft state of the peer raftState *rspb.RaftLocalState // current apply state of the peer applyState *rspb.RaftApplyState // current snapshot state snapState snap.SnapState // regionSched used to schedule task to region worker regionSched chan\u003c- worker.Task // generate snapshot tried count snapTriedCnt int // Engine include two badger instance: Raft and Kv Engines *engine_util.Engines // Tag used for logging Tag string } 根据PeerStorage​定义，推断metapb.Region​即RegionLocalState​。在Snapshot.MetaData​中似乎没有找到与之相关的内容。 type Region struct { Id uint64 `protobuf:\"varint,1,opt,name=id,proto3\" json:\"id,omitempty\"` // Region key range [start_key, end_key). StartKey []byte `protobuf:\"bytes,2,opt,name=start_key,json=startKey,proto3\" json:\"start_key,omitempty\"` EndKey []byte `protobuf:\"bytes,3,opt,name=end_key,json=endKey,proto3\" json:\"end_key,omitempty\"` RegionEpoch *RegionEpoch `protobuf:\"bytes,4,opt,name=region_epoch,json=regionEpoch\" json:\"region_epoch,omitempty\"` Peers []*Peer `protobuf:\"bytes,5,rep,name=peers\" json:\"peers,omitempty\"` XXX_NoUnkeyedLiteral struct{} `json:\"-\"` XXX_unrecognized []byte `json:\"-\"` XXX_sizecache int32 `json:\"-\"` } 但是在已经写好的代码当中，有一个RaftSnapshotData​类型的snapData​是将Snapshot​中的Data​解析后的数据，可以看到其中的region​数据，正是想要的。 type RaftSnapshotData struct { Region *metapb.Region `protobuf:\"bytes,1,opt,name=region\" json:\"region,omitempty\"` FileSize uint64 `protobuf:\"varint,2,opt,name=file_size,json=fileSize,proto3\" json:\"file_size,omitempty\"` Data []*KeyValue `protobuf:\"bytes,3,rep,name=data\" json:\"data,omitempty\"` Meta *SnapshotMeta `protobuf:\"bytes,5,opt,name=meta\" json:\"meta,omitempty\"` XXX_NoUnkeyedLiteral struct{} `json:\"-\"` XXX_unrecognized []byte `json:\"-\"` XXX_sizecache int32 `json:\"-\"` } // Apply the peer with given snapshot func (ps *PeerStorage) ApplySnapshot(snapshot *eraftpb.Snapshot, kvWB *engine_util.WriteBatch, raftWB *engine_util.WriteBatch) (*ApplySnapResult, error) { log.Infof(\"%v begin to apply snapshot\", ps.Tag) snapData := new(rspb.RaftSnapshotData) if err := snapData.Unmarshal(snapshot.Data); err != nil { return nil, err } // Hint: things need to do here including: update peer storage state like raftState and applyState, etc, // and send RegionTaskApply task to region worker through ps.regionSched, also remember call ps.clearMeta // and ps.clearExtraData to delete stale data // Your Code Here (2C). return nil, nil } 但是想用kvWB.SetMeta(meta.RegionStateKey(snapData.Region.Id), snapData.Region)​来持久化时却发现snapData.Region​参数不匹配，*rspb.RegionLocalState​。 后面在kv/raftstore/meta/values.go找到了WriteRegionState​函数，正是我们想要的。 func WriteRegionState(kvWB *engine_util.WriteBatch, region *metapb.","date":"2024-08-14","objectID":"/posts/tinykv_p2c/:3:2","tags":["raft","kv","tinykv"],"title":"TinyKV Project 2C","uri":"/posts/tinykv_p2c/"},{"categories":["kv"],"content":"Request执行流程和调用链分析 梳理tinykv中Request执行流程和调用链有助于理解project2B文档的要求和需要实现的逻辑，推荐看一看Talent Plan学习营分享课的对应部分，里面对Request执行流程分析的很到位。 Talent Plan 2021 KV 学习营分享课 (pingcap.cn) 我这里就照搬一下原视频中的图片。 ​ Request执行流程 ​ ‍ ","date":"2024-08-14","objectID":"/posts/tinykv_p2b/:1:0","tags":["raft","kv","tinykv"],"title":"TinyKV Project 2B","uri":"/posts/tinykv_p2b/"},{"categories":["kv"],"content":"1 Server -\u003e RaftStorage 当一个请求req到达时，首先Server对其进行一层封装传到RaftStorage。 server接口：proto/pkg/tinykvpb/tinykvpb.pb.go // Server API for TinyKv service type TinyKvServer interface { // KV commands with mvcc/txn supported. KvGet(context.Context, *kvrpcpb.GetRequest) (*kvrpcpb.GetResponse, error) KvScan(context.Context, *kvrpcpb.ScanRequest) (*kvrpcpb.ScanResponse, error) KvPrewrite(context.Context, *kvrpcpb.PrewriteRequest) (*kvrpcpb.PrewriteResponse, error) KvCommit(context.Context, *kvrpcpb.CommitRequest) (*kvrpcpb.CommitResponse, error) KvCheckTxnStatus(context.Context, *kvrpcpb.CheckTxnStatusRequest) (*kvrpcpb.CheckTxnStatusResponse, error) KvBatchRollback(context.Context, *kvrpcpb.BatchRollbackRequest) (*kvrpcpb.BatchRollbackResponse, error) KvResolveLock(context.Context, *kvrpcpb.ResolveLockRequest) (*kvrpcpb.ResolveLockResponse, error) // RawKV commands. RawGet(context.Context, *kvrpcpb.RawGetRequest) (*kvrpcpb.RawGetResponse, error) RawPut(context.Context, *kvrpcpb.RawPutRequest) (*kvrpcpb.RawPutResponse, error) RawDelete(context.Context, *kvrpcpb.RawDeleteRequest) (*kvrpcpb.RawDeleteResponse, error) RawScan(context.Context, *kvrpcpb.RawScanRequest) (*kvrpcpb.RawScanResponse, error) // Raft commands (tinykv \u003c-\u003e tinykv). Raft(TinyKv_RaftServer) error Snapshot(TinyKv_SnapshotServer) error // Coprocessor Coprocessor(context.Context, *coprocessor.Request) (*coprocessor.Response, error) } server实现：kv/server/server.go type Server struct { storage storage.Storage // (Used in 4B) Latches *latches.Latches // 锁集合，实现多版本并发控制 // coprocessor API handler, out of course scope copHandler *coprocessor.CopHandler } 目前主要需要看Storage，在此处使用的是kv/storage/raft_storage/raft_server.go中实现的RaftStorage​。 以它的Write​功能为例，首先将batch​中的put、Get、Delete​抽出来封装在raft_cmdpb.Request​中，再添加一些header​等打包成一个RaftCommand​发送给下层 ​RaftStorage​就是Project2和Project3中需要实现的代码处理 ","date":"2024-08-14","objectID":"/posts/tinykv_p2b/:1:1","tags":["raft","kv","tinykv"],"title":"TinyKV Project 2B","uri":"/posts/tinykv_p2b/"},{"categories":["kv"],"content":"2 RaftStorage -\u003e Peer RaftStorage定义如下： type RaftStorage struct { engines *engine_util.Engines config *config.Config node *raftstore.Node snapManager *snap.SnapManager raftRouter *raftstore.RaftstoreRouter raftSystem *raftstore.Raftstore resolveWorker *worker.Worker snapWorker *worker.Worker wg sync.WaitGroup } type RaftstoreRouter struct { router *router } type router struct { peers sync.Map // regionID -\u003e peerState peerSender chan message.Msg storeSender chan\u003c- message.Msg } 这里需要注意的是RaftstoreRouter​，它是对router​的封装。router中有两个channel，都是生产者，作为中心的消息路由。RaftstoreRouter​增加了两个意义更加明确的函数SendTaftMessage​、SendRaftCommand​，用于传递消息给peer​。调用router​的send​接口就会把Msg传递给peerSender​这个channel。 以上就是消息生产者的部分，接下来看消费者的部分。 ​Server​传到RaftStorage​的消息分kv​和raft​，对应RaftStorage​封装后分别是RaftCmd​和RaftMessage​，经过Rounter​，RaftCmd​会走到RatfWorker​（peer层前面专门处理Msg）里面，RaftMessage​会给到StoreWorker​，它会做一些region上的处理，同时把Msg再送到RaftWoker​里面去。 kv/raftstore/raft_worker.go：raftWorker​作为消费者，可以看到在初始创建的时候会拿到router​的peerSender​channel type raftWorker struct { pr *router // receiver of messages should sent to raft, including: // * raft command from `raftStorage` // * raft inner messages from other peers sent by network raftCh chan message.Msg ctx *GlobalContext closeCh \u003c-chan struct{} } func newRaftWorker(ctx *GlobalContext, pm *router) *raftWorker { return \u0026raftWorker{ raftCh: pm.peerSender, ctx: ctx, pr: pm, } } ​raftWorker​跑起来之后会批量地取peerSender​中Msg，取了之后通过当前的PeerState生成PeerMsgHandler​来HandleMsg​。 func (rw *raftWorker) run(closeCh \u003c-chan struct{}, wg *sync.WaitGroup) { defer wg.Done() var msgs []message.Msg for { msgs = msgs[:0] select { case \u003c-closeCh: return case msg := \u003c-rw.raftCh: msgs = append(msgs, msg) } pending := len(rw.raftCh) for i := 0; i \u003c pending; i++ { msgs = append(msgs, \u003c-rw.raftCh) } peerStateMap := make(map[uint64]*peerState) for _, msg := range msgs { peerState := rw.getPeerState(peerStateMap, msg.RegionID) if peerState == nil { continue } newPeerMsgHandler(peerState.peer, rw.ctx).HandleMsg(msg) } for _, peerState := range peerStateMap { newPeerMsgHandler(peerState.peer, rw.ctx).HandleRaftReady() } } } 因此消息传递的流程大致如下： ​ 消息传递流程 ​ ","date":"2024-08-14","objectID":"/posts/tinykv_p2b/:1:2","tags":["raft","kv","tinykv"],"title":"TinyKV Project 2B","uri":"/posts/tinykv_p2b/"},{"categories":["kv"],"content":"3 Peer -\u003e RawNode_Raft kv/raftstore/peer_msg_handler.go 就是在Project2中主要需要实现的代码文件。我们要实现的第一个func就是proposeRaftCommand​。此时由于请求还没走到Raft层，现在需要把它预先丢给Raft层，在经过共识之后才能实际地去应用。 func (d *peerMsgHandler) HandleMsg(msg message.Msg) { switch msg.Type { case message.MsgTypeRaftMessage: raftMsg := msg.Data.(*rspb.RaftMessage) if err := d.onRaftMsg(raftMsg); err != nil { log.Errorf(\"%s handle raft message error %v\", d.Tag, err) } case message.MsgTypeRaftCmd: raftCMD := msg.Data.(*message.MsgRaftCmd) d.proposeRaftCommand(raftCMD.Request, raftCMD.Callback) case message.MsgTypeTick: d.onTick() case message.MsgTypeSplitRegion: split := msg.Data.(*message.MsgSplitRegion) log.Infof(\"%s on split with %v\", d.Tag, split.SplitKey) d.onPrepareSplitRegion(split.RegionEpoch, split.SplitKey, split.Callback) case message.MsgTypeRegionApproximateSize: d.onApproximateRegionSize(msg.Data.(uint64)) case message.MsgTypeGcSnap: gcSnap := msg.Data.(*message.MsgGCSnap) d.onGCSnap(gcSnap.Snaps) case message.MsgTypeStart: d.startTicker() } } func (d *peerMsgHandler) proposeRaftCommand(msg *raft_cmdpb.RaftCmdRequest, cb *message.Callback) { err := d.preProposeRaftCommand(msg) if err != nil { cb.Done(ErrResp(err)) return } // Your Code Here (2B). } ","date":"2024-08-14","objectID":"/posts/tinykv_p2b/:1:3","tags":["raft","kv","tinykv"],"title":"TinyKV Project 2B","uri":"/posts/tinykv_p2b/"},{"categories":["kv"],"content":"4 RawNode_Raft -\u003e Peer 当日志条目被Raft层提交以后，根据Project2A​，Peer​会通过RawNode.Ready()​异步地获取已经被提交的条目进行应用。我们回到刚才kv/raftstore/raft_worker.go中run​函数，可以看到在调用HandleMsg(msg)​之后，会批量地生成PeerMsgHandler​来HandleRaftReady()​。 func (rw *raftWorker) run(closeCh \u003c-chan struct{}, wg *sync.WaitGroup) { defer wg.Done() var msgs []message.Msg for { msgs = msgs[:0] select { case \u003c-closeCh: return case msg := \u003c-rw.raftCh: msgs = append(msgs, msg) } pending := len(rw.raftCh) for i := 0; i \u003c pending; i++ { msgs = append(msgs, \u003c-rw.raftCh) } peerStateMap := make(map[uint64]*peerState) for _, msg := range msgs { peerState := rw.getPeerState(peerStateMap, msg.RegionID) if peerState == nil { continue } newPeerMsgHandler(peerState.peer, rw.ctx).HandleMsg(msg) } for _, peerState := range peerStateMap { newPeerMsgHandler(peerState.peer, rw.ctx).HandleRaftReady() } } } 我们实现的重点就是HandleRaftReady()​，这个函数执行的大致逻辑如下： 首先判断有没有Ready，这需要调用Raft层的判断函数HasReady()​（raft/rawnode.go），如果没有的话就说明共识还没完成，如果有的话就会进行Apply 如果要进行Apply的话，首先需要保存memory states 到磁盘当中，需要实现SaveReadyState()​（kv/raftstore/peer_storage.go），SaveReadyState​会做两件事情： 将当前的Raft日志持久化，把已经Ready，可以Apply的一些状态保存下来 保存snapshot（2C实现） 接下来就是实际上去进行Apply的逻辑，对kv进行实际的写入读取等操作。 在此之后，我们就可以Advance​（raft/rawnode.go）了。这个我们在Project2A已经完成了，Advance​做的事情也就是在实际完成Apply之后，Raft层中的节点的RaftLog​中的stable、applied​进行更新，同时也要更新一下RaftNode​中的prevSoftState​、prevHardState​（用于后续的状态对比HasReady​）。 ‍ 总结一下，我们在Project2B中需要实现Request流程中的以下几个环节： Propose，通过peerMsgHandler.proposeRaftCommand​向Raft层传递日志条目 获取Ready -\u003e Stable log（对应kv/raftstore/peer_storage.go中的PeerStorage.SaveReadyState()​） -\u003e Apply（同时CallBack）-\u003eAdvance更新Raft节点状态，这些都是peerMsgHandler.HandleRaftReady()​需要实现的。 ‍ 下面讲述Project2B的实现思路 ","date":"2024-08-14","objectID":"/posts/tinykv_p2b/:1:4","tags":["raft","kv","tinykv"],"title":"TinyKV Project 2B","uri":"/posts/tinykv_p2b/"},{"categories":["kv"],"content":"PeerStorage PeerStorage部分主要实现SaveReadyState​函数，对应Stable log环节，顾名思义，需要持久化一些数据。 根据文档，这个函数的作用是将raft.Ready​中更新的数据保存到 badger 中（即持久化），包括追加日志条目、保存 Raft HardState、应用Snapshot（2C）等。 ","date":"2024-08-14","objectID":"/posts/tinykv_p2b/:2:0","tags":["raft","kv","tinykv"],"title":"TinyKV Project 2B","uri":"/posts/tinykv_p2b/"},{"categories":["kv"],"content":"Append实现 追加日志对应PeerStorage.Append()​，根据已有的注释，该函数将给定的条目Append到raftDB，并更新ps.raftState，还将删除永远不会提交的日志条目。 // Append the given entries to the raft log and update ps.raftState also delete log entries that will // never be committed func (ps *PeerStorage) Append(entries []eraftpb.Entry, raftWB *engine_util.WriteBatch) error { // Your Code Here (2B). return nil } 实际上就是封装好一个WriteBatch​，方便后续对raftDB​进行WriteToDB​，实现持久化。 执行步骤： ​if len(entries) == 0​，返回 获取stableLast := entries[len(entries)-1].Index​，prevLast, _ := ps.LastIndex()​，分别代表目前需要持久化的最大Index，和已经持久化的最大Index。 更新当前peerStorage的raftState的LastIndex​和LastTerm​ 遍历entries​，调用raftWB.SetMeta​实现Append 最后需要删除的条目对应索引[stableLast + 1, preLast]​，这些条目永远都不会提交（常见的一种情况是被覆盖掉了），通过调用raftWB.DeleteMeta​实现。 ","date":"2024-08-14","objectID":"/posts/tinykv_p2b/:2:1","tags":["raft","kv","tinykv"],"title":"TinyKV Project 2B","uri":"/posts/tinykv_p2b/"},{"categories":["kv"],"content":"SaveReadyState 实现 这个函数实现比较简单，目前不用实现ApplySnapshot，因此按以下步骤： Append日志条目 更新raftState.HardState​ 持久化日志条目和更新后的raftState​，也是通过SetMeta​实现 func (ps *PeerStorage) SaveReadyState(ready *raft.Ready) (*ApplySnapResult, error) { // Hint: you may call `Append()` and `ApplySnapshot()` in this function // Your Code Here (2B/2C). // 1. append wb := \u0026engine_util.WriteBatch{} ps.Append(ready.Entries, wb) // 2. update hardstate and save raftState if !raft.IsEmptyHardState(ready.HardState) { ps.raftState.HardState = \u0026ready.HardState } wb.SetMeta(meta.RaftStateKey(ps.region.Id), ps.raftState) // 3. write to DB ps.Engines.WriteRaft(wb) return nil, nil } ‍ ","date":"2024-08-14","objectID":"/posts/tinykv_p2b/:2:2","tags":["raft","kv","tinykv"],"title":"TinyKV Project 2B","uri":"/posts/tinykv_p2b/"},{"categories":["kv"],"content":"PeerMsgHandler 根据之前“Request执行流程和调用链分析”的描述，在PeerMsgHandler​中我们要实现proposeRaftCommand​和HandleRaftReady​两个函数。 ","date":"2024-08-14","objectID":"/posts/tinykv_p2b/:3:0","tags":["raft","kv","tinykv"],"title":"TinyKV Project 2B","uri":"/posts/tinykv_p2b/"},{"categories":["kv"],"content":"ProposeRaftCommand ​proposeRaftCommand​是将上层的RaftCmdRequest​打包传递给Raft节点。 根据文档提示，要对以下两个错误进行处理： ErrNotLeader：在追随者上Propose raftCmd。因此，使用它来让客户端尝试其他peer。 ErrStaleCommand：可能是由于领导者更改，某些日志未提交并被新的领导者日志覆盖。但客户并不知道这一点，仍在等待响应。因此，您应该返回此命令以让客户端知道并重试该命令。 很显然这里我们只要处理ErrNotLeader​这一错误，通过CallBack函数来返回错误响应。 if !d.IsLeader() { leader := d.getPeerFromCache(d.LeaderId()) cb.Done(ErrResp(\u0026util.ErrNotLeader{RegionId: d.regionId, Leader: leader})) return } 不过以上部分其实已经在d.preProposeRaftCommand(msg)​事先做好了。 func (d *peerMsgHandler) proposeRaftCommand(msg *raft_cmdpb.RaftCmdRequest, cb *message.Callback) { err := d.preProposeRaftCommand(msg) if err != nil { cb.Done(ErrResp(err)) return } // Your Code Here (2B). return } 接下来我们要在peer​当中保存callback，于是查看peer​结构体找到了关键的结构proposals​，注释也说得非常直白。 // Record the callback of the proposals // (Used in 2B) proposals []*proposal type proposal struct { // index + term for unique identification index uint64 term uint64 cb *message.Callback } 相关的函数在peer_msg_handler.go里面也为我们封装好了，直接调用即可 proposal := \u0026proposal{ index: d.nextProposalIndex(), term: d.Term(), cb: cb, } 紧接着Propose的逻辑，之前在2A部分提到过，上层的条目都是通过RawNode.Propose()​传递给Raft模块，因此这里是d.RaftGroup.Propose​，但是参数应该是[]byte​，所以这里需要对msg​做一些处理，在RaftCmdRequest​类下查找辅助函数，可以找到Marshal​正是我们需要的。完整代码如下： proposal := \u0026proposal{ index: d.nextProposalIndex(), term: d.Term(), cb: cb, } data, marErr := msg.Marshal() if marErr != nil { cb.Done(ErrResp(marErr)) return } perr := d.RaftGroup.Propose(data) if perr != nil { cb.Done(ErrResp(perr)) return } // log.DPrintf(\"kv/raftstore/peer_msg_handler.go: proposal: %v\", proposal) d.proposals = append(d.proposals, proposal) 注意d.proposals = append(d.proposals, proposal)​最后再调用，有可能前面的一些环节出错，例如Propose​到了非Leader节点，这时应该直接通过回调返回错误的响应。 ","date":"2024-08-14","objectID":"/posts/tinykv_p2b/:3:1","tags":["raft","kv","tinykv"],"title":"TinyKV Project 2B","uri":"/posts/tinykv_p2b/"},{"categories":["kv"],"content":"HandleRaftReady ​HandleRaftReady​是获取Raft节点是否有Ready（即状态更新），然后把这些更新进行持久化和实际应用的函数。是整个project2B的重点。 文档中对于它的执行逻辑描述比较详细，还贴心地给出了处理流程： for { select { case \u003c-s.Ticker: Node.Tick() default: if Node.HasReady() { rd := Node.Ready() saveToStorage(rd.State, rd.Entries, rd.Snapshot) send(rd.Messages) for _, entry := range rd.CommittedEntries { process(entry) } s.Node.Advance(rd) } } ​saveToStorage​实际上就是调用之前实现的peerStorage​的SaveReadyState​函数，作用就是做持久化。 ​send​是把Raft节点要转发给其他节点的消息转发出去。 ","date":"2024-08-14","objectID":"/posts/tinykv_p2b/:4:0","tags":["raft","kv","tinykv"],"title":"TinyKV Project 2B","uri":"/posts/tinykv_p2b/"},{"categories":["kv"],"content":"process 我们重点要实现的就是process​，即entry应用的逻辑。这里要理解一下“应用”的含义，就是把entry中的操作拿给kvDB​去实际执行（例如put、delete​某个数据），与之前持久化日志条目（写入raftDB​）本质上是一样的，都是给badger.DB​做持久化。但是“应用”还需要一个额外的操作，还记得之前存入peer.proposals​的callback函数么，之前遇到错误时直接调用cb.Done​返回了错误响应，这里成功“应用”之后，同样需要调用它返回成功的响应。 这也带来了一个新的问题，如果在proposals​中的RaftCmd​没有被raft节点提交并且被覆盖了呢？显然永远不会执行这个RaftCmd​了，还记得在一开始提到的错误处理么？这就是第二个需要检查的错误情况ErrStaleCommand​，给出错误的响应。 ErrStaleCommand：可能是由于领导者更改，某些日志未提交并被新的领导者日志覆盖。但客户并不知道这一点，仍在等待响应。因此，您应该返回此命令以让客户端知道并重试该命令。 新的问题又来了，怎么去识别这一问题的发生呢？ 仔细思考“日志未提交并被新的领导者日志覆盖”这一情况，同一个Index下的Term肯定发生了变化。因此只要对相同Index下的entry和proposals的Term进行检查就可以发现错误。 下面给出我的做法，对于每个应用完成的条目，调用clearStaleAndGetTargetProposal​，会首先根据Index清理已经过时的一些proposals，接着利用目标entry的index和term进行匹配，如果成功返回true，此时d.proposals[0]​就是目标proposal；否则返回false，没有找到对应的proposal，应用之后不做响应（这种情况通常是follower节点，因为follower节点中不会保存callback，它应用的条目是leader发送的而不是客户端发送的，不用负责返回响应）。 // if return true, d.proposals[0] is the target proposal func (d *peerMsgHandler) clearStaleAndGetTargetProposal(entry *eraftpb.Entry) bool { d.clearStaleProposals(entry) if len(d.proposals) \u003e 0 \u0026\u0026 d.proposals[0].index == entry.Index { p := d.proposals[0] if p.term != entry.Term { NotifyStaleReq(entry.Term, p.cb) d.proposals = d.proposals[1:] return false } else { return true } } else { return false } } func (d *peerMsgHandler) clearStaleProposals(entry *eraftpb.Entry) { var i int for i = 0; i \u003c len(d.proposals) \u0026\u0026 d.proposals[i].index \u003c entry.Index; i++ { d.proposals[i].cb.Done(ErrResp(\u0026util.ErrStaleCommand{})) } d.proposals = d.proposals[i:] } 2B目前只需要我们实现普通Request​处理（Put、Delete、Get、Snap​），而不需要对AdminRequest​进行处理（AdminRequest​主要为对Raft集群的一些操作，例如集群成员变更等等） 值得注意的是，对于普通Request​，一个entry可能包含多个操作，因此响应需要包含多个操作的响应；而对于AdminReuqest​，一个entry只能包含一个操作。具体可以看proto/pkg/raft_cmdpb/raft_cmdpb.pb.go中的RaftCmdResponse​类，可以看到Responses​是一个指针切片，而AdminResponse​则是一个指针。 type RaftCmdResponse struct { Header *RaftResponseHeader `protobuf:\"bytes,1,opt,name=header\" json:\"header,omitempty\"` Responses []*Response `protobuf:\"bytes,2,rep,name=responses\" json:\"responses,omitempty\"` AdminResponse *AdminResponse `protobuf:\"bytes,3,opt,name=admin_response,json=adminResponse\" json:\"admin_response,omitempty\"` XXX_NoUnkeyedLiteral struct{} `json:\"-\"` XXX_unrecognized []byte `json:\"-\"` XXX_sizecache int32 `json:\"-\"` } 为了方便进一步理解process普通Reuqest​的流程，给出关键部分的伪代码： var p *proposal resp := \u0026raft_cmdpb.RaftCmdResponse{ Header: \u0026raft_cmdpb.RaftResponseHeader{}, Responses: []*raft_cmdpb.Response{}, } for _, req := range cmd.Requests { switch req.CmdType { case raft_cmdpb.CmdType_Put: wb.SetCF(req.Put.Cf, req.Put.Key, req.Put.Value) case raft_cmdpb.CmdType_Delete: wb.DeleteCF(req.Delete.Cf, req.Delete.Key) case raft_cmdpb.CmdType_Get: case raft_cmdpb.CmdType_Snap: } if d.clearStaleAndGetTargetProposal(entry) { p = d.proposals[0] switch req.CmdType { case raft_cmdpb.CmdType_Put: resp.Responses = append(resp.Responses, \u0026raft_cmdpb.Response{CmdType: raft_cmdpb.CmdType_Put, Put: \u0026raft_cmdpb.PutResponse{}}) case raft_cmdpb.CmdType_Delete: resp.Responses = append(resp.Responses, \u0026raft_cmdpb.Response{CmdType: raft_cmdpb.CmdType_Delete, Delete: \u0026raft_cmdpb.DeleteResponse{}}) case raft_cmdpb.CmdType_Get: val, err := engine_util.GetCF(d.peerStorage.Engines.Kv, req.Get.Cf, req.Get.Key) if err != nil { panic(err) } resp.Responses = append(resp.Responses, \u0026raft_cmdpb.Response{CmdType: raft_cmdpb.CmdType_Get, Get: \u0026raft_cmdpb.GetResponse{Value: val}}) case raft_cmdpb.CmdType_Snap: resp.Responses = append(resp.Responses, \u0026raft_cmdpb.Response{CmdType: raft_cmdpb.CmdType_Snap, Snap: \u0026raft_cmdpb.SnapResponse{Region: d.Region()}}) p.cb.Txn = d.peerStorage.Engines.Kv.NewTransaction(false) default: resp.Responses = append(resp.Responses, \u0026raft_cmdpb.Response{CmdType: raft_cmdpb.CmdType_Invalid}) } } } if p != nil { p.cb.Done(resp) d.proposals = d.proposals[1:] } ","date":"2024-08-14","objectID":"/posts/tinykv_p2b/:4:1","tags":["raft","kv","tinykv"],"title":"TinyKV Project 2B","uri":"/posts/tinykv_p2b/"},{"categories":["kv"],"content":"这几天想写tinykv project2B的思路，但是感觉内容太多不知道从何处写起。思来想去觉得可以先翻译一下project2B的文档，将比较关键的地方标注出来。 ","date":"2024-08-14","objectID":"/posts/tinykv_p2b_translate/:0:0","tags":["raft","kv","tinykv"],"title":"TinyKV Project 2B 文档翻译","uri":"/posts/tinykv_p2b_translate/"},{"categories":["kv"],"content":"B部分 在这一部分中，您将使用 A 部分中实现的 Raft 模块构建一个容错的键值存储服务。你的键/值服务将是一个复制的状态机，由几个使用 Raft 进行复制的键/值服务器组成。只要大多数服务器处于活动状态并且可以通信，键/值服务就应继续处理客户端请求，即使存在其他故障或网络分区。 在 project1 中，您已经实现了一个独立的 kv 服务器，因此您应该已经熟悉 kv 服务器 Storage​ 的API 和接口。 在介绍代码之前，您需要先了解三个术语：Store​, Peer​ and Region​，以及proto/proto/metapb.proto​中定义的术语。 Store 代表 tinykv-server 的实例 Peer 代表运行在 Store 上的 Raft 节点 Region 是 Peers 的集合，也称为 Raft Group（Region将键空间以范围range划分，是实现Project3 MuitiRaft的关键概念，Project2中默认只有一个Region） ​ region ​ 为简单起见，project2 的 Store 上只有一个 Peer 节点，集群中只有一个 Region。因此，您现在无需考虑 Region 的范围。Project3 中将进一步引入多个 Region。 ","date":"2024-08-14","objectID":"/posts/tinykv_p2b_translate/:1:0","tags":["raft","kv","tinykv"],"title":"TinyKV Project 2B 文档翻译","uri":"/posts/tinykv_p2b_translate/"},{"categories":["kv"],"content":"the Code 首先，您应该看看kv/storage/raft_storage/raft_server.go​中的RaftStorage​，其也实现了Storage​接口。与StandaloneStorage​直接写入或读取底层引擎不同，它首先将每个写入和读取请求发送到 Raft，然后在 Raft 提交请求后对底层引擎进行实际的写入和读取。通过这种方式，可以保持多个Stores​之间的一致性。 ​RaftStorage​创建一个Raftstore​驱动 Raft。在调用Reader​orWrite​函数时，它实际上会通过通道（通道为raftWorker​的raftCh​）向 raftstore 发送一个定义在proto/proto/raft_cmdpb.proto​中的RaftCmdRequest​和四种基本命令类型（Get/Put/Delete/Snap），并在 Raft 提交并应用命令后利用回调向客户端返回响应。Reader​和Write​函数的参数kvrpc.Context​现在很有用，它从客户端的角度携带Region​信息，并作为RaftCmdRequest​的标头传递。信息可能不正确或过时，因此 raftstore 需要检查它们并决定是否将该请求Propose到 Raft 层进行共识。 然后，TinyKV的核心就来了——raftstore。结构有点复杂，请阅读 TiKV 参考文献，以便更好地理解设计： https://pingcap.com/blog-cn/the-design-and-implementation-of-multi-raft/#raftstore （中文版，非常推荐，对于整个Project2、Project3的理解有很大帮助，不过TiKV做了很多目前我们不用关心的优化，例如异步Apply、读写分离） https://pingcap.com/blog/design-and-implementation-of-multi-raft/#raftstore （英文版） raftstore的入口是Raftstore​，见kv/raftstore/raftstore.go​。它启动了一些工作线程异步处理特定任务，其中大多数现在没有使用，因此您可以忽略它们。您需要关注的只是raftWorker​.（kv/raftstore/raft_worker.go） 整个过程分为两部分：raft worker 轮询raftCh​以获取消息，包括驱动 Raft 模块的 base tick 和作为 Raft 条目提出（proposed）Raft 命令；它从 Raft 模块获取并处理 ready，处理流程包括转发 raft 消息、持久化状态、将提交的条目应用于状态机。应用后，还要将响应通过回调返回给客户端。 ","date":"2024-08-14","objectID":"/posts/tinykv_p2b_translate/:1:1","tags":["raft","kv","tinykv"],"title":"TinyKV Project 2B 文档翻译","uri":"/posts/tinykv_p2b_translate/"},{"categories":["kv"],"content":"实现peer storage peer storage是你通过 A 部分中的Storage​接口进行交互的内容，但除了 raft 日志之外，peer storage还管理其他持久化的元数据，这对于重启后恢复一致的状态机非常重要。此外，proto/proto/raft_serverpb.proto​中定义了三个重要状态： RaftLocalState：用于存储当前 Raft 的 HardState 和最后一个 Log 的 Index。 RaftApplyState：用于存储 Raft 应用的最后一个 Log 索引和一些截断的 Log 信息。 RegionLocalState：用于存储此 Store 上的 Region 信息和相应的 Peer State。Normal 表示该 Peer 正常，Tombstone 表示该 Peer 已从 Region 中移除，无法加入 Raft Group。 这些状态存储在两个 badger 实例中：raftdb 和 kvdb： raftdb 存储 Raft 日志和RaftLocalState​ kvdb 将键值数据存储在不同的列族，RegionLocalState​和RaftApplyState​中。你可以把 kvdb 看作是 Raft 论文中提到的状态机 格式如下，kv/raftstore/meta​中提供了一些辅助函数，并用writebatch.SetMeta()​将它们设置为 badger。 Key KeyFormat Value DB raft_log_key 0x01 0x02 region_id 0x01 log_idx Entry raft raft_state_key 0x01 0x02 region_id 0x02 RaftLocalState raft apply_state_key 0x01 0x02 region_id 0x03 RaftApplyState kv region_state_key 0x01 0x03 region_id 0x01 RegionLocalState kv 您可能想知道为什么 TinyKV 需要两个 badger 实例。实际上，它可以使用一个badger来存储 raft 日志和状态机数据。分成两个实例只是为了和 TiKV 的设计保持一致。 应在PeerStorage​中创建和更新这些元数据。创建 PeerStorage 时，请参见kv/raftstore/peer_storage.go​。它初始化此 Peer 的 RaftLocalState、RaftApplyState，或者在重启时从底层引擎获取上一个值。请注意，RAFT_INIT_LOG_TERM 和 RAFT_INIT_LOG_INDEX 的值均为 5（只要它大于 1），但不是 0。之所以不将其设置为 0，是为了与 Peer 在 conf 更改后被动创建的情况区分开来。你现在可能还不太明白，所以只要记住它，当你实现conf change时，细节将在project3b中描述。 这部分需要实现的代码只有一个函数：PeerStorage.SaveReadyState​，这个函数的作用是将raft.Ready​中的数据保存到 badger存储引擎（kvdb和raftdb，Project2C ​ApplySnapshot会涉及kvdb）中，包括Append​日志条目（即持久化 Raft 日志到 raftdb）和保存 Raft HardState。 要Append​日志条目，只需将在raft.Ready.Entries​中的所有日志条目保存到 raftdb 中，并删除之前追加的日志条目，这些条目永远不会被提交。另外，更新peer storage的RaftLocalState​并将其保存到 raftdb。 保存hard state​也非常简单，只需更新 peer storage的RaftLocalState.HardState​并将其保存到 raftdb 即可。 提示： 用WriteBatch​一次保存这些状态。 有关如何读取和写入这些状态，请参阅peer_storage.go​中的其他函数。 设置环境变量 LOG_LEVEL=debug，这可能有助于您进行调试，另请参阅所有可用的日志级别。 ","date":"2024-08-14","objectID":"/posts/tinykv_p2b_translate/:1:2","tags":["raft","kv","tinykv"],"title":"TinyKV Project 2B 文档翻译","uri":"/posts/tinykv_p2b_translate/"},{"categories":["kv"],"content":"Implement Raft ready process 在 project2 的 A 部分中，您构建了一个基于 tick 的 Raft 模块。现在，您需要编写外部进程来驱动它。大多数代码已经在kv/raftstore/peer_msg_handler.go​和kv/raftstore/peer.go​下实现。所以你需要学习代码并完成proposeRaftCommand和HandleRaftReady​的逻辑。以下是对该框架的一些解释。 Raft RawNode​已经使用PeerStorage​创建并存储在peer​中。在 raft worker 中，您可以看到它包含了peer​，采用peerMsgHandler​将其封装 。主要有两个功能：一个是HandleMsg​，另一个是HandleRaftReady​。 ​HandleMsg​处理从 raftCh 接收到的所有消息，包括MsgTypeTick​ （调用RawNode.Tick()​来驱动Raft），MsgTypeRaftCmd​包装来自客户端的请求，以及MsgTypeRaftMessage​ Raft 对等节点之间传输的消息。所有消息类型都在kv/raftstore/message/msg.go​中定义。您可以查看它的详细信息，其中一些将在以下部分中使用。 消息处理完毕后，Raft 节点应该会有一些状态更新。所以HandleRaftReady应该从 Raft 模块中获取 Ready 并执行相应的操作，例如持久化日志条目、应用已提交的条目并通过网络向其他 Peer 节点发送 Raft 消息。 在伪代码中，raftstore 使用 Raft，如下所示： for { select { case \u003c-s.Ticker: Node.Tick() default: if Node.HasReady() { rd := Node.Ready() saveToStorage(rd.State, rd.Entries, rd.Snapshot) send(rd.Messages) for _, entry := range rd.CommittedEntries { process(entry) } s.Node.Advance(rd) } } 上述伪代码的default​分支大致展示了HandleRaftReady​的处理流程。 因此，读取或写入的整个过程将是这样的： 客户端调用 RPC RawGet/RawPut/RawDelete/RawScan RPC 处理程序调用RaftStorage​相关方法 ​RaftStorage​向 raftstore 发送 Raft 命令请求，等待响应 ​RaftStore​将 Raft 命令请求作为 Raft 日志Propose​给 Raft 层 Raft 模块Append​日志，并使用PeerStorage​进行持久化 Raft 模块提交日志（该日志被大多数节点接受） Raft worker 在处理 Raft ready 时应用 Raft 命令，并通过 callback 返回响应 ​RaftStorage​接收来自回调的响应并返回到 RPC 处理程序 RPC 处理程序执行一些操作并将 RPC 响应返回给客户端 您应该运行make project2b​以通过所有测试。整个测试运行一个模拟集群，包括多个具有模拟网络的 TinyKV 实例。它执行一些读取和写入操作，并检查返回值是否符合预期。 需要注意的是，错误处理是通过测试的重要组成部分。您可能已经注意到proto/proto/errorpb.proto​其中定义了一些错误，并且该错误是 gRPC 响应的一个字段。此外，实现error​接口的相应错误在kv/raftstore/util/error.go​中定义，因此您可以将它们用作函数的返回值。 这些错误主要与 Region 有关。所以它也是RaftResponseHeader​ of RaftCmdResponse​的成员。在 Propose 请求或应用命令时，可能会出现一些错误。如果是这样，你应该返回带有错误的 raft 命令响应，然后错误将进一步传递给 gRPC 响应。您可以使用kv/raftstore/cmd_resp.go​提供的​BindRespError​将这些错误转换为在返回带有错误的响应时定义的错误。 在此阶段，您可以考虑以下这些错误，其他错误将在 project3 中处理： ErrNotLeader：在 follower 上 Propose raft 命令。因此，使用它来让客户端尝试将请求发送给其他 Peer。 ErrStaleCommand：可能是由于领导者更改，某些日志未提交并被新的领导者日志覆盖。但客户并不知道这一点，仍在等待响应。因此，您应该返回此命令以让客户端知道并重试该命令。 提示： ​PeerStorage​实现了 Raft 模块的Storage​接口，你应该使用提供的SaveReadyState()​方法来持久化 Raft 相关的状态。 使用engine_util​中的WriteBatch​以原子方式进行多次写入，例如，您需要确保在一个WriteBatch中应用已提交的条目并更新Applied索引（这两个写入不要分开来做）。 用Transport​向其他 Peer 节点发送 raft 消息，它在GlobalContext​中 如果服务器不是多数服务器的一部分，并且没有最新数据，则不应完成 get RPC。你可以直接将 get 操作放入 raft 日志中，或者实现 Raft 论文第 8 节中描述的只读操作的优化（建议先完成最基本的功能，再考虑实现各种优化）。 在应用日志条目时，不要忘记更新并保留ApplyState​。 你可以像 TiKV 一样异步应用已提交的 Raft 日志条目。这不是必需的，尽管提高性能是一个很大的挑战。 Propose时记录命令的回调，应用后返回回调。 对于 snap 命令响应，应将 badger Txn 显式设置为回调。 在 2A 之后采用随机测试，您可能需要多次运行某些测试以查找错误 ","date":"2024-08-14","objectID":"/posts/tinykv_p2b_translate/:1:3","tags":["raft","kv","tinykv"],"title":"TinyKV Project 2B 文档翻译","uri":"/posts/tinykv_p2b_translate/"},{"categories":["kv"],"content":"project2需要实现一个基于raft算法的高可用kv服务器。根据文档，有三部分需要完成，包括： A：实现基本的 Raft 算法 B：在 Raft 之上构建容错 KV 服务器 C：新增 raftlog GC 和 snapshot 支持 Project2A主要任务是实现Raft算法。 ","date":"2024-08-14","objectID":"/posts/tinykv_p2a/:0:0","tags":["raft","kv","tinykv"],"title":"TinyKV Project 2A","uri":"/posts/tinykv_p2a/"},{"categories":["kv"],"content":"准备工作 非常建议先阅读一遍raft论文再来做project2A，不然在很多概念的理解上都会很吃力。一个非常有帮助的链接： Raft Consensus Algorithm ‍ ","date":"2024-08-14","objectID":"/posts/tinykv_p2a/:1:0","tags":["raft","kv","tinykv"],"title":"TinyKV Project 2A","uri":"/posts/tinykv_p2a/"},{"categories":["kv"],"content":"aa选举和ab日志复制 这一部分主要实现Raft层进行共识（实现一致性）的逻辑，主要代码在raft/log.go​和raft/raft.go​当中。 ","date":"2024-08-14","objectID":"/posts/tinykv_p2a/:2:0","tags":["raft","kv","tinykv"],"title":"TinyKV Project 2A","uri":"/posts/tinykv_p2a/"},{"categories":["kv"],"content":"Raft和RaftLog类定义 如题目中所说，raft.Raft​提供了 Raft 算法的核心，包括消息处理、驱动逻辑时钟等。 Raft类定义解析如下： id：节点id Term：任期 Vote：当前任期投票给了谁 RaftLog：缓存日志条目 Prs：用于leader维护各节点日志复制的进度（Match代表已经匹配的Index，Next代表即将发送的下一个Index） State：节点状态 votes：记录投票信息 msgs：需要发送的消息，消息分本地消息（本节点处理）和普通消息 Lead：只想当前的leader heartbeatTimeout：心跳超时 electionTimeout：选举超时 heartbeatElapsed：记录距上次心跳超时经过的时间，用于判断是否心跳超时 electionElapsed：记录，用于判断是否选举超时 leadTransferee、PendingConfIndex：在3A领导权转移和配置变更中使用，此处忽略 结合Raft算法的要求，我增加了以下成员： randElectTimeout：随机选举超时，必需，减少选举失败。 voteCount：当前票数，不必需 denialCount：当前拒绝投票数，不必需，方便统计 ‍ ​raft.RaftLog​是一个辅助结构体，主要用于raft节点缓存日志条目，通过raft/storage.go​中定义的Storage​接口与上层应用进行交互，能够获取日志条目和快照等持久化数据。 RaftLog类定义解析如下： storage：存储了自从上次快照以来的所有stable的日志条目，stable指已经持久化了。storage中还提供了获取初始状态、FirstIndex、LastIndex等信息的接口，初始化需要用到。 applied、committed、stabled是非常重要的三个索引：分别代表已应用的、已提交的、已持久化的日志条目的最大索引，所有的日志条目的索引有如下关系： // snapshot/first.....applied....committed....stabled.....last // --------|------------------------------------------------| // log entries // entries：所有还未compact的日志条目，这里compact的含义暂时忽略。 pendingSnapshot：在2C中使用，暂时忽略。 参考代码中storage具有一个dummy entry的设计（题目也有说明：默认第一次启动raft时应具有条目 0），为RaftLog增加一个成员： dummyIndex：第一个日志的索引，用于方便计算下标位置 ","date":"2024-08-14","objectID":"/posts/tinykv_p2a/:2:1","tags":["raft","kv","tinykv"],"title":"TinyKV Project 2A","uri":"/posts/tinykv_p2a/"},{"categories":["kv"],"content":"原理和数据处理流程 上面提到的日志条目实际上是代表某个操作的条目（例如put、get​等），tinykv首先要对这些操作的条目利用raft算法进行共识，只有在集群内大多数节点“接受”了某个操作，这个操作才会被“应用”到存储引擎中进行实际的数据操作。Raft模块就是维护集群在这些操作条目上的一致性，进而上层应用可以执行相同的操作，实现数据一致性。 如题目所述，Raft模块的处理流程都是由上层应用通过调用RawNode​（RawNode是对Raft的封装）的各个接口来异步推进的。 上层应用负责通过调用RawNode.Tick()​来推进Raft节点的逻辑时钟，从而推进选举和leader发送心跳。 上层应用通过调用RawNode.Step()​来推动Raft节点处理消息，Raft节点只需要把消息推送到raft.Raft.msgs​中，在raft.Raft.Step()​中实现消息处理的逻辑。 上层应用通过调用RawNode.Propose()​向raft节点发送新的日志条目。 通过RawNode.Ready()​来获取raft节点的各项状态更新，据此进行消息转发、持久化、实际数据操作等。 通过RawNode.Advance()​来更新raft节点的内部状态，如应用的索引、稳定日志索引等。 。。。 在project2A中，我们只需要理解： 上层的 RawNode 会定时调用 tick()​，驱动 Raft，同时如果有消息，则通过 Step()​ 函数传递给 Raft。然后 Raft 进行一系列的处理。将需要发送给其他节点的消息存放在 r.msgs​ 中，RawNode 会在生成 Ready 的时候取走并发送给别的节点。整个流程是线性的，Tick()​ 和Step()​ 不会被同时触发，这里不存在多线程的情况。 因此在2aa和2ab中，主要需要完成： 针对tick()​的处理，即推进选举和leader发送心跳消息heartbeatMsg​等 针对Step()​的处理，即消息处理，按照raft算法对各种类型的消息执行相应的动作，这些消息包含本地的消息、其他节点的消息、上层发来的propose消息等。 辅助结构RaftLog​模块 ","date":"2024-08-14","objectID":"/posts/tinykv_p2a/:2:2","tags":["raft","kv","tinykv"],"title":"TinyKV Project 2A","uri":"/posts/tinykv_p2a/"},{"categories":["kv"],"content":"RaftLog模块 首先是newLog​，根据参数，需要从storage​中获取数据来进行初始化动作。由于storage​中保存的是所有已经持久化但是未被应用的日志条目，因此applied = firstIndex - 1​，stabled = lastIndex​。storage​中保存的hardState​保存了Term、Vote、Commit​三个信息，前两个用于Raft初始化，Commit​用来初始化RaftLog.committed​。获取entries​的方式也很简单，storage​提供了Entries​接口，因此可以通过调用Entries(firstIndex, lastIndex+1)​获取。 func newLog(storage Storage) *RaftLog { // Your Code Here (2A). firstIndex, _ := storage.FirstIndex() lastIndex, _ := storage.LastIndex() hardState, _, _ := storage.InitialState() entries := make([]pb.Entry, 0) if firstIndex \u003c= lastIndex { entries, _ = storage.Entries(firstIndex, lastIndex+1) if entries == nil { entries = make([]pb.Entry, 0) } } log := \u0026RaftLog{ storage: storage, committed: hardState.Commit, applied: firstIndex - 1, // 对applied进行修正 stabled: lastIndex, entries: entries, pendingSnapshot: nil, dummyIndex: firstIndex, } return log } 其余函数 ​unstableEntries​：所有未持久化的日志，字面意思处理即可 ​nextEnts​：所有已经提交但没有应用的日志(applied, committed]​ ​lastIndex​：最后一个条目的索引 ​Term​：获取指定索引的日志条目的term，注意要返回ErrUnavailable​错误，如果不在[firstIndex, lastIndex]，调用storage.Term​即可 这一部分代码编写要十分注意几个Index的细节处理和条件判断，否则容易出现越界等错误。 ","date":"2024-08-14","objectID":"/posts/tinykv_p2a/:2:3","tags":["raft","kv","tinykv"],"title":"TinyKV Project 2A","uri":"/posts/tinykv_p2a/"},{"categories":["kv"],"content":"Raft模块 newRaft // newRaft return a raft peer with the given config func newRaft(c *Config) *Raft { if err := c.validate(); err != nil { panic(err.Error()) } // Your Code Here (2A). return nil } 参数Config​提供了初始化所需的数据。值得注意的是Storage​，需要用它新建RaftLog​，获取Term​、Vote​等状态数据。另外，c.peers​有时会为空，这时也需要通过Storage.InitialState()​获取。节点初始身份为follower​。 tick() func (r *Raft) tick() { // Your Code Here (2A). switch r.State { case StateFollower: r.tickFollower() case StateCandidate: r.tickCandidate() case StateLeader: r.tickLeader() } } 逻辑时钟推进时，根据当前节点的身份状态选择下一步的处理。 对于Follower​和Candidate​，tick的作用是推进选举超时，如果一旦发现超时，首先清零选举超时，然后发起选举。这里发起选举是通过Step()​处理一个本地消息MessageType_MsgHup​触发的，后面会提到。 func (r *Raft) tickFollower() { r.electionElapsed += 1 if r.electionElapsed \u003e= r.randElectTimeout { r.electionElapsed = 0 r.Step(pb.Message{From: r.id, To: r.id, MsgType: pb.MessageType_MsgHup}) } } func (r *Raft) tickCandidate() { r.electionElapsed += 1 if r.electionElapsed \u003e= r.randElectTimeout { r.electionElapsed = 0 r.Step(pb.Message{From: r.id, To: r.id, MsgType: pb.MessageType_MsgHup}) } } 而对于Leader​，tick的作用是触发心跳超时。如果发现超时，同样清零以后再广播心跳给其他节点BroadCastHeartBeat​，这里广播心跳也是通过Step()​处理一个本地消息MessageType_HeartBeat​触发的。 func (r *Raft) tickLeader() { r.heartbeatElapsed += 1 if r.heartbeatElapsed \u003e= r.heartbeatTimeout { r.heartbeatElapsed = 0 r.Step(pb.Message{From: r.id, To: r.id, MsgType: pb.MessageType_MsgBeat}) } } 身份转换becomexxx 根据Raft算法，节点有三个身份状态：follower​、candidate​、leader​。一个顺利的选举流程的身份转换过程通常是：follower-\u003ecandidate-\u003eleader​，其余情况大多都是切换回follower​。 在身份转换时要对raft节点的一些数据进行更改。 对于becomefollower​，需要更新Vote​、State​、Term​、Lead​，清空votes​、leadTransferee​，重置两个超时时间，最后随机化选举超时。 // becomeFollower transform this peer's state to Follower func (r *Raft) becomeFollower(term uint64, lead uint64) { // Your Code Here (2A). if term \u003e r.Term { r.Vote = None } r.State = StateFollower r.Term = term r.Lead = lead r.votes = nil r.voteCount = 0 r.denialCount = 0 r.leadTransferee = None // reset r.heartbeatElapsed = 0 r.electionElapsed = 0 r.randElectTimeout = r.electionTimeout + rand.Intn(r.electionTimeout) } 对于becomeCandidate​，根据Raft算法，要自增Term​，然后先投票给自己（如果当前集群只有一个节点，直接becomeLeader​）。也要重置超时，随机化选举超时。 对于becomeLeader​，可以看到注释里面提示：// NOTE: Leader should propose a noop entry on its term​，即需要向其他节点发送一条空条目，这是为了更新Prs​（Match代表已经匹配的Index，Next代表即将发送的下一个Index），来获取follower​的进度。 ","date":"2024-08-14","objectID":"/posts/tinykv_p2a/:2:4","tags":["raft","kv","tinykv"],"title":"TinyKV Project 2A","uri":"/posts/tinykv_p2a/"},{"categories":["kv"],"content":"step()消息处理 func (r *Raft) Step(m pb.Message) error { // Your Code Here (2A). // node is not in cluster or has been removed switch r.State { case StateFollower: return r.stepFollower(m) case StateCandidate: return r.stepCandidate(m) case StateLeader: return r.stepLeader(m) } return nil } func (r *Raft) stepFollower(m pb.Message) error { switch m.MsgType { case pb.MessageType_MsgHup: r.handleHup() case pb.MessageType_MsgPropose: return ErrProposalDropped case pb.MessageType_MsgAppend: r.handleAppendEntries(m) case pb.MessageType_MsgRequestVote: r.handleRequestVote(m) case pb.MessageType_MsgSnapshot: r.handleSnapshot(m) case pb.MessageType_MsgHeartbeat: r.handleHeartbeat(m) case pb.MessageType_MsgTransferLeader: r.handleTransferLeader(m) case pb.MessageType_MsgTimeoutNow: r.handleHup() } return nil } ... ​Step​结合当前节点身份状态r.State​和消息类型MsgType​选择要执行的动作。这部分是实现Raft层的重点，首先要理解各个Msg的作用以及含义。 文档中提到，对于一个Raft节点来说，Msg分本地消息和普通消息，其中普通消息是要发给其他节点的，要放到r.msgs​中供上层异步地取用；本地消息是本地发起的，因此套个Step()​直接进行消息处理。之前在tick()​中发起的就是本地消息MsgHup​和MsgBeat​ Raft 通过一个Message​结构体包含了所有种类的msg，所以其字段对于某一种msg会存在富余的情况，所以要结合raft算法来确定某一种msg设置了哪些字段，以及各个字段的作用是什么。 type Message struct { MsgType MessageType `protobuf:\"varint,1,opt,name=msg_type,json=msgType,proto3,enum=eraftpb.MessageType\" json:\"msg_type,omitempty\"` To uint64 `protobuf:\"varint,2,opt,name=to,proto3\" json:\"to,omitempty\"` From uint64 `protobuf:\"varint,3,opt,name=from,proto3\" json:\"from,omitempty\"` Term uint64 `protobuf:\"varint,4,opt,name=term,proto3\" json:\"term,omitempty\"` LogTerm uint64 `protobuf:\"varint,5,opt,name=log_term,json=logTerm,proto3\" json:\"log_term,omitempty\"` Index uint64 `protobuf:\"varint,6,opt,name=index,proto3\" json:\"index,omitempty\"` Entries []*Entry `protobuf:\"bytes,7,rep,name=entries\" json:\"entries,omitempty\"` Commit uint64 `protobuf:\"varint,8,opt,name=commit,proto3\" json:\"commit,omitempty\"` Snapshot *Snapshot `protobuf:\"bytes,9,opt,name=snapshot\" json:\"snapshot,omitempty\"` Reject bool `protobuf:\"varint,10,opt,name=reject,proto3\" json:\"reject,omitempty\"` XXX_NoUnkeyedLiteral struct{} `json:\"-\"` XXX_unrecognized []byte `json:\"-\"` XXX_sizecache int32 `json:\"-\"` } tinykv中Raft的所有消息类型如下： var MessageType_name = map[int32]string{ 0: \"MsgHup\", 1: \"MsgBeat\", 2: \"MsgPropose\", 3: \"MsgAppend\", 4: \"MsgAppendResponse\", 5: \"MsgRequestVote\", 6: \"MsgRequestVoteResponse\", 7: \"MsgSnapshot\", 8: \"MsgHeartbeat\", 9: \"MsgHeartbeatResponse\", 11: \"MsgTransferLeader\", 12: \"MsgTimeoutNow\", } 以下列表汇总了所有消息类型的描述、有效字段，以及哪些身份状态需要处理该类型的消息： Name 描述 有效字段 follower candidate leader MsgHup 本地消息，发起选举 MsgType ✔ ✔ MsgBeat 本地消息，Leader广播心跳，用于检查follower进度和避免新的选举 MsgType ✔ MsgPropose 本地消息，来自上层应用的Propose请求。只有Leader实际处理，其他都返回ErrProposalDropped​ MsgType Entries To ✔ ✔ ✔ MsgAppend 用于Leader向其他节点同步数据，即日志复制。 MsgType Index Term LogTerm Entries Commit To From ✔ ✔ ✔ MsgAppendResponse 用于回复收到的 MessageType_MsgAppend​ 和 MessageType_MsgSnapshot​。Leader据此更新Match和Next等记录 MsgType Index Term Reject To From ✔ MsgRequestVote candidate发送的投票请求 MsgType Index Term LogTerm To From ✔ ✔ ✔ MsgRequestVoteResponse 投票响应 MsgType Term Reject To From ✔ MsgSnapshot project2c，当 Leader 发现目标节点所需的日志已经被 compact 的时候，则发送 Snapshot。 MsgType Term Snapshot To From ✔ ✔ ✔ MsgHeartbeat 来自Leader的心跳消息 MsgType Term To From ✔ MsgHeartbeatResponse 心跳响应，包含当前follower的Commit索引，Leader据此判断它的进度 MsgType Term Commit To From Reject ✔ MsgTransferLeader 本地消息，领导权转移，Project3A MsgType From ✔ ✔ ✔ MsgTimeoutNow 目标节点收到该消息，即刻自增 term 发起选举 MsgType From To ✔ ✔ 下面分为领导人选举和日志复制两个部分分别介绍这些消息的处理逻辑。 ","date":"2024-08-14","objectID":"/posts/tinykv_p2a/:2:5","tags":["raft","kv","tinykv"],"title":"TinyKV Project 2A","uri":"/posts/tinykv_p2a/"},{"categories":["kv"],"content":"领导人选举 handleHup 首先，follower​和candidate​选举超时都会触发MsgHup​，用于直接发起选举。handleHup的处理逻辑如下： 判断能否进行选举： 判断自己是否在当前的 Prs 里面。因为自己可能已经被移除（Project3 涉及）。 判断是否有 pendingSnapshot。如果正在 applying snapshot，则不要发起选举，因为一定是其他的 leader 发给你 snapshot，如果你发起选举，你的 term+1 必定大于现有的 leader，这会造成集群的不稳定。 ​becomeCandidate​ 向其他所有节点发送投票请求MsgRequestVote​ handleRequestVote ​follower​、candidate​、leader​都有可能收到投票请求。在Raft论文中，为了避免已经提交的日志被覆盖保障一致性，除了最基本的任期条件，还增加了一个选举限制条件，即日志的新旧条件，具体可以再看一下论文中的描述。handleRequestVote的处理逻辑如下： 如果任期比自己大，becomeFollower**（此时不一定投票给他）** 如果任期没自己大，拒绝 如果当前任期已经投票了，且不是他，拒绝 如果日志比自己旧（比较最后一个条目的Term和Index，先比较Term，相等的情况下再比较Index），拒绝，否则becomefollower​并且投票给他 handleRequestVoteResponse candidate收到投票响应后： 如果任期比自己大，becomeFollower，此时直接返回 根据Reject​更新r.votes、r.voteCount、r.denialCount​ 如果r.voteCount​超过半数，直接becomeLeader​，选举成功；r.denialCount​同理，如果超过半数，直接becomeFollower​，选举失败。 注意，不能直接r.voteCount++​和r.denialCount++​记录票数，而要通过遍历r.votes​，因为后面3B的测试中会出现重复的响应，会出现一直选举不出来的情况。 ","date":"2024-08-14","objectID":"/posts/tinykv_p2a/:2:6","tags":["raft","kv","tinykv"],"title":"TinyKV Project 2A","uri":"/posts/tinykv_p2a/"},{"categories":["kv"],"content":"日志复制 节点成为Leader后，会不断向其他节点发送心跳来维持自己的地位，并通过心跳响应来了解其他节点的日志提交的进度，进而决定是否需要同步日志给他。 handleBeat 直接广播heartbeat func (r *Raft) handleBroadcastHeartBeat() { for peer := range r.Prs { if peer == r.id { continue } r.sendHeartbeat(peer) } } handleHeartbeat 只要根据Term决定是否Reject​，返回的响应中要附带Commit​，方便Leader收到后进行日志同步 handleHeartbeatResponse leader在收到心跳响应后 判断返回的Term是否大于当前Term，如果是则becomeFollower​直接返回 根据follower的Commit​信息和leader中记录的Match​，判断leader是否需要同步日志给他 func (r *Raft) handleHeartBeatResponse(m pb.Message) { if m.Term \u003e r.Term { r.becomeFollower(m.Term, None) return } // if it doesn't have update-to-date log if m.Commit \u003c r.RaftLog.committed || r.Prs[m.From].Match \u003c r.RaftLog.LastIndex() { r.sendAppend(m.From) } } 节点成为Leader后，上层应用会发送日志条目给他，即MsgPropose​，他需要将这些条目加入到自己的RaftLog​当中，然后同步给其他节点。 handlePropose 只有leader处理，如果不是leader，直接返回ErrPorosalDropped​。 判断r.leadTransferee​，如果正在进行领导权转移，不能接受propose，直接返回ErrPorosalDropped​（Project3A） 将propose消息中的Entries​加入到r.RaftLog​中 广播Append消息给其他节点，即日志复制 尝试更新Commit，tryUpdateCommit​ sendAppend的实现 ​sendAppend(to)​即发送MsgAppend​给节点to，有许多值得注意的地方，各字段赋值如下： msg := pb.Message{ MsgType: pb.MessageType_MsgAppend, To: to, From: r.id, Term: r.Term, LogTerm: preLogTerm, Index: preLogIndex, Entries: sendEntreis, Commit: r.RaftLog.committed, } 其中LogTerm、Index​分别代表论文中的preLogTerm、preLogIndex​（preLogTerm = Term(Next-1)、preLogIndex = Next-1​），是进行日志匹配的关键项，根据论文中的定理，如果在next-1的位置处匹配（即Term相同），代表之前的所有日志都匹配，就可以放心地接收Next​往后的所有条目了。 ​Commit​的作用如上所述是让其他节点也能更新自己的committed​。 在project2C中，当Leader找不到preLogIndex​对应的日志条目时，代表它已经被Compact掉了，也代表节点to落后自己太多了，此时不再发送append消息，而是发送一个Snapshot给它。 tryUpdateCommit的实现 Leader需要根据集群所有节点的进度更新自己的Commit同时推进其他节点的Commit，日志的提交需要满足“大多数”的条件，只有当日志被提交以后才能被上层实际应用。 根据Raft算法， 我们取所有Match​的中位数，如果有更新，则广播给其他节点。 func (r *Raft) tryUpdateCommit() { if len(r.Prs) == 1 { r.RaftLog.committed = r.RaftLog.LastIndex() return } // get all match var matches []uint64 for peer := range r.Prs { if peer == r.id { continue } matches = append(matches, r.Prs[peer].Match) } sort.Sort(UInt64Slice(matches)) // get mid midIndex := matches[len(matches)/2] midTerm, _ := r.RaftLog.Term(midIndex) // if mid \u003e commit, update commit if midIndex \u003e r.RaftLog.committed \u0026\u0026 midTerm == r.Term { r.RaftLog.committed = midIndex // send append to all peers r.bcastAppend() } } handleAppendEntries 这部分是实现日志复制的核心，有很多需要注意的细节。 我的实现步骤如下： 如果比当前任期小，直接返回拒绝 becomeFollower() 如果m.Index​比当前节点的lastIndex要大，说明Next-1 \u003e lastIndex，显然Next要减小一点，于是直接拒绝，返回的响应中 Index = lastIndex + 1，让 Next减小到lastIndex+1（日志复制部分要注意理解r.Prs​中Match​和Next​字段的含义） 如果r.RaftLog.Term(m.Index)​与m.LogTerm​不相同，说明在m.Index​处发生冲突，接下来论文中的原始做法是返回Index = m.Index​，从而让Next = m.Index​，即Next–。优化做法是返回在r.RaftLog.Term(m.Index)​这一任期下该节点的第一条日志的索引conflictIndex​，即让Next = conflictIndex​，这样能加快日志匹配的速度。 如果上述两种情况都没有发生，说明该节点在m.Index​及之前的日志与Leader都是匹配，因此可以“接受”这个Append请求。但是有一种特殊情况需要考虑，就是被append的所有日志条目m.Entries​已经在该节点中存在了，这时可能后面还有一些日志条目，因此不能无脑截断然后append，如果没有冲突应当要保留这些多出来的日志。我的做法是首先判断该节点在m.Index​往后的日志长度是否大于len(m.Entries)​，如果不大于则可以无脑截断然后append，如果大于，则要逐个检查是否与m.Entries​中的条目相匹配，满足则不做任何修改，只要一个条目有冲突都直接截断然后append。 accept之后，要根据m.Commit​对r.RaftLog.committed​进行更新，我的做法是r.RaftLog.committed = min(m.Commit, m.Index+uint64(len(m.Entries)))​ 返回accpet的响应，注意对Index​的设置 handleAppendResponse 根据Raft算法，当leader收到Reject Append的响应后，要更新Next​（回退到某个可能匹配的位置），然后重新sendAppend​。 在MsgAppendResponse m​中： 若Reject == true​，m.Index​代表Next需要回退到的位置； 若Reject == false​，m.Index​代表最新匹配的位置，即新的Match​值。 因此，具体处理流程如下： 如果m.Term​大于当前任期，Leader直接becomeFollower​并返回 如果m.Reject == true​，令r.Prs[m.From].Next = m.Index​，重新调用sendAppend​ 如果m.Reject == false \u0026\u0026 m.Index \u003e r.Prs[m.From].Match​，说明该follower接收了新的条目，更新对应的Match​和Next​，调用tryUpdateCommit​尝试提交日志条目。 ‍ 以上就实现了基本的领导人选举和日志复制，后面的项目还会增加Snapshot​、TransferLeader​等操作。 ","date":"2024-08-14","objectID":"/posts/tinykv_p2a/:2:7","tags":["raft","kv","tinykv"],"title":"TinyKV Project 2A","uri":"/posts/tinykv_p2a/"},{"categories":["kv"],"content":"一些注意点 选举超时随机化 根据raft论文，每次触发选举超时都要对这个超时时间进行随机化，能够有效减少选举冲突的情况发生。我的做法是在becomexxx​函数，即节点状态变更的函数中都加入以下代码来进行时间重置和选举超时随机化。 r.heartbeatElapsed = 0 r.electionElapsed = 0 r.randElectTimeout = r.electionTimeout + rand.Intn(r.electionTimeout) 那么如何进行随机化？从哪里知道使用r.randElectTimeout = r.electionTimeout + rand.Intn(r.electionTimeout)​，即控制在[et,2et]​之间呢？ 这是从2AA的测试点testNonleaderElectionTimeoutRandomized​中得知的。 func testNonleaderElectionTimeoutRandomized(t *testing.T, state StateType) { et := 10 r := newTestRaft(1, []uint64{1, 2, 3}, et, 1, NewMemoryStorage()) timeouts := make(map[int]bool) for round := 0; round \u003c 50*et; round++ { switch state { case StateFollower: r.becomeFollower(r.Term+1, 2) case StateCandidate: r.becomeCandidate() } time := 0 for len(r.readMessages()) == 0 { r.tick() time++ } timeouts[time] = true } for d := et + 1; d \u003c 2*et; d++ { if !timeouts[d] { t.Errorf(\"timeout in %d ticks should happen\", d) } } } 上述测试代码做的事情就是进行足够多次的模拟，验证随机化的选举超时有没有覆盖[et+1,2et)​。由此可知随机化的具体标准。 发起选举 在2aa测试点中经常出现集群只有一个节点的情况，因此要对这种情况做特殊处理，使Candidate直接成为leader。 “给其他节点发送投票请求”时要注意附带Term、LogTerm、Index​三个字段，其中LogTerm​是最大日志的任期，Index​是最大日志的索引，其他节点可以通过它们来判断投票发起者的日志是否比自己的新，日志比自己旧那么就投反对票。这样就实现了论文中的“选举限制”，保障了安全性。 becomeLeader ​becomeLeader​存在一些需要特别注意的特殊处理： 初始化r.Prs​，即每个节点的Progress{Match, Next}​。之前提到过是用于leader维护各节点日志复制的进度（Match代表已经匹配的Index，Next代表即将发送的下一个Index）。Match初始化为0即可，Next则要初始化为r.RaftLog.LastIndex() + 1​，代表后续给其他节点同步日志都先从Next​的位置开始，发生冲突再让对应节点的Next​回退。 for _, peer := range r.Prs { peer.Match = 0 peer.Next = r.RaftLog.LastIndex() + 1 } 根据文档和代码注释中的提示“Leader should propose a noop entry on its term​”，也就是becomeLeader​时append一个在Leader任期下的一个空条目，这样做的目的是让所有节点进行一次初始的日志同步操作 broadCastAppend 我的raft实现中主要有两处要broadCastAppend​，一处是上面提到的becomeLeader​，一处是handlePropose​（接收来自上层的日志条目）。 在每次broadCastAppend​之后都要立即tryUpdateCommit​而不等待AppendResponse​，这是考虑了单节点这一特殊情况，否则单节点将永远无法更新自己的committed​。 handleAppendEntries ​handleAppendEntries​的实现一定要仔细，注意覆盖所有的条件，注意测试点中提出的要求。 例如，在日志匹配的前提下，对于目标节点超出preLogIndex​的日志条目，不能够无脑截断掉。因为这些条目很可能与即将append的条目也是完全匹配的，那么再往后多出来的部分就不能够删除掉而要保留下来，因为本质上没做任何修改。 例如，在日志不匹配的前提下，同样不能对目标节点的日志做任何修改。我一开始为了方便处理，对于目标节点超出preLogIndex​的日志条目都直接截断掉，结果很多测试出现问题。 消息处理的幂等性 文档中有这么一句描述： Raft RPC是幂等的,收到相同的RPC不会由任何危害.例如，如果追随者收到一个AppendEntries请求，其中包含其日志中已经存在的日志条目，则会忽略新请求中的这些条目。 所以我们在实现各种handle函数时，要经常考虑有重复请求的情况，例如重复的投票，重复的append消息等等。 ‍ ","date":"2024-08-14","objectID":"/posts/tinykv_p2a/:2:8","tags":["raft","kv","tinykv"],"title":"TinyKV Project 2A","uri":"/posts/tinykv_p2a/"},{"categories":["kv"],"content":"测试问题 2aa和2ab的测试大多数问题都可以靠错误日志发现原因。发生错误时，可以看看测试点的代码，里面可能有一些特殊的要求，对应的修改即可。 我在testCandidateResetTerm​遇到过测试卡死，通过输出发现问题原因是处理Msg_Append​时逻辑出错，导致日志冲突而next​没有发生变化，所以死循环了，一直在sendAppend。 这一部分问题比较多的就是handleAppendEntries​，我当时也是改了很多次才形成了最终的这个版本。 ‍ ","date":"2024-08-14","objectID":"/posts/tinykv_p2a/:2:9","tags":["raft","kv","tinykv"],"title":"TinyKV Project 2A","uri":"/posts/tinykv_p2a/"},{"categories":["kv"],"content":"project2ac RawNode Ready 这一部分代码主要在raft/rawnode.go​，在理解了具体要求之后实现比较简单。 RawNode​是对Raft​的一个封装，提供了Raft模块与上层交互的相关函数。 ​Ready()​：返回一个Ready类型的数据，其中包含了当前Raft节点的状态，上层应用根据Ready中的数据进行持久化、日志条目的应用等操作。 ​HasReady()​：判断是否有状态更新。 ​Advance(rd Ready)​：由上层应用处理完某个Ready调用，用来更新Raft节点的一些状态。 首先我们需要考虑RawNode​的类定义中除了Raft​还需要什么成员。 ​Ready​类定义如下： ​*SoftState​：包含Lead​和RaftState​ ​pb.HardState​：包含Term​、Vote​、Commit​ ​Entrise​：存放还没持久化的所有条目 ​Snapshot​：存放需要应用的snapshot ​CommittedEntries​：存放已经提交但是还没有被应用的日志条目 ​Messages​：存放raft节点中需要转发的一些消息（RequestVote、AppendEntries等等） 由上面的定义可以得知我们的HasReady()​需要对这6种数据进行检查，看是否有更新，后四个都只需要判断有没有即可，前两个则需要观察是否有变化，所以RawNode​的定义中要包含prevSoftState​和prevHardState​。 type RawNode struct { Raft *Raft // Your Data Here (2A). prevSoftState *SoftState prevHardState pb.HardState } ​NewRaftNode​也很简单。 func NewRawNode(config *Config) (*RawNode, error) { // Your Code Here (2A). raft := newRaft(config) rn := \u0026RawNode{ Raft: raft, prevSoftState: \u0026SoftState{ Lead: raft.Lead, RaftState: raft.State, }, prevHardState: pb.HardState{ Term: raft.Term, Vote: raft.Vote, Commit: raft.RaftLog.committed, }, } return rn, nil } ​HasReady()​检查是否有状态更新；Ready()​提取这些更新的状态数据，需要注意的是获取Raft.msgs​之后要clear掉，并且骨架代码中已经提供了很多辅助函数，例如isHardStateEqual​、isEmptySnap​等。 ​Advance()​需要做的是更新两个索引值stabled​和applied​，同时update prevSoftState、prevHardState​即可。从这个函数可以看出上层对于Ready中的数据处理主要为“持久化”和“应用”两个方面。 ‍ 最后附上通过的截图（太长了只截取了一部分） ​ pass ​ ","date":"2024-08-14","objectID":"/posts/tinykv_p2a/:3:0","tags":["raft","kv","tinykv"],"title":"TinyKV Project 2A","uri":"/posts/tinykv_p2a/"},{"categories":["kv"],"content":"project1是一个热身项目，内容非常简单。项目文档中提到需要实现两部分的内容：（1）独立存储引擎；（2）原始kv服务处理程序。下面分别介绍一下基本的思路。","date":"2024-08-14","objectID":"/posts/tinykv_p1/","tags":["raft","kv","tinykv"],"title":"TinyKV Project 1","uri":"/posts/tinykv_p1/"},{"categories":["kv"],"content":"project1是一个热身项目，内容非常简单。项目文档中提到需要实现两部分的内容：（1）独立存储引擎；（2）原始kv服务处理程序。下面分别介绍一下基本的思路。 ","date":"2024-08-14","objectID":"/posts/tinykv_p1/:0:0","tags":["raft","kv","tinykv"],"title":"TinyKV Project 1","uri":"/posts/tinykv_p1/"},{"categories":["kv"],"content":"独立存储引擎 文档中提到该任务是对badgerDB的读写API的封装以支持列族（Column Family, CF）。通过运行grep -rIi \"Your Code Here (1).\"​发现需要编写的代码主要在kv/storage/standalone_storage/standalone_storage.go​，需要实现： 定义StandAloneStorage​的数据结构 定义NewStandAloneStorage​函数，根据参数Config​创建一个StandAloneStorage​对象 填充Start, Stop, Reader, Write​四个函数 ","date":"2024-08-14","objectID":"/posts/tinykv_p1/:1:0","tags":["raft","kv","tinykv"],"title":"TinyKV Project 1","uri":"/posts/tinykv_p1/"},{"categories":["kv"],"content":"​StandAloneStorage​​​的定义和新建 根据项目文档，engine_util包 （kv/util/engine_util​）中提供了所有的读写操作，即需要封装的API。但是我们并不知道它们在哪里，kv/util/engine_util/doc.go​中给出了一些信息： * engines: a data structure for keeping engines required by unistore. * write_batch: code to batch writes into a single, atomic 'transaction'. * cf_iterator: code to iterate over a whole column family in badger. 从命名和描述可以看出engines是存储引擎、write_batch是将多个写入整合到一个batch中，cf_iterator则是在badger中迭代列族。 再来看具体的文件。 ​kv/util/engine_util/engines.go​中定义了Engines​类，包含了两个badger.DB​的指针，还提供了NewEngines​、WriteKV​、WriteRaft​、Close​、Destroy​、CreateDB​这些函数。很显然StandAloneStorage​是对Engines​的封装，需要包含一个engine_util.Engines​成员，NewStandAloneStorage​需要进行CreateDB​和NewEngines​两个步骤。Start​不需要修改，Stop​需要调用en.Close()​; type Engines struct { // Data, including data which is committed (i.e., committed across other nodes) and un-committed (i.e., only present // locally). Kv *badger.DB KvPath string // Metadata used by Raft. Raft *badger.DB RaftPath string } type StandAloneStorage struct { // Your Data Here (1). en *engine_util.Engines } ","date":"2024-08-14","objectID":"/posts/tinykv_p1/:1:1","tags":["raft","kv","tinykv"],"title":"TinyKV Project 1","uri":"/posts/tinykv_p1/"},{"categories":["kv"],"content":"Write 关于Write的实现，文档告诉我们参数ctx *kvrpcpb.Context​暂时不用管，只剩下一个参数batch []storage.Modify​，可知是接口Modify​类型切片，通过观察类定义和相关函数发现，Modify​中可以是类型为Put​或Delete​类型的Data​，使用.Key().Cf().Value()​取对应的字段。 type Modify struct { Data interface{} } type Put struct { Key []byte Value []byte Cf string } type Delete struct { Key []byte Cf string } func (m *Modify) Key() []byte { switch m.Data.(type) { case Put: return m.Data.(Put).Key case Delete: return m.Data.(Delete).Key } return nil } func (m *Modify) Value() []byte { if putData, ok := m.Data.(Put); ok { return putData.Value } return nil } func (m *Modify) Cf() string { switch m.Data.(type) { case Put: return m.Data.(Put).Cf case Delete: return m.Data.(Delete).Cf } return \"\" } 因此我们在Write​中需要做的应该是：声明一个空的WriteBatch wb，然后遍历参数batch​，提取每一个key,val,cf​，使用辅助函数添加到wb中；遍历结束后调用en.WriteKV​实现写入。 通过阅读kv/util/engine_util/write_batch.go​可知要用到的辅助函数是类WriteBatch​的SetCF​和DeleteCF​接口。 ","date":"2024-08-14","objectID":"/posts/tinykv_p1/:1:2","tags":["raft","kv","tinykv"],"title":"TinyKV Project 1","uri":"/posts/tinykv_p1/"},{"categories":["kv"],"content":"Reader ​Reader​方法返回一个StorageReader​接口，要求实现GetCF()、IterCF()、Close()​三个方法 type StorageReader interface { // When the key doesn't exist, return nil for the value GetCF(cf string, key []byte) ([]byte, error) IterCF(cf string) engine_util.DBIterator Close() } 文档中说应该使用badger.Txn​来实现Reader​函数，但是badger.Txn​并没有GetCF()、IterCF()、Close()​这三个方法，因此不能满足StorageReader​接口的要求（go语言接口interface的限制），我们要对其进行封装。 我这里定义了一个StandAloneStorageReader​类，包含了一个badger.Txn​成员，分别使用engine_util.GetCFFromTxn​、engine_util.NewCFIterator​、txn.Discard​实现了GetCF()、IterCF()、Close()​。在Reader​中只需要调用en.Kv.NewTransaction​创建一个txn​，然后返回一个StandAloneStorageReader​即可。 func (s *StandAloneStorage) Reader(ctx *kvrpcpb.Context) (storage.StorageReader, error) { // Your Code Here (1). txn := s.en.Kv.NewTransaction(false) return \u0026StandAloneStorageReader{ txn: txn, }, nil } type StandAloneStorageReader struct { txn *badger.Txn } ","date":"2024-08-14","objectID":"/posts/tinykv_p1/:1:3","tags":["raft","kv","tinykv"],"title":"TinyKV Project 1","uri":"/posts/tinykv_p1/"},{"categories":["kv"],"content":"实现服务处理程序 需要实现的代码在kv/server/raw_api.go​当中，需要实现RawGet​、RawPut​、RawDelete​、RawScan​。前三个都很简单，没有需要自己寻找或设计的数据结构，部分接口需要查找，不过都是可以根据参数和成员得到的。 ​RawScan​函数主要是要弄清它的功能：从startkey​开始，取至多limit​个kvPairs​。 这样就知道需要用Reader​的迭代器，首先Seek​到StartKey​的位置，然后取至多limit​个kvPairs​。 ​iter​的使用方式参考：iter-\u003eSeek(StartKey)​ –\u003eiter-\u003eValid()​ –\u003eiter.Next()​ ","date":"2024-08-14","objectID":"/posts/tinykv_p1/:2:0","tags":["raft","kv","tinykv"],"title":"TinyKV Project 1","uri":"/posts/tinykv_p1/"},{"categories":["kv"],"content":"测试问题 出现一个问题 ​ problem ​ 查看测试代码，发现当KeyNotFound时要求err返回nil（后面许多关卡很多都需要面向测试点编程） ​ test code ​ 解决后成功通过！完成 project 1！ ​ success ​ ","date":"2024-08-14","objectID":"/posts/tinykv_p1/:3:0","tags":["raft","kv","tinykv"],"title":"TinyKV Project 1","uri":"/posts/tinykv_p1/"},{"categories":["kv"],"content":"前段时间磕磕绊绊学习了Tinykv这个项目，通过所有测试就没再理了，现在觉得不能把工夫浪费了，因此写个Blog记录一下。","date":"2024-08-13","objectID":"/posts/tinykv_start/","tags":["raft","kv","tinykv"],"title":"TinyKV 启~动!","uri":"/posts/tinykv_start/"},{"categories":["kv"],"content":"前段时间磕磕绊绊学习了Tinykv这个项目，通过所有测试就没再理了，现在觉得不能把工夫浪费了，因此写个Blog记录一下。 ","date":"2024-08-13","objectID":"/posts/tinykv_start/:0:0","tags":["raft","kv","tinykv"],"title":"TinyKV 启~动!","uri":"/posts/tinykv_start/"},{"categories":["kv"],"content":"什么是Tinykv？ Tinykv是PingCAP推出的一个学习分布式系统的课程，这个公司在存储领域很厉害，比较著名的产品是TiDB。 Tinykv这个课程的具体内容是使用Raft共识算法构建一个具有分布式事务支持的键值存储系统，它提供了一些骨架代码，我们只需要填充其中的一些核心逻辑，例如Raft层实现共识，RaftStore实现消息处理等等。 整个课程有4个Project，分别为StandaloneKv、raftKv、MuiltiRaftKv、Transactions，整体难度上：porject3 \u003e project2 » project4 \u003e project1。其中project2B和project3B是两个分水岭，一般都会Bug满天飞，需要打印详细的日志仔细地跟踪，并且有的Bug复现概率较低，要反复跑好几次，因此要坚持下来需要一定的毅力。 如果集中精力做的话，我了解到的几个同学都是暑假一个月不到的时间就完成了，整体代码量也不多，就是测试修Bug比较折磨。我当时还有学业上的别的事情，并没有太多的精力集中搞这个，从开始到完成花了有三个月，期间也是卡在2B和3B摆烂了很久。不过所幸是坚持到了最后。 如果时间凑巧的话，还可以报名PingCAP定期举办的tinykv学习营（Talent Plan | TiDB 社区），能起到一定的督促作用，还能与其他同学交流经验心得，可以少踩很多坑。 整个课程做下来能够学到的东西还是挺多的，例如raft共识算法、分布式系统架构、多版本并发控制等等，对于想走存储、分布式系统方向的同学还是很有用处的。 ‍ ","date":"2024-08-13","objectID":"/posts/tinykv_start/:1:0","tags":["raft","kv","tinykv"],"title":"TinyKV 启~动!","uri":"/posts/tinykv_start/"},{"categories":["kv"],"content":"资源链接 github仓库 talent-plan/tinykv：基于 TiKV 模型构建分布式键值服务的课程 (github.com) 讲解视频 Talent Plan 2021 KV 学习营分享课 (pingcap.com) 很有帮助的文章，Tinykv白皮书 如何快速通关 Talent Plan TinyKV？ - 知乎 (zhihu.com) ‍ ","date":"2024-08-13","objectID":"/posts/tinykv_start/:2:0","tags":["raft","kv","tinykv"],"title":"TinyKV 启~动!","uri":"/posts/tinykv_start/"},{"categories":["kv"],"content":"环境搭建 tinykv对硬件稍微有点要求，否则跑得太慢又Bug满天飞心态很容易炸。我是使用14核处理器，32GB内存，SSD固态的笔记本，搭配wsl2的linux环境，运行起来非常流畅。固态据说是刚需，其他感觉与这差不多的配置或者低一点也没关系。非常推荐使用wsl，可以直接连接vscode非常快，环境配起来也嘎嘎轻松；用虚拟机也可以，不过我感觉有点慢。有配置好服务器的话更好，可以一直挂着跑。 附上我当时配置wsl2参考的博客 如何在Windows11上安装WSL2的Ubuntu22.04（包括换源）_wsl2换源_syqkali的博客-CSDN博客 由于项目用go实现，所以也要配置golang的运行环境，项目里还用到了make，我这里直接用apt安装即可 sudo apt update apt search golang-go sudo apt install golang-go go version sudo apt install make 接着从github仓库下载源码，得到的就是只有骨架代码的版本了。 ​git clone https://github.com/talent-plan/tinykv.git​ 建议在github上创建一个私有仓库来维护代码，这样就知道在哪些地方做了修改，并且可以很方便地恢复了。 ‍ ","date":"2024-08-13","objectID":"/posts/tinykv_start/:3:0","tags":["raft","kv","tinykv"],"title":"TinyKV 启~动!","uri":"/posts/tinykv_start/"},{"categories":["kv"],"content":"快速掌握Go语言基本语法 如果对Go语言的基本语法不熟悉，可以通过以下链接的教程快速掌握，对于Tinykv已经足够了。 Go 语言之旅 (go-zh.org) ‍ ","date":"2024-08-13","objectID":"/posts/tinykv_start/:4:0","tags":["raft","kv","tinykv"],"title":"TinyKV 启~动!","uri":"/posts/tinykv_start/"},{"categories":["kv"],"content":"Tinykv快速上手 tinykv根目录下的doc目录存放了四个project的说明文档。文档是全英文的（如果英文不是很好的话，可以先找中文翻译了解一下大体的框架，细节上还是要看英文，有些翻译得不准会造成误解），文档介绍了每个项目的要实现的具体功能和一些细节上的要求，不过整体上文档还是不够全面，很多功能需要仔细阅读它的骨架代码才能实现。 ","date":"2024-08-13","objectID":"/posts/tinykv_start/:5:0","tags":["raft","kv","tinykv"],"title":"TinyKV 启~动!","uri":"/posts/tinykv_start/"},{"categories":["kv"],"content":"代码编写 每个project所要编写的代码的位置都给出了注释提示，可以通过grep命令找到需要编写代码的地方。 以project1为例，可以使用命令grep -rIi \"Your Code Here(1)\"​，来查找project1的项目文件以及需要编写代码的地方，主要的函数名和参数返回值都已经定义好了，只要填充其中的逻辑即可。注意project1之后的234都分成了A、B、C三部分，这时查找则是例如grep -rIi \"Your Code Here(2A)\"​这样。 ​​ grep -rIi ​ 当然虽然给出了主要的函数名和参数返回值，在project2和3还是要增加很多自定义的函数。 ","date":"2024-08-13","objectID":"/posts/tinykv_start/:5:1","tags":["raft","kv","tinykv"],"title":"TinyKV 启~动!","uri":"/posts/tinykv_start/"},{"categories":["kv"],"content":"测试方式 make projectxxx project2b、2c、3b、3c测试一遍是不够的，需要跑很多次才能复现一些Bug，可以使用shell脚本批量跑。 下面是我使用的测试脚本，只需要改运行次数times和project，removelog是在PASS的情况下删除日志。 #!/bin/bash # settings to change times=20 project=\"2b\" removelog=1 # don't change if [ ! -d \"./test_output\" ]; then mkdir \"./test_output\" fi logdir=\"./test_output/${project}\" if [ ! -d $logdir ]; then mkdir $logdir fi lastdir=\"${logdir}/`date +%Y%m%d%H%M%S`\" if [ ! -d $lastdir ]; then mkdir $lastdir fi summary=\"${lastdir}/summary.log\" echo \"times pass fail panic runtime\" \u003e\u003e $summary totalpass=0 totalfail=0 totalpanic=0 totalruntime=0 for i in $(seq 1 $times) do logfile=\"${lastdir}/$i.log\" start=$(date +%s) echo \"make project${project} $i times\" make project${project} \u003e\u003e $logfile end=$(date +%s) pass_count=$(grep -i \"PASS\" $logfile | wc -l) echo \"pass count: $pass_count\" fail_count=$(grep -i \"fail\" $logfile | wc -l) echo \"fail count: $fail_count\" panic_count=$(grep -i \"panic\" $logfile | wc -l) echo \"panic count: $panic_count\" runtime=$((end-start)) echo \"$i $pass_count $fail_count $panic_count $runtime\" \u003e\u003e $summary totalpass=$((totalpass+pass_count)) totalfail=$((totalfail+fail_count)) totalpanic=$((totalpanic+panic_count)) totalruntime=$((totalruntime+runtime)) # if pass, remove the log if [ $removelog -eq 1 ]; then if [ $panic_count -lt 0 ]; then rm $logfile fi sleep 5 fi done echo \"total $totalfail $totalpanic $totalruntime\" \u003e\u003e $summary Project2B、2C、3B的测试时间比较长，有时只需要解决某一个测试点的Bug，没必要跑所有的测试点浪费时间。可以用命令GO111MODULE=on go test -v --count=1 --parallel=1 -p=1 ./kv/test_raftstore -run ^TestSplitConfChangeSnapshotUnreliableRecover3B|| true来运行单个测试点。 下面是我使用的单测试点的测试脚本，注意要改title和中间的测试命令： #!/bin/bash # settings to change project=\"3b\" removelog=1 times=50 # title=\"TestConfChangeRemoveLeader3B\" # title=\"TestSplitRecoverManyClients3B\" # title=\"TestConfChangeRecoverManyClients3B\" title=\"TestSplitConfChangeSnapshotUnreliableRecover3B\" # title=\"TestConfChangeRemoveLeader3B\" # title=\"TestConfChangeSnapshotUnreliableRecover3B\" # title=\"TestSplitConfChangeSnapshotUnreliableRecoverConcurrentPartition3B\" # title=\"TestConfChangeUnreliableRecover3B\" # no change below this line if [ ! -d \"./test_output\" ]; then mkdir \"./test_output\" fi logdir=\"test_output/$project/$title\" if [ ! -d $logdir ]; then mkdir $logdir fi lastdir=\"$logdir/`date +%Y%m%d%H%M%S`\" if [ ! -d $lastdir ]; then mkdir $lastdir fi summary=\"$lastdir/summary.log\" echo \"times pass fail panic runtime panicinfo\" \u003e\u003e $summary totalpass=0 totalfail=0 totalpanic=0 totalruntime=0 for i in $(seq 1 $times) do logfile=\"${lastdir}/$i.log\" start=$(date +%s) echo \"start $i times\" # (GO111MODULE=on go test -v --count=1 --parallel=1 -p=1 ./kv/test_raftstore -run ^TestConfChangeRemoveLeader3B|| true) \u003e\u003e $logfile # (GO111MODULE=on go test -v --count=1 --parallel=1 -p=1 ./kv/test_raftstore -run ^TestSplitRecover3B|| true) \u003e\u003e $logfile # (GO111MODULE=on go test -v --count=1 --parallel=1 -p=1 ./kv/test_raftstore -run ^TestConfChangeRemoveLeader3B|| true) \u003e\u003e $logfile # (GO111MODULE=on go test -v --count=1 --parallel=1 -p=1 ./kv/test_raftstore -run ^TestSplitRecoverManyClients3B|| true) \u003e\u003e $logfile # (GO111MODULE=on go test -v --count=1 --parallel=1 -p=1 ./kv/test_raftstore -run ^TestConfChangeRecoverManyClients3B|| true) \u003e\u003e $logfile (GO111MODULE=on go test -v --count=1 --parallel=1 -p=1 ./kv/test_raftstore -run ^TestSplitConfChangeSnapshotUnreliableRecover3B|| true) \u003e\u003e $logfile # (GO111MODULE=on go test -v --count=1 --parallel=1 -p=1 ./kv/test_raftstore -run ^TestConfChangeRemoveLeader3B|| true) \u003e\u003e $logfile # (GO111MODULE=on go test -v --count=1 --parallel=1 -p=1 ./kv/test_raftstore -run ^TestConfChangeSnapshotUnreliableRecover3B|| true) \u003e\u003e $logfile # (GO111MODULE=on go test -v --count=1 --parallel=1 -p=1 ./kv/test_raftstore -run ^TestSplitConfChangeSnapshotUnreliableRecoverConcurrentPartition3B|| true) \u003e\u003e $logfile # (GO111MODULE=on go test -v --count=1 --parallel=1 -p=1 ./kv/test_raftstore -run ^TestConfChangeUnreliableRecover3B|| true) \u003e\u003e $logfile end=$(date +%s) pass_count=","date":"2024-08-13","objectID":"/posts/tinykv_start/:5:2","tags":["raft","kv","tinykv"],"title":"TinyKV 启~动!","uri":"/posts/tinykv_start/"},{"categories":["kv"],"content":"打印日志 打印日志非常重要，尤其是对于跟踪Project2B、2C、3B中的Bug。 可以在log/log.go​的末尾增加以下代码： const debug = 0 const debug_raft = 0 const debug_raftStore = 0 func DPrintf(format string, a ...interface{}) (n int, err error) { if debug \u003e 0 { _log.Infof(format, a...) } return } func DPrintfRaft(format string, a ...interface{}) (n int, err error) { if debug_raft \u003e 0 { log.Printf(\"[Raft]: \"+format, a...) } return } func DPrintfRaftStore(format string, a ...interface{}) (n int, err error) { if debug_raftStore \u003e 0 { log.Printf(\"[RaftStore]: \"+format, a...) } return } 将debug_xxx​改为大于0之后，在需要打印日志的地方调用log.DPrintfxxx​即可。 我这里分了三种日志类型，最重要的是Raft​和RaftStore​，分别对应Raft层的日志和RaftStore的日志，这样做的好处是面对海量的日志能够很容易找到哪个模块的代码出了问题，也可以很轻松过地关闭一个模块的日志减少干扰。 ‍ ","date":"2024-08-13","objectID":"/posts/tinykv_start/:5:3","tags":["raft","kv","tinykv"],"title":"TinyKV 启~动!","uri":"/posts/tinykv_start/"},{"categories":["kv"],"content":"Tinykv架构 在正式编码之前，我们可以大致了解一下Tinykv的架构，这里我直接结合官方的图大致来讲一下自己的理解。 ​ tinykv架构 ​ Tinykv只关注分布式数据库系统的存储层，可以处理响应来自SQL层的RPC请求，同时还有一个TinyScheduler组件作为整个Tinykv集群的控制中心，从Tinykv的心跳中收集信息，负责一些调度工作，向Tinykv节点发送调度命令以实现负载均衡（project 3c）等等功能。 自下而上解析Tinykv的组成： Engine，即kv存储引擎，是实际存储kv的地方。在Tinykv中使用的是badgerDB，使用了两个DB实例：kv和raftkv，分别用于存储实际的kv数据和raft日志，当然还分别存储了一些状态数据。 Storage，我理解为分布式逻辑层，这里分为了Standalone Storage和RaftStorage Standalone Storage对应project1，在badgerDB的API基础上封装了列族（可以理解为给每个key加上了前缀以实现分类），这是为了在后面project4中实现事务机制。 RaftStorage：project2和3的重点。负责了请求的处理和响应。包括了Raft层和Raft层之上的逻辑RaftStore Raft层用于接收来自上层的Raft日志，实现共识算法，会定期给上层反馈已经提交的日志和要转发的消息等等。 RaftStore则负责消息的封装和路由（分发到raft节点）、raft日志的持久化和命令的应用等等，比较复杂。 server，接收RPC调用和响应，实现MVCC（多版本并发控制）和封装事务性API，对应project4 ‍ 项目目录： ​kv​包含键值存储的实现。 ​raft​包含 Raft 共识算法的实现。 ​scheduler​包含 TinyScheduler 的实现，该实现负责管理 TinyKV 节点和生成时间戳。 ​proto​包含节点和进程之间所有通信的实现，使用基于 gRPC 的协议缓冲区。此包包含 TinyKV 使用的协议定义，以及您可以使用的生成的 Go 代码。 ​log​包含基于级别输出日志的实用程序。 ‍ ","date":"2024-08-13","objectID":"/posts/tinykv_start/:6:0","tags":["raft","kv","tinykv"],"title":"TinyKV 启~动!","uri":"/posts/tinykv_start/"},{"categories":["kv"],"content":"总结 有了上述的一些准备之后就可以开始攻克Tinykv的各个project了，万事开头难，这也是我正儿八经的第一篇博客，后续会更新四个project的思路和踩的一些坑。由于缺乏有关分布式系统的很多知识，理解上可能会有很多谬误和遗漏，欢迎多多提问和指正。 Tinykv 启~动！ ","date":"2024-08-13","objectID":"/posts/tinykv_start/:7:0","tags":["raft","kv","tinykv"],"title":"TinyKV 启~动!","uri":"/posts/tinykv_start/"},{"categories":null,"content":"关于我","date":"2019-08-02","objectID":"/about/","tags":null,"title":"关于我","uri":"/about/"},{"categories":null,"content":"nothing now ","date":"2019-08-02","objectID":"/about/:0:0","tags":null,"title":"关于我","uri":"/about/"}]