# TinyKV Project 3B ç–‘éš¾æ‚ç—‡



æˆ‘åœ¨åštinykvè¿™ä¸ªè¯¾ç¨‹çš„è¿‡ç¨‹ä¸­ï¼Œ3bä¹‹å‰é‡åˆ°çš„é—®é¢˜å¤§å¤šèƒ½åœ¨ç½‘ä¸Šæ‰¾åˆ°è§£å†³æ–¹æ³•ï¼Œå› æ­¤ä¹Ÿæ²¡æ€ä¹ˆåšè®°å½•ï¼›è€Œ3bç”±äºå¼•å…¥äº†Multi Raftä»¥åŠé›†ç¾¤æˆå‘˜å˜æ›´ã€Split Regionç­‰Adminå‘½ä»¤ï¼Œå¯¹å¾ˆå¤šç»†èŠ‚å¤„ç†è¦æ±‚ä¸¥æ ¼ï¼Œå¹¶ä¸”æµ‹è¯•ç‚¹ä¼šæ¨¡æ‹Ÿä¸¢åŒ…å’Œç½‘ç»œéš”ç¦»ç­‰å¼‚å¸¸æƒ…å†µï¼Œéœ€è¦åšè¾ƒå¤šé¢å¤–çš„é’ˆå¯¹æ€§å¤„ç†ï¼Œå› æ­¤æˆ‘åœ¨åšçš„æ—¶å€™é‡åˆ°äº†è®¸å¤šç½‘ä¸Šéš¾ä»¥æ‰¾åˆ°çš„é—®é¢˜ï¼Œé€šè¿‡åœ¨æµ·é‡æ—¥å¿—ä¸­ä¸€æ­¥æ­¥è·Ÿè¸ªæ‰è§£å†³äº†å®ƒä»¬ã€‚

ä¸‹é¢å°†å¯¹æˆ‘åœ¨3bä¸­é‡åˆ°çš„ç–‘éš¾æ‚ç—‡åŠè§£å†³æ–¹æ³•è¿›è¡Œæ±‡æ€»ã€‚

## SnapShotç›¸å…³

### ï¼ˆ1ï¼‰Request Timeout-1

é—®é¢˜åŸå› ï¼šå½“å‰èŠ‚ç‚¹æœ‰snapshotæ­£åœ¨è¢«åº”ç”¨æ—¶ï¼Œæ”¶åˆ°æ–°çš„snapshotæ²¡æœ‰è¿‡æ»¤æ‰

è§£å†³æ–¹æ³•ï¼šå½“`r.RaftLog.pendingSnapshot != nil`â€‹æ—¶ä¸æ¥å—æ–°çš„snapShotï¼Œå¹¶ä¸”ä¸å¤„ç†`Append`â€‹æ¶ˆæ¯ã€‚

```go
func (r *Raft) handleAppendEntries(m pb.Message) {
  if r.RaftLog.pendingSnapshot != nil {
    return
  }
  ...
```

```go
func (r *Raft) handleSnapshot(m pb.Message) {
  if r.RaftLog.pendingSnapshot != nil {
    return
  }
  ...
```

### ï¼ˆ2ï¼‰Request Timeout-2

é—®é¢˜åŸå› ï¼šfolloweråœ¨å¤„ç†appendæ¶ˆæ¯çš„æ—¶å€™ï¼Œ`prevLogIndex`â€‹çš„æ¡ç›®åœ¨followerå·²ç»è¢«Compactæ‰äº†ï¼ˆ**Leaderå½“ä¸­åè€Œæ²¡æœ‰è¢«Compactï¼Œæ€ªäº†**ï¼‰ï¼Œæ­¤æ—¶é€šè¿‡`RaftLog.Term()`â€‹ä¼šè¿”å›é”™è¯¯ï¼Œæ— æ³•å®Œæˆæ—¥å¿—åŒ¹é…åŠ¨ä½œï¼Œå¯¼è‡´leaderä¸­è®°å½•çš„Matchä¼šä¿æŒä¸å˜ï¼Œåœ¨é›†ç¾¤ä¸­åªæœ‰ä¸¤ä¸ªèŠ‚ç‚¹çš„æƒ…å†µä¸‹committedæ— æ³•æ¨è¿›ã€‚

è§£å†³æ–¹æ³•ï¼šfollowerè¿”å›å“åº”ï¼Œä»¤leaderä¸»åŠ¨å‘é€ä¸€ä¸ªSnapshotç»™è‡ªå·±ï¼Œæˆ‘è¿™é‡Œåˆ©ç”¨äº†`MsgAppendResponse`â€‹ä¸­æ²¡æœ‰ä½¿ç”¨çš„`commit`â€‹å­—æ®µã€‚

```go
const ShouldSendSnapByCommit uint64 = 123456

func (r *Raft) handleAppendEntries(m pb.Message) {
ã€‚ã€‚ã€‚
  tTerm, terr := r.RaftLog.Term(m.Index)
    // debug
    lastTerm, _ := r.RaftLog.Term(r.RaftLog.LastIndex())
    if terr != nil {
      log.DPrintfRaft("node %d handleAppendEntries from %d failed, get term failed: %v\n", r.id, m.From, terr)
      log.DPrintfRaft("l.entries: %d, i: %d, l.dummyIndex: %d, l.lastIndex: %d\n", len(r.RaftLog.entries), m.Index, r.RaftLog.dummyIndex, r.RaftLog.LastIndex())

      // tell leader should send snapshot
      r.msgs = append(r.msgs, pb.Message{
        MsgType: pb.MessageType_MsgAppendResponse,
        To:      m.From,
        From:    r.id,
        Term:    lastTerm,
        Index:   0,
        Reject:  true,
        Commit:  ShouldSendSnapByCommit,
      })
      return
ã€‚ã€‚ã€‚
}


func (r *Raft) handleAppendResponse(m pb.Message) {
  if m.Term > r.Term {
    r.greaterTermRspNum += 1
    if r.greaterTermRspNum > GreaterTermRspThreshold {
      r.greaterTermRspNum = 0
      r.becomeFollower(m.Term, None)
      return
    }
    // r.becomeFollower(m.Term, None)
    return
  }

  // special handle for need to send snapshot
  if m.Commit == ShouldSendSnapByCommit {
    r.sendSnapshot(m.From)
    return
  }
ã€‚ã€‚ã€‚
}
```

â€

## TransferLeaderç›¸å…³

### ï¼ˆ1ï¼‰Request Timeoutï¼ˆé€‰ä¸¾å†²çªçš„é—®é¢˜ï¼‰

```go
[RaftStore]: node[[region 1] 2] propose common request: header:<region_id:1 peer:<id:2 store_id:2 > region_epoch:<conf_ver:4 version:1 > > requests:<cmd_type:Put put:<cf:"default" key:"2 00000053" value:"x 2 53 y" > > , leader: 2
2024/02/22 07:53:32 [Raft]: node 2 is leader, but leadTransferee is not None, stop accept propose
2024/02/22 07:53:32 [Raft]: node 2 tryUpdateCommit: len(r.Prs) = 2, r.RaftLog.LastIndex() = 584, r.committed = 583
2024/02/22 07:53:32 [Raft]: node 2 transfer leader to 1 success! sendTimeoutNow!
2024/02/22 07:53:32 [RaftStore]: -------------------------------------------------------------------------
2024/02/22 07:53:32 [RaftStore]: node[[region 1] 2] propose common request: header:<region_id:1 peer:<id:2 store_id:2 > region_epoch:<conf_ver:4 version:1 > > requests:<cmd_type:Snap snap:<> > , leader: 2
2024/02/22 07:53:32 [Raft]: node 2 tryUpdateCommit: len(r.Prs) = 2, r.RaftLog.LastIndex() = 585, r.committed = 584
2024/02/22 07:53:32.433521 cluster.go:200: [0;37m[info] [Cluster] request err resp. err: message:"raft proposal dropped" , req: header:<region_id:1 peer:<id:2 store_id:2 > region_epoch:<conf_ver:4 version:1 > > requests:<cmd_type:Put put:<cf:"default" key:"1 00000062" value:"x 1 62 y" > > [0m
2024/02/22 07:53:32.433521 cluster.go:200: [0;37m[info] [Cluster] request err resp. err: message:"raft proposal dropped" , req: header:<region_id:1 peer:<id:2 store_id:2 > region_epoch:<conf_ver:4 version:1 > > requests:<cmd_type:Put put:<cf:"default" key:"4 00000046" value:"x 4 46 y" > > [0m
2024/02/22 07:53:32.433564 cluster.go:200: [0;37m[info] [Cluster] request err resp. err: message:"raft proposal dropped" , req: header:<region_id:1 peer:<id:2 store_id:2 > region_epoch:<conf_ver:4 version:1 > > requests:<cmd_type:Put put:<cf:"default" key:"2 00000053" value:"x 2 53 y" > > [0m
2024/02/22 07:53:32 [RaftStore]: node[[region 1] 2]: process common request: header:<region_id:1 peer:<id:2 store_id:2 > region_epoch:<conf_ver:4 version:1 > > requests:<cmd_type:Put put:<cf:"default" key:"0 00000057" value:"x 0 57 y" > > 
2024/02/22 07:53:32 [Raft]: node 1 handleAppendEntries from 2, lastIndex: 584,preLogIndex: 583, preLogTerm: 6, len(m.Entries): 1
2024/02/22 07:53:32 [Raft]: node 1 handleAppendEntries from 2 success, conflictFlag: true
2024/02/22 07:53:32 [Raft]: node 1 handleAppendEntries from 2, lastIndex: 584,preLogIndex: 583, preLogTerm: 6, len(m.Entries): 1
2024/02/22 07:53:32 [Raft]: node 1 handleAppendEntries from 2 success, conflictFlag: true
2024/02/22 07:53:32 [Raft]: node 1 handleAppendEntries from 2, lastIndex: 584,preLogIndex: 584, preLogTerm: 6, len(m.Entries): 0
2024/02/22 07:53:32 [Raft]: node 1 handleAppendEntries from 2 success, conflictFlag: true
```

å¯ä»¥çœ‹åˆ°ï¼Œå½“Node 2`sendTimeOutNow`â€‹ç»™1ä¹‹åï¼Œç”±äºè¿˜æ˜¯leaderï¼Œä¸”`leaderTransferee == None`â€‹ï¼Œèƒ½å¤Ÿproposeæ–°çš„entryï¼Œå¯¼è‡´node 1å‘èµ·é€‰ä¸¾ä¹‹åï¼Œæ²¡åŠæ³•å¾—åˆ°é€‰ç¥¨ï¼Œæ²¡åŠæ³•å½“é€‰ï¼Œä»è€Œé€ æˆå†²çªã€‚

è§£å†³æ–¹æ³•ï¼šleaderåœ¨`sendTimeoutNow`â€‹ä¹‹åä¸»åŠ¨å˜ä¸ºfollowerï¼Œä»è€Œä¸ä¼šæ¥æ”¶åˆ°proposeäº§ç”Ÿæ–°çš„entryï¼Œç›®æ ‡èŠ‚ç‚¹èƒ½å¤Ÿé¡ºåˆ©å½“é€‰ã€‚

â€

### ï¼ˆ2ï¼‰TransferLeaderçš„è¶…æ—¶æœºåˆ¶

å¦‚æœé¢†å¯¼æƒè½¬ç§»çš„ç›®æ ‡èŠ‚ç‚¹å¤±è”ï¼Œä¼šå¯¼è‡´leaderä¸€ç›´å¤„äºleadTransfereeçš„çŠ¶æ€ï¼Œæ²¡åŠæ³•propose entryï¼Œå› æ­¤è¦ä¸ºTransferLeaderå¢åŠ è¶…æ—¶æœºåˆ¶ï¼Œæˆ‘ç›´æ¥ä½¿ç”¨é€‰ä¸¾è¶…æ—¶ä½œä¸ºé¢†å¯¼æƒè½¬ç§»çš„è¶…æ—¶æ—¶é—´ã€‚TransferLeaderæˆåŠŸå`becomefollower()`â€‹ä¼šå®Œæˆæ¸…é›¶é€‰ä¸¾è¶…æ—¶çš„å·¥ä½œã€‚

```go
func (r *Raft) tickLeader() {
  // special handle with leadTransforee
  if r.leadTransferee != None {
    r.electionElapsed += 1
    // can't transfer success before a ElectTimeout, give up
    if r.electionElapsed >= r.randElectTimeout {
      r.leadTransferee = None
      r.electionElapsed = 0
      r.randElectTimeout = r.electionTimeout + rand.Intn(r.electionTimeout)
    }
  }
  ...
```

â€

â€

## confChangeç›¸å…³

### ï¼ˆ1ï¼‰æ— æ³•æ­£ç¡®å¤„ç†confChangeå‘½ä»¤

```go
--- FAIL: TestBasicConfChange3B (6.34s)
panic: have peer: id:2 store_id:2  [recovered]
  panic: have peer: id:2 store_id:2
```

é”™è¯¯åŸå› ï¼šConfChangeæ¡ç›®ä¸­çš„æ•°æ®ä¸æ­£ç¡®

Solutionï¼šåœ¨Proposeä¸­ï¼Œé’ˆå¯¹ ConfChange è¯·æ±‚çš„åºåˆ—åŒ–å’Œååºåˆ—åŒ–æ–¹å¼ä¸å…¶ä»–è¯·æ±‚ä¸åŒ

```go
  ctx, marErr := msg.Marshal()
  if marErr != nil {
    panic(marErr)
  }
  err := d.RaftGroup.ProposeConfChange(eraftpb.ConfChange{
    ChangeType: msg.AdminRequest.ChangePeer.ChangeType,
    NodeId:     msg.AdminRequest.ChangePeer.Peer.Id,
    Context:    ctx,			// notice
  })
```

### ï¼ˆ2ï¼‰AddNodeå RequestTimeoutï¼ˆå¯¹äºæœªåˆå§‹åŒ–çš„RaftèŠ‚ç‚¹çš„å¤„ç†ï¼‰

```go
-- FAIL: TestBasicConfChange3B (52.68s)
panic: request timeout [recovered]
  panic: request timeout
```

é”™è¯¯åŸå› ï¼šå¯¹äºRaftèŠ‚ç‚¹ä¸åœ¨é›†ç¾¤å½“ä¸­ï¼ˆå°šæœªåˆå§‹åŒ–ï¼‰çš„æƒ…å†µï¼Œå±è”½æ‰äº†`HeartBeat`â€‹å’Œ`Snapshot`â€‹æ¶ˆæ¯ï¼Œä½¿å¾—èŠ‚ç‚¹ä¸èƒ½é€šè¿‡leaderå‘é€çš„Snapshotå®Œæˆåˆå§‹åŒ–ï¼Œè¿›è€Œå¯¼è‡´committedå§‹ç»ˆæ— æ³•å‘å‰æ¨è¿›ï¼ˆ**ç”±äºæ–°èŠ‚ç‚¹çš„åŸå› æ— æ³•æ»¡è¶³â€œé›†ç¾¤å¤§å¤šæ•°â€çš„æ¡ä»¶**ï¼‰ï¼Œè¯·æ±‚æ— æ³•åº”ç”¨è€Œè¶…æ—¶ã€‚

Solutionï¼šå¯¹äºRaftèŠ‚ç‚¹ä¸åœ¨é›†ç¾¤å½“ä¸­ï¼ˆå°šæœªåˆå§‹åŒ–ï¼‰çš„æƒ…å†µï¼Œåªèƒ½å±è”½é€‰ä¸¾ç›¸å…³çš„æ¶ˆæ¯ã€‚

### ï¼ˆ3ï¼‰â€‹[region x] x meta corruption detectedâ€‹

```go
2023/12/29 12:16:14 request err resp.
panic: [region 1] 5 meta corruption detected

goroutine 321 [running]:
github.com/pingcap-incubator/tinykv/kv/raftstore.(*peerMsgHandler).destroyPeer(0xc17c9b9e10)
  /home/sszgwdk/project/tinykv/tinykv/kv/raftstore/peer_msg_handler.go:975 +0x265
github.com/pingcap-incubator/tinykv/kv/raftstore.(*peerMsgHandler).processConfChange(0xc17c9b9e10, 0xc17c924d50?, 0xc17c9b9aa0)
  /home/sszgwdk/project/tinykv/tinykv/kv/raftstore/peer_msg_handler.go:253 +0x6de
github.com/pingcap-incubator/tinykv/kv/raftstore.(*peerMsgHandler).process(0xc17c9b9e10, 0xc17c9b9c40)
  /home/sszgwdk/project/tinykv/tinykv/kv/raftstore/peer_msg_handler.go:351 +0x199
github.com/pingcap-incubator/tinykv/kv/raftstore.(*peerMsgHandler).HandleRaftReady(0xc17c9b9e10)
  /home/sszgwdk/project/tinykv/tinykv/kv/raftstore/peer_msg_handler.go:450 +0x408
github.com/pingcap-incubator/tinykv/kv/raftstore.(*raftWorker).run(0xc000264920, 0xc000039440, 0x0?)
  /home/sszgwdk/project/tinykv/tinykv/kv/raftstore/raft_worker.go:70 +0x439
created by github.com/pingcap-incubator/tinykv/kv/raftstore.(*Raftstore).startWorkers
  /home/sszgwdk/project/tinykv/tinykv/kv/raftstore/raftstore.go:271 +0x17b
FAIL	github.com/pingcap-incubator/tinykv/kv/test_raftstore	2.015s
FAIL
```

æ ¹æ®æŠ¥é”™ä¿¡æ¯å¯ä»¥æ‰¾åˆ°å‡ºé”™çš„ä½ç½®åœ¨`destroyPeer`â€‹ï¼Œå‡ºé”™çš„ä»£ç ï¼š

```go
if _, ok := meta.regions[regionID]; !ok {
  panic(d.Tag + " meta corruption detected")
}
```

å‡ºé”™åŸå› ï¼šå¯¹äºRemoveNodeå‘½ä»¤çš„applyï¼Œä¸éœ€è¦å¯¹`storeMeta`â€‹è¿›è¡Œä¿®æ”¹ï¼Œ`destoryPeer`â€‹ä¼šè´Ÿè´£å¯¹`storeMeta`â€‹çš„ä¿®æ”¹ã€‚å› æ­¤ä¸‹åˆ—æ“ä½œæ˜¯ä¸åº”è¯¥åšçš„ï¼š

```go
d.ctx.storeMeta.Lock()
delete(d.ctx.storeMeta.regions, targetPeer.Id)
d.ctx.storeMeta.regionRanges.Delete(&regionItem{region: region})
d.ctx.storeMeta.Unlock()
```

Solutionï¼šåœ¨RemoveNodeçš„Applyå¤„ç†ä¸­å»æ‰

### ï¼ˆ4ï¼‰confChange Request TimeOutï¼ˆproposeæˆåŠŸï¼Œä½†æ˜¯confChangeæ— æ³•æ·»åŠ åˆ°RaftLogå½“ä¸­ï¼‰

```go
--- FAIL: TestConfChangeRecover3B (58.85s)
panic: request timeout [recovered]
  panic: request timeout
```

é”™è¯¯åŸå› ï¼šå½“`r.PendingConfIndex != None`â€‹ï¼Œä¼šå¿½ç•¥æ‰€æœ‰æ–°çš„confChange Entryè€Œä¸æŠ¥é”™ï¼Œè¿™å°±é€ æˆäº†èƒ½å¤Ÿ`ProposeConfChange`â€‹æˆåŠŸï¼Œä½†æ˜¯Entryå´æ²¡æœ‰appendè¿›å»ã€‚

```go
func (r *Raft) appendEntries(entries ...*pb.Entry) {
  ents := make([]pb.Entry, 0)
  for _, e := range entries {
    if e.EntryType == pb.EntryType_EntryConfChange {
      if r.PendingConfIndex != None {
        continue
      }
      r.PendingConfIndex = e.Index
    }
    ents = append(ents, pb.Entry{
      EntryType: e.EntryType,
      Term:      e.Term,
      Index:     e.Index,
      Data:      e.Data,
    })
  }
  r.RaftLog.appendEntries(ents...)
  r.Prs[r.id].Match = r.RaftLog.LastIndex()
  r.Prs[r.id].Next = r.RaftLog.LastIndex() + 1
}
```

æ˜¾ç„¶è¿™ä¸ªæ¡ä»¶`r.PendingConfIndex != None`â€‹æ˜¯ä¸åˆé€‚çš„ï¼Œé”™è¯¯åœ°å¿½ç•¥äº†ä¸è¯¥å¿½ç•¥çš„confChange Entryã€‚

æ­£ç¡®çš„æ¡ä»¶åº”è¯¥æ˜¯ï¼š`r.PendingConfIndex != None && r.PendingConfIndex > r.RaftLog.applied`â€‹ï¼Œä»£è¡¨è¿˜æœ‰confChange Entryæ²¡æœ‰è¢«Applyï¼ˆå·²ç»Applyçš„æ˜¾ç„¶å¯ä»¥å¿½ç•¥äº†ï¼‰

### ï¼ˆ5ï¼‰åˆ é™¤èŠ‚ç‚¹æ—¶ä¼šé‡åˆ° Request timeout é—®é¢˜

è§‚å¯Ÿæ—¥å¿—ï¼Œå‘ç°ä¸€ç›´åœ¨è¿›è¡Œé€‰ä¸¾

â€‹![image](image-20240121115130-zowanex.png "test log")â€‹

è§£å†³å‚è€ƒï¼š

[TinyKV-White-Paper/Project3-MultiRaftKV.md at main Â· Smith-Cruise/TinyKV-White-Paper (github.com)](https://github.com/Smith-Cruise/TinyKV-White-Paper/blob/main/Project3-MultiRaftKV.md)

é—®é¢˜åŸå› ï¼šåªå‰©ä¸¤ä¸ªèŠ‚ç‚¹ï¼Œç„¶åè¢«ç§»é™¤çš„é‚£ä¸ªèŠ‚ç‚¹æ­£å¥½æ˜¯ Leaderã€‚å› ä¸ºç½‘ç»œæ˜¯ unreliableï¼ŒLeader å¹¿æ’­ç»™å¦ä¸€ä¸ª Node çš„å¿ƒè·³æ­£å¥½è¢«ä¸¢äº†ï¼Œä¹Ÿå°±æ˜¯å¦ä¸€ä¸ªèŠ‚ç‚¹çš„ commit å¹¶ä¸ä¼šè¢«æ¨è¿›ï¼Œä¹Ÿå°±æ˜¯å¯¹æ–¹èŠ‚ç‚¹å¹¶ä¸ä¼šæ‰§è¡Œ remove node æ“ä½œã€‚è€Œè¿™ä¸€åˆ‡ Leader å¹¶ä¸çŸ¥é“ï¼Œå®ƒè‡ªå·±è°ƒç”¨ `d.destroyPeer()`â€‹ å·²ç»é”€æ¯äº†ã€‚æ­¤æ—¶å¦ä¸€ä¸ªèŠ‚ç‚¹å¹¶æ²¡æœ‰ç§»é™¤ Leaderï¼Œå®ƒä¼šå‘èµ·é€‰ä¸¾ï¼Œä½†æ˜¯æ°¸è¿œèµ¢ä¸äº†ï¼Œå› ä¸ºéœ€è¦æ”¶åˆ°è¢«ç§»é™¤ Leader çš„æŠ•ç¥¨ã€‚

ç™½çš®ä¹¦æä¾›çš„è§£å†³æ–¹æ³•ï¼š

1. åœ¨ propose é˜¶æ®µï¼Œå¦‚æœå·²ç»å¤„äºä¸¤èŠ‚ç‚¹ï¼Œè¢«ç§»é™¤çš„æ­£å¥½æ˜¯ Leaderï¼Œé‚£ä¹ˆç›´æ¥æ‹’ç»è¯¥ proposeï¼Œå¹¶ä¸”å‘èµ· Transfer Leader åˆ°å¦ä¸€ä¸ªèŠ‚ç‚¹ä¸Šã€‚Client åˆ°æ—¶å€™ä¼šé‡è¯• remove node æŒ‡ä»¤ã€‚
2. åœ¨ apply é˜¶æ®µ DestroyPeer ä¸Šé¢åšä¸€ä¸ªä¿é™©ï¼ˆæœ‰å¿…è¦ï¼Œè™½ç„¶ç”¨åˆ°çš„æ¦‚ç‡å¾ˆä½ï¼‰ï¼Œè®© Leader åœ¨è‡ªå·±è¢« remove å‰é‡å¤å¤šæ¬¡å‘é€å¿ƒè·³åˆ°ç›®æ ‡èŠ‚ç‚¹ï¼Œå°è¯•æ¨åŠ¨ç›®æ ‡èŠ‚ç‚¹çš„ commitã€‚é‡å¤å¤šæ¬¡æ˜¯ä¸ºäº†æŠµæ¶ˆæµ‹è¯•ç”¨ä¾‹çš„ unreliable

æˆ‘è¿™é‡Œä¸¤ä¸ªéƒ½å®ç°äº†ï¼Œä¸åŒçš„æ˜¯ç¬¬äºŒä¸ªçš„ä¿é™©æªæ–½æˆ‘å‘é€çš„æ˜¯Appendæ¶ˆæ¯è€Œä¸æ˜¯å¿ƒè·³æ¶ˆæ¯ï¼ˆæˆ‘çš„Raftå±‚å®ç°ä¸­followerå¤„ç†å¿ƒè·³æ²¡æœ‰æ¨è¿›Commitï¼‰ï¼Œå¹¶ä¸”æ ¹æ®æˆ‘çš„æµ‹è¯•ï¼Œé‡å¤çš„æ¬¡æ•°ä¸º5å·²ç»å¤Ÿäº†ã€‚

å³ä½¿å¦‚æ­¤ï¼Œä»ç„¶åªæ˜¯åœ¨æ¦‚ç‡ä¸Šé¿å…äº†è¿™ä¸ªé—®é¢˜ï¼Œä¸èƒ½å®Œç¾çš„è§£å†³ã€‚

â€

## Splitç›¸å…³

### ï¼ˆ1ï¼‰Snapè¯·æ±‚å¤„ç†è¶…æ—¶

æ—¥å¿—æ˜¾ç¤ºæ€»æ˜¯snapæ¶ˆæ¯çš„å¤„ç†é—®é¢˜ï¼Œæ€»æ˜¯æŠ¥é”™`key not in region`â€‹ï¼Œè€Œsnapä¸æ¶‰åŠkeyã€‚

é—®é¢˜åŸå› ï¼šå‡ºåœ¨proposeä¸Šé¢ï¼Œå¯¹common requestè¿›è¡Œ`key in region`â€‹æ£€æŸ¥æ—¶åº”è¯¥è·³è¿‡`Snap request`â€‹ï¼Œè€Œè¿™é‡Œå¿˜è®°è·³è¿‡äº†ã€‚

è§£å†³æ–¹æ³•ï¼š

```go
  // check if key in region
  for _, req := range msg.Requests {
    if req.CmdType != raft_cmdpb.CmdType_Snap {
      if err := util.CheckKeyInRegion(getRequestKey(req), d.Region()); err != nil {
        cb.Done(ErrResp(err))
        return
      }
    }
  }
```

### ï¼ˆ2ï¼‰key [xxx...] is not in region xx

```go
panic: key [49 55 32 48 48 48 48 48 48 48 48] is not in region id:1 end_key:"13 00000001" region_epoch:<conf_ver:1 version:2 > peers:<id:1 store_id:1 > peers:<id:2 store_id:2 > peers:<id:3 store_id:3 > peers:<id:4 store_id:4 > peers:<id:5 store_id:5 > 

goroutine 327 [running]:
github.com/pingcap-incubator/tinykv/kv/storage/raft_storage.(*RegionIterator).Seek(0xc17cb84ed0, {0xc17cc01d50, 0xb, 0x10?})
  /home/sszgwdk/project/tinykv/tinykv/kv/storage/raft_storage/region_reader.go:83 +0xec
github.com/pingcap-incubator/tinykv/kv/test_raftstore.(*Cluster).Scan(0xb86019?, {0xc17cc01d50, 0xb, 0x10}, {0xc17c6dddb0, 0xb, 0x20})
  /home/sszgwdk/project/tinykv/tinykv/kv/test_raftstore/cluster.go:388 +0x322
github.com/pingcap-incubator/tinykv/kv/test_raftstore.GenericTest.func1(0x11, 0x0?)
  /home/sszgwdk/project/tinykv/tinykv/kv/test_raftstore/test_test.go:218 +0x533
github.com/pingcap-incubator/tinykv/kv/test_raftstore.runClient(0x0?, 0x0?, 0x0?, 0x0?)
  /home/sszgwdk/project/tinykv/tinykv/kv/test_raftstore/test_test.go:27 +0x78
created by github.com/pingcap-incubator/tinykv/kv/test_raftstore.SpawnClientsAndWait
  /home/sszgwdk/project/tinykv/tinykv/kv/test_raftstore/test_test.go:37 +0x85
FAIL	github.com/pingcap-incubator/tinykv/kv/test_raftstore	6.645s
FAIL
```

é—®é¢˜åŸå› åˆ†æï¼šæŠ¥é”™æ˜¯ç”±Scanæ“ä½œï¼ˆå¯¹åº”Snapå‘½ä»¤ï¼‰å¼•èµ·çš„ï¼ŒSeek()å‡½æ•°äº§ç”Ÿçš„æŠ¥é”™ï¼Œé”™è¯¯åŸå› æ˜¯keyä¸åœ¨è¯¥regionä¸­ã€‚

```go
func (it *RegionIterator) Seek(key []byte) {
  if err := util.CheckKeyInRegion(key, it.region); err != nil {
    panic(err)
  }
  it.iter.Seek(key)
}
```

çŒœæƒ³å“ªäº›æƒ…å†µä¼šå¯¼è‡´ä¸Šè¿°é”™è¯¯çš„å‘ç”Ÿï¼š

1. å¤„ç†Snapå‘½ä»¤æ—¶`region := c.GetRegion(key)`â€‹è·å–äº†é”™è¯¯çš„regionï¼ˆè¿™ç§æƒ…å†µæœ‰å¯èƒ½æ˜¯åœ¨æ›´æ–°regionåï¼Œæ²¡æœ‰åŠæ—¶å‘ŠçŸ¥Schedulerï¼‰
2. ç›®æ ‡regionåœ¨è¯·æ±‚è¿‡ç¨‹ä¸­å‘ç”Ÿäº†å˜åŒ–ï¼Œå¯¼è‡´é”™è¯¯äº§ç”Ÿï¼ˆç„¶è€ŒSnapè¯·æ±‚ä¸åŒ…å«keyå­—æ®µï¼Œæ²¡åŠæ³•åœ¨raftStoreä¸­é€šè¿‡`checkKeyInRegion`â€‹æ£€æŸ¥ï¼Œæ‰€ä»¥ä¸ºäº†é¿å…Snapçš„keyé”™è¯¯ï¼Œå½“regionå‘ç”Ÿå˜åŒ–æ—¶ï¼Œæ¸…ç©ºæ‰€æœ‰proposalsä¹ˆï¼Ÿï¼Ÿï¼‰

å†ä»”ç»†æ‰“å°å’Œè§‚å¯Ÿæ—¥å¿—

```go
2024/02/04 17:23:03 [RaftStore]: process common request: header:<region_id:1 peer:<id:3 store_id:3 > region_epoch:<conf_ver:1 version:1 > > requests:<cmd_type:Snap snap:<> > 
2024/02/04 17:23:03 [RaftStore]: process common request: header:<region_id:1 peer:<id:3 store_id:3 > region_epoch:<conf_ver:1 version:1 > > requests:<cmd_type:Snap snap:<> > 
2024/02/04 17:23:03 [RaftStore]: process admin request: Split
2024/02/04 17:23:03.477082 cluster.go:204: [0;37m[info] [Cluster] kv/test_raftstore/cluster.go: request success. req: header:<region_id:1 peer:<id:3 store_id:3 > region_epoch:<conf_ver:1 version:1 > > requests:<cmd_type:Snap snap:<> > [0m
2024/02/04 17:23:03.477110 cluster.go:204: [0;37m[info] [Cluster] kv/test_raftstore/cluster.go: request success. req: header:<region_id:1 peer:<id:3 store_id:3 > region_epoch:<conf_ver:1 version:1 > > requests:<cmd_type:Snap snap:<> > [0m
2024/02/04 17:23:03.477145 region_reader.go:84: [0;37m[info] seek key [51 32 48 48 48 48 48 48 48 48] in region 1 failed: key [51 32 48 48 48 48 48 48 48 48] is not in region id:1 end_key:"18 00000000" region_epoch:<conf_ver:1 version:2 > peers:<id:1 store_id:1 > peers:<id:2 store_id:2 > peers:<id:3 store_id:3 > peers:<id:4 store_id:4 > peers:<id:5 store_id:5 > [0m
2024/02/04 17:23:03.477158 region_reader.go:84: [0;37m[info] seek key [56 32 48 48 48 48 48 48 48 48] in region 1 failed: key [56 32 48 48 48 48 48 48 48 48] is not in region id:1 end_key:"18 00000000" region_epoch:<conf_ver:1 version:2 > peers:<id:1 store_id:1 > peers:<id:2 store_id:2 > peers:<id:3 store_id:3 > peers:<id:4 store_id:4 > peers:<id:5 store_id:5 > [0m
```

å‘ç°é”™è¯¯åŸå› æ˜¯ï¼Œå·²ç»å¤„ç†å®Œäº†Snapè¯·æ±‚ï¼Œä½†æ˜¯ä¹‹åæ‰§è¡Œäº†ä¸€ä¸ªSplitæ“ä½œï¼Œå¯¼è‡´regionæ›´æ–°ä»¥åï¼Œå†è¿›è¡ŒScanå°±å‡ºé”™äº†ï¼Œè¿™æ˜¯å› ä¸ºSnapå“åº”ä¸­åŒ…å«ä¸€ä¸ªå½“å‰regionçš„æŒ‡é’ˆï¼ŒSplitä¹‹åå°±ä¼šå¯¼è‡´`key not in region`â€‹çš„é”™è¯¯ã€‚

è§£å†³æ–¹æ³•ï¼šåœ¨Snapå‘½ä»¤ä¹‹åæ‰§è¡Œçš„Splitå‘½ä»¤ä¸åº”å½“é€ æˆå½±å“ï¼Œå› æ­¤è¦åœ¨è¿”å›Snapå“åº”æ—¶æ‹·è´ä¸€ä»½regionï¼Œä»è€Œé¿å…åé¢çš„å‘½ä»¤å¯¹regionçš„ä¿®æ”¹

```go
case raft_cmdpb.CmdType_Snap:
  // solve key not in region error: copy region
  region := new(metapb.Region)
  err := util.CloneMsg(d.Region(), region)
  if err != nil {
    panic(err)
  }

  // resp.Responses = []*raft_cmdpb.Response{{CmdType: raft_cmdpb.CmdType_Snap, Snap: &raft_cmdpb.SnapResponse{Region: d.Region()}}}
  resp.Responses = append(resp.Responses, &raft_cmdpb.Response{CmdType: raft_cmdpb.CmdType_Snap, Snap: &raft_cmdpb.SnapResponse{Region: region}})
  p.cb.Txn = d.peerStorage.Engines.Kv.NewTransaction(false)

```

### ï¼ˆ3ï¼‰test timed out after 10m0s

è¿™ä¸ªæ˜¯å›°æ‰°æˆ‘æœ€ä¹…çš„ä¸€ä¸ªé”™è¯¯ï¼Œåœ¨ä¸Šä¸€ç¯‡project3bæ€è·¯ä¸­ä¹Ÿæåˆ°äº†ã€‚

é”™è¯¯åŸå› æ˜¯æµ‹è¯•æ—¶é—´å¤ªä¹…äº†ã€‚ä½†æ˜¯æˆ‘æ ¹æœ¬ä¸çŸ¥é“ä¸ºä»€ä¹ˆè¿™ä¹ˆä¹…ï¼Œä¸€å¼€å§‹ä»¥ä¸ºæ˜¯é€‰ä¸¾å¤ªæ…¢äº†ï¼Œç›´åˆ°çœ‹åˆ°äº†`peer`â€‹ä¸­ä¸¤ä¸ªæˆå‘˜ï¼š

```go
  // An inaccurate difference in region size since last reset.
  // split checker is triggered when it exceeds the threshold, it makes split checker not scan the data very often
  // (Used in 3B split)
  SizeDiffHint uint64
  // Approximate size of the region.
  // It's updated everytime the split checker scan the data
  // (Used in 3B split)
  ApproximateSize *uint64
```

åŸæ¥split checkerä¼šä¾æ®`SizeDiffHint`â€‹æ¥åˆ¤æ–­regionæ‰¿è½½çš„æ•°æ®é‡æ˜¯å¦è¶…å‡ºé˜ˆå€¼ï¼Œä»è€Œè§¦å‘splitæ“ä½œã€‚

å› æ­¤è¦åšå¦‚ä¸‹ä¸¤ä¸ªä¿®æ”¹

1. Apply Admin_Splitå®Œæˆåï¼Œè¦å¯¹`SizeDiffHint`â€‹å’Œ`ApproximateSize`â€‹æ›´æ–°

    ```go
    // clear region size
    d.SizeDiffHint = 0
    d.ApproximateSize = new(uint64)
    ```
2. Apply `put/delete`â€‹æ—¶å¯¹`SizeDiffHint`â€‹è¿›è¡Œè°ƒæ•´ï¼Œåœ¨ `Put`â€‹ çš„æ—¶å€™ï¼Œ`SizeDiffHint`â€‹ åŠ ä¸Š `key`â€‹ å’Œ `value`â€‹ çš„å¤§å°ï¼›åœ¨ `Delete`â€‹ çš„æ—¶å€™ï¼Œå‡å» `key`â€‹ çš„å¤§å°ã€‚

    ```go
    case raft_cmdpb.CmdType_Put:
      wb.SetCF(req.Put.Cf, req.Put.Key, req.Put.Value)
      d.SizeDiffHint += uint64(len(req.Put.Key) + len(req.Put.Value))

    case raft_cmdpb.CmdType_Delete:
      wb.DeleteCF(req.Delete.Cf, req.Delete.Key)
      d.SizeDiffHint -= uint64(len(req.Delete.Key))
    ```

> å½“æ—¶æ²¡ä»”ç»†çœ‹ç™½çš®ä¹¦ï¼Œå®ƒé‡Œé¢æœ‰æåˆ°è¿‡è¿™ä¸ªé—®é¢˜ï¼š
>
> åœ¨ `nclient >= 8 && crash = true && split = true`â€‹ è¿™ç§æ¡ä»¶ä¸‹ï¼Œæµ‹è¯•åœ¨ Delete é˜¶æ®µå¡æ­»é—®é¢˜ï¼Œè¿™æ˜¯å› ä¸ºåœ¨ apply `CmdType_Put`â€‹ å’Œ `CmdType_Delete`â€‹ è¯·æ±‚çš„æ—¶å€™æ²¡æœ‰æ›´æ–° `SizeDiffHint`â€‹ã€‚å› æ­¤éœ€è¦åœ¨ `Put`â€‹ çš„æ—¶å€™ï¼Œ`SizeDiffHint`â€‹ åŠ ä¸Š `key`â€‹ å’Œ `value`â€‹ çš„å¤§å°ï¼›åœ¨ `Delete`â€‹ çš„æ—¶å€™ï¼Œå‡å» `key`â€‹ çš„å¤§å°ã€‚

### ï¼ˆ4ï¼‰runtime error: index out of range [1] with length 1

```go
2024/02/16 00:50:40.882060 peer.go:55: [0;37m[info] [region 1] replicates peer with ID 14[0m
2024/02/16 00:50:40.885097 peer_storage.go:182: [0;37m[info] [region 1] 5 requesting snapshot[0m
2024/02/16 00:50:40.984182 peer_storage.go:341: [0;37m[info] [region 1] 14 begin to apply snapshot[0m
2024/02/16 00:50:40.984211 region_task.go:93: [0;37m[info] begin apply snap data. [regionId: 1][0m
2024/02/16 00:50:40.984223 region_task.go:137: [0;37m[info] succeed in deleting data in range. [regionId: 1, startKey: , endKey: 32203030303030303033][0m
2024/02/16 00:50:40.996109 snap.go:700: [0;37m[info] apply snapshot ingested 1 tables[0m
2024/02/16 00:50:40.996163 region_task.go:117: [0;37m[info] applying new data. [regionId: 1, timeTakes: 11.892027ms][0m
panic: runtime error: index out of range [1] with length 1

goroutine 288 [running]:
github.com/pingcap-incubator/tinykv/kv/raftstore.(*peerMsgHandler).processAdminRequest(0xc191a41e10, 0xc00000000a?, 0x27?)
  /home/sszgwdk/project/tinykv/tinykv/kv/raftstore/peer_msg_handler.go:137 +0xe08
github.com/pingcap-incubator/tinykv/kv/raftstore.(*peerMsgHandler).process(0xc191a41e10, 0xc224bbdea0)
  /home/sszgwdk/project/tinykv/tinykv/kv/raftstore/peer_msg_handler.go:442 +0x347
github.com/pingcap-incubator/tinykv/kv/raftstore.(*peerMsgHandler).HandleRaftReady(0xc191a41e10)
  /home/sszgwdk/project/tinykv/tinykv/kv/raftstore/peer_msg_handler.go:677 +0x57f
github.com/pingcap-incubator/tinykv/kv/raftstore.(*raftWorker).run(0xc1051b3840, 0xc000194d20, 0x0?)
  /home/sszgwdk/project/tinykv/tinykv/kv/raftstore/raft_worker.go:70 +0x439
created by github.com/pingcap-incubator/tinykv/kv/raftstore.(*Raftstore).startWorkers
  /home/sszgwdk/project/tinykv/tinykv/kv/raftstore/raftstore.go:271 +0x17b
FAIL	github.com/pingcap-incubator/tinykv/kv/test_raftstore	6.899s
FAIL
```

æŠ¥é”™ç‚¹åœ¨åº”ç”¨Splitå‘½ä»¤å¤„

```go
  case raft_cmdpb.AdminCmdType_Split:
    leftRegion := d.Region()
    // check
    ...

    // backup rawRegion for delete regionRange
    rawRegion := new(metapb.Region)
    util.CloneMsg(leftRegion, rawRegion)

    // create rightRegion
    //
    // create rightRegion
    //
    rightRegion := new(metapb.Region)
    util.CloneMsg(leftRegion, rightRegion)
    // newPeers and split Region
    newPeers := make([]*metapb.Peer, 0)
    for i, peer := range leftRegion.Peers {
      newPeers = append(newPeers, &metapb.Peer{
        Id:      split.NewPeerIds[i],
        StoreId: peer.StoreId,
      })
    }

    // [StartKey, SplitKey) -> leftRegion
    // [SplitKey, EndKey) -> rightRegion
    rightRegion.Id = split.NewRegionId
    rightRegion.StartKey = split.SplitKey
    rightRegion.Peers = newPeers
    leftRegion.EndKey = split.SplitKey

    leftRegion.RegionEpoch.Version += 1
    rightRegion.RegionEpoch.Version += 1
    ...
```

è¿™é‡Œé»˜è®¤`leftRegion.Peers`â€‹ä¸`split.NewPeerIds`â€‹é•¿åº¦ç›¸åŒäº†ï¼Œä½†å®é™…å¯èƒ½æ²¡æœ‰ï¼Œå› æ­¤è¦ä¿®æ”¹ï¼Œä¿è¯ä¸ä¼šè¶Šç•Œå³å¯ã€‚

æˆ‘è¿™é‡Œç›´æ¥åˆ¤å®šé•¿åº¦æ˜¯å¦ç›¸ç­‰äº†ï¼Œå¦‚æœä¸ç›¸ç­‰ç›´æ¥è¿”å›ï¼Œç­‰å®¢æˆ·ç«¯é‡è¯•ã€‚

```go
    if len(leftRegion.Peers) != len(split.NewPeerIds) {
      return
    }
```

### ï¼ˆ5ï¼‰find no peer for store 4 in region id:13 start_key:"0 00000011" end_key:"2 00000004"

```go
panic: find no peer for store 4 in region id:13 start_key:"0 00000011" end_key:"2 00000004" region_epoch:<conf_ver:6 version:3 > peers:<id:14 store_id:2 > 

goroutine 271 [running]:
github.com/pingcap-incubator/tinykv/kv/raftstore.(*peerMsgHandler).processAdminRequest(0xc17bdbfe10, 0xc10000000a?, 0x27?)
  /home/sszgwdk/project/tinykv/tinykv/kv/raftstore/peer_msg_handler.go:187 +0xdfd
github.com/pingcap-incubator/tinykv/kv/raftstore.(*peerMsgHandler).process(0xc17bdbfe10, 0xc20d840140)
  /home/sszgwdk/project/tinykv/tinykv/kv/raftstore/peer_msg_handler.go:445 +0x347
github.com/pingcap-incubator/tinykv/kv/raftstore.(*peerMsgHandler).HandleRaftReady(0xc17bdbfe10)
  /home/sszgwdk/project/tinykv/tinykv/kv/raftstore/peer_msg_handler.go:680 +0x57f
github.com/pingcap-incubator/tinykv/kv/raftstore.(*raftWorker).run(0xc000130b80, 0xc0002a51a0, 0x0?)
  /home/sszgwdk/project/tinykv/tinykv/kv/raftstore/raft_worker.go:70 +0x439
created by github.com/pingcap-incubator/tinykv/kv/raftstore.(*Raftstore).startWorkers
  /home/sszgwdk/project/tinykv/tinykv/kv/raftstore/raftstore.go:271 +0x17b
FAIL	github.com/pingcap-incubator/tinykv/kv/test_raftstore	7.331s
FAIL
rm -rf /tmp/*test-raftstore*
```

æŠ¥é”™ä½ç½®ï¼šApply Splitåˆ›å»ºæ–°çš„Peer

```go
    // Peer: creat and register and start
    newPeer, err := createPeer(d.ctx.store.Id, d.ctx.cfg, d.ctx.regionTaskSender, d.ctx.engine, rightRegion)
    if err != nil {
      panic(err)
    }
```

å‡ºé—®é¢˜çš„åœ°æ–¹ä¸ä¸Šä¸€ä¸ªé”™è¯¯ä¸€æ ·ï¼Œæ²¡æœ‰å¯¹`newPeers`â€‹æ˜¯å¦ä¸ºç©ºè¿›è¡Œåˆ¤æ–­ï¼Œä¹Ÿæ²¡æœ‰å¯¹`newPeers`â€‹æ˜¯å¦åŒ…å«å½“å‰storeè¿›è¡Œæ£€æŸ¥ã€‚

è§£å†³æ–¹æ³•ï¼š

```go
    newPeers := make([]*metapb.Peer, 0)
    hasCurrentStore := false
    if len(leftRegion.Peers) != len(split.NewPeerIds) {
      return
    }

    for i, peer := range leftRegion.Peers {
      if i < len(split.NewPeerIds) {
        newPeers = append(newPeers, &metapb.Peer{
          Id:      split.NewPeerIds[i],
          StoreId: peer.StoreId,
        })
        if peer.StoreId == d.ctx.store.GetId() {
          hasCurrentStore = true
        }
      }
    }
    if len(newPeers) == 0 || !hasCurrentStore {
      return
    }
```

